{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing data.txt\n",
      "31784 31784 13622 13622\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/driving_dataset/' \n",
    "TRAIN_FILE = os.path.join(DATA_FOLDER, 'data.txt')\n",
    "\n",
    "\n",
    "split =0.7\n",
    "X = []\n",
    "y = []\n",
    "with open(TRAIN_FILE) as fp:\n",
    "    for line in fp:\n",
    "        path, angle = line.strip().split()\n",
    "        full_path = os.path.join(DATA_FOLDER, path)\n",
    "        X.append(full_path)\n",
    "        \n",
    "        # converting angle from degrees to radians\n",
    "        y.append(float(angle) * pi / 180 )\n",
    "\n",
    "\n",
    "y = np.array(y)\n",
    "print(\"Completed processing data.txt\")\n",
    "\n",
    "split_ratio = int(len(X) * 0.7)\n",
    "\n",
    "train_x = X[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = X[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "print(len(train_x), len(train_y), len(test_x), len(test_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6QGGe6S1CDDXZIaZLhLUoMMd0lq0P8DjW5mOldQzNoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PDF of train and test 'y' values. \n",
    "import matplotlib.pyplot as plt \n",
    "plt.hist(train_y, bins=50, normed=1, color='green', histtype ='step');\n",
    "plt.hist(test_y, bins=50, normed=1, color='red', histtype ='step');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MSE(MEAN):0.241561\n",
      "Test_MSE(ZERO):0.241107\n"
     ]
    }
   ],
   "source": [
    "#Model 0: Base line Model: y_test_pred = mean(y_train_i) \n",
    "train_mean_y = np.mean(train_y)\n",
    "\n",
    "print('Test_MSE(MEAN):%f' % np.mean(np.square(test_y-train_mean_y)) )\n",
    "\n",
    "print('Test_MSE(ZERO):%f' % np.mean(np.square(test_y-0.0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) \n",
    "        read_image_road = read_image[-150:] \n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) \n",
    "        read_image_final = read_image_resize/255.0\n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)]) #appending corresponding labels\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) \n",
    "        read_image_road = read_image[-150:] \n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) \n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) #appending corresponding labels\n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weightVariable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2D(previous_input, filter_input, strides):\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3])\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "\n",
    "input_image = x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#First convolution layer\n",
    "W_Conv1 = weightVariable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.relu(conv2D(input_image, W_Conv1, 2) + B_Conv1)\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weightVariable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.relu(conv2D(Conv1, W_Conv2, 2) + B_Conv2)\n",
    "\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weightVariable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.relu(conv2D(Conv2, W_Conv3, 2) + B_Conv3)\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weightVariable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.relu(conv2D(Conv3, W_Conv4, 1) + B_Conv4)\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weightVariable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.relu(conv2D(Conv4, W_Conv5, 1) + B_Conv5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_FC1 = weightVariable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152])\n",
    "Output_FC1 = tf.nn.relu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) \n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "\n",
    "W_FC2 = weightVariable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.relu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) \n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "W_FC3 = weightVariable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.relu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) \n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "W_FC4 = weightVariable([50, 10])\n",
    "\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.relu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) \n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "W_FC5 = weightVariable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "\n",
    "y_predicted = tf.multiply(tf.atan(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5),2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 19.420602798461914, Test_Loss: 19.830364227294922 *\n",
      "Epoch: 1, Train_Loss: 27.89381980895996, Test_Loss: 19.873416900634766\n",
      "Epoch: 1, Train_Loss: 22.030961990356445, Test_Loss: 19.785560607910156 *\n",
      "Epoch: 1, Train_Loss: 19.37044334411621, Test_Loss: 19.628219604492188 *\n",
      "Epoch: 1, Train_Loss: 25.271411895751953, Test_Loss: 19.401042938232422 *\n",
      "Epoch: 1, Train_Loss: 19.471393585205078, Test_Loss: 18.92800521850586 *\n",
      "Epoch: 1, Train_Loss: 21.915739059448242, Test_Loss: 19.299386978149414\n",
      "Epoch: 1, Train_Loss: 18.860393524169922, Test_Loss: 19.60858917236328\n",
      "Epoch: 1, Train_Loss: 18.838003158569336, Test_Loss: 19.16059112548828 *\n",
      "Epoch: 1, Train_Loss: 18.832393646240234, Test_Loss: 19.57221031188965\n",
      "Epoch: 1, Train_Loss: 18.70285987854004, Test_Loss: 18.71562385559082 *\n",
      "Epoch: 1, Train_Loss: 18.646743774414062, Test_Loss: 18.565217971801758 *\n",
      "Epoch: 1, Train_Loss: 18.61345863342285, Test_Loss: 19.325063705444336\n",
      "Epoch: 1, Train_Loss: 18.610361099243164, Test_Loss: 19.70792007446289\n",
      "Epoch: 1, Train_Loss: 18.435014724731445, Test_Loss: 19.08921241760254 *\n",
      "Epoch: 1, Train_Loss: 18.363229751586914, Test_Loss: 18.39752197265625 *\n",
      "Epoch: 1, Train_Loss: 18.287731170654297, Test_Loss: 19.076282501220703\n",
      "Epoch: 1, Train_Loss: 18.24899673461914, Test_Loss: 18.37957000732422 *\n",
      "Epoch: 1, Train_Loss: 18.13007354736328, Test_Loss: 18.414688110351562\n",
      "Epoch: 1, Train_Loss: 18.086069107055664, Test_Loss: 19.02676010131836\n",
      "Epoch: 1, Train_Loss: 18.092222213745117, Test_Loss: 18.446372985839844 *\n",
      "Epoch: 1, Train_Loss: 18.045249938964844, Test_Loss: 18.094234466552734 *\n",
      "Epoch: 1, Train_Loss: 22.619016647338867, Test_Loss: 18.010560989379883 *\n",
      "Epoch: 1, Train_Loss: 17.931947708129883, Test_Loss: 18.328269958496094\n",
      "Epoch: 1, Train_Loss: 17.92731285095215, Test_Loss: 17.9345645904541 *\n",
      "Epoch: 1, Train_Loss: 17.88017463684082, Test_Loss: 17.936397552490234\n",
      "Epoch: 1, Train_Loss: 18.020341873168945, Test_Loss: 19.025970458984375\n",
      "Epoch: 1, Train_Loss: 17.903114318847656, Test_Loss: 18.258588790893555 *\n",
      "Epoch: 1, Train_Loss: 17.926668167114258, Test_Loss: 17.831571578979492 *\n",
      "Epoch: 1, Train_Loss: 17.960668563842773, Test_Loss: 17.780838012695312 *\n",
      "Epoch: 1, Train_Loss: 17.777820587158203, Test_Loss: 17.79485511779785\n",
      "Epoch: 1, Train_Loss: 17.737491607666016, Test_Loss: 17.717844009399414 *\n",
      "Epoch: 1, Train_Loss: 17.692468643188477, Test_Loss: 17.897151947021484\n",
      "Epoch: 1, Train_Loss: 17.61901092529297, Test_Loss: 17.743928909301758 *\n",
      "Epoch: 1, Train_Loss: 20.512027740478516, Test_Loss: 17.620389938354492 *\n",
      "Epoch: 1, Train_Loss: 17.571475982666016, Test_Loss: 17.589473724365234 *\n",
      "Epoch: 1, Train_Loss: 17.55953598022461, Test_Loss: 20.502037048339844\n",
      "Epoch: 1, Train_Loss: 17.51742172241211, Test_Loss: 17.50925064086914 *\n",
      "Epoch: 1, Train_Loss: 17.50290298461914, Test_Loss: 17.482837677001953 *\n",
      "Epoch: 1, Train_Loss: 17.46778678894043, Test_Loss: 17.441370010375977 *\n",
      "Epoch: 1, Train_Loss: 17.43007469177246, Test_Loss: 17.430089950561523 *\n",
      "Epoch: 1, Train_Loss: 17.39609718322754, Test_Loss: 17.409589767456055 *\n",
      "Epoch: 1, Train_Loss: 17.40639305114746, Test_Loss: 17.383169174194336 *\n",
      "Epoch: 1, Train_Loss: 17.3836727142334, Test_Loss: 17.35584259033203 *\n",
      "Epoch: 1, Train_Loss: 17.29904556274414, Test_Loss: 17.327045440673828 *\n",
      "Epoch: 1, Train_Loss: 17.33378028869629, Test_Loss: 17.28707504272461 *\n",
      "Epoch: 1, Train_Loss: 17.28871726989746, Test_Loss: 17.275026321411133 *\n",
      "Epoch: 1, Train_Loss: 17.2994327545166, Test_Loss: 17.229576110839844 *\n",
      "Epoch: 1, Train_Loss: 17.258602142333984, Test_Loss: 17.212194442749023 *\n",
      "Epoch: 1, Train_Loss: 17.24372673034668, Test_Loss: 17.180091857910156 *\n",
      "Epoch: 1, Train_Loss: 17.149145126342773, Test_Loss: 17.157522201538086 *\n",
      "Epoch: 1, Train_Loss: 17.136362075805664, Test_Loss: 17.17836570739746\n",
      "Epoch: 1, Train_Loss: 17.182817459106445, Test_Loss: 19.98103141784668\n",
      "Epoch: 1, Train_Loss: 19.53806495666504, Test_Loss: 17.07717514038086 *\n",
      "Epoch: 1, Train_Loss: 17.06702423095703, Test_Loss: 17.07261848449707 *\n",
      "Epoch: 1, Train_Loss: 17.007381439208984, Test_Loss: 17.06983757019043 *\n",
      "Epoch: 1, Train_Loss: 16.982004165649414, Test_Loss: 17.048370361328125 *\n",
      "Epoch: 1, Train_Loss: 16.952173233032227, Test_Loss: 16.986631393432617 *\n",
      "Epoch: 1, Train_Loss: 17.048954010009766, Test_Loss: 16.93899154663086 *\n",
      "Epoch: 1, Train_Loss: 17.022075653076172, Test_Loss: 16.96556282043457\n",
      "Epoch: 1, Train_Loss: 16.92729949951172, Test_Loss: 17.003461837768555\n",
      "Epoch: 1, Train_Loss: 17.041648864746094, Test_Loss: 16.885334014892578 *\n",
      "Epoch: 1, Train_Loss: 16.894254684448242, Test_Loss: 16.858963012695312 *\n",
      "Epoch: 1, Train_Loss: 16.79082679748535, Test_Loss: 16.819129943847656 *\n",
      "Epoch: 1, Train_Loss: 16.76641845703125, Test_Loss: 16.794706344604492 *\n",
      "Epoch: 1, Train_Loss: 16.748233795166016, Test_Loss: 16.76532745361328 *\n",
      "Epoch: 1, Train_Loss: 16.719804763793945, Test_Loss: 16.71714210510254 *\n",
      "Epoch: 1, Train_Loss: 16.75227165222168, Test_Loss: 16.717239379882812\n",
      "Epoch: 1, Train_Loss: 16.801862716674805, Test_Loss: 16.861581802368164\n",
      "Epoch: 1, Train_Loss: 16.747112274169922, Test_Loss: 16.69133758544922 *\n",
      "Epoch: 1, Train_Loss: 16.724828720092773, Test_Loss: 16.82612419128418\n",
      "Epoch: 1, Train_Loss: 16.83371925354004, Test_Loss: 16.778783798217773 *\n",
      "Epoch: 1, Train_Loss: 16.589696884155273, Test_Loss: 16.72907257080078 *\n",
      "Epoch: 1, Train_Loss: 18.273902893066406, Test_Loss: 16.57227325439453 *\n",
      "Epoch: 1, Train_Loss: 16.538799285888672, Test_Loss: 16.568099975585938 *\n",
      "Epoch: 1, Train_Loss: 16.5189266204834, Test_Loss: 17.04533576965332\n",
      "Epoch: 1, Train_Loss: 16.48398208618164, Test_Loss: 17.138870239257812\n",
      "Epoch: 1, Train_Loss: 16.589035034179688, Test_Loss: 16.903289794921875 *\n",
      "Epoch: 1, Train_Loss: 16.52687644958496, Test_Loss: 16.42686653137207 *\n",
      "Epoch: 1, Train_Loss: 16.43589973449707, Test_Loss: 16.488540649414062\n",
      "Epoch: 1, Train_Loss: 16.37740135192871, Test_Loss: 16.949146270751953\n",
      "Epoch: 1, Train_Loss: 16.344154357910156, Test_Loss: 17.601802825927734\n",
      "Epoch: 1, Train_Loss: 16.35163116455078, Test_Loss: 17.019338607788086 *\n",
      "Epoch: 1, Train_Loss: 16.330001831054688, Test_Loss: 16.337793350219727 *\n",
      "Epoch: 1, Train_Loss: 16.26517677307129, Test_Loss: 17.319721221923828\n",
      "Epoch: 1, Train_Loss: 16.233182907104492, Test_Loss: 16.30231475830078 *\n",
      "Epoch: 1, Train_Loss: 16.2093505859375, Test_Loss: 16.46358299255371\n",
      "Epoch: 1, Train_Loss: 16.19192886352539, Test_Loss: 16.755491256713867\n",
      "Epoch: 1, Train_Loss: 16.164077758789062, Test_Loss: 16.685224533081055 *\n",
      "Epoch: 1, Train_Loss: 16.1356258392334, Test_Loss: 16.153827667236328 *\n",
      "Epoch: 1, Train_Loss: 16.113676071166992, Test_Loss: 16.145671844482422 *\n",
      "Epoch: 1, Train_Loss: 16.086997985839844, Test_Loss: 16.64405059814453\n",
      "Epoch: 1, Train_Loss: 16.0687313079834, Test_Loss: 16.154178619384766 *\n",
      "Epoch: 1, Train_Loss: 16.04193687438965, Test_Loss: 16.062868118286133 *\n",
      "Epoch: 1, Train_Loss: 16.013891220092773, Test_Loss: 16.819116592407227\n",
      "Epoch: 1, Train_Loss: 16.047693252563477, Test_Loss: 16.589506149291992 *\n",
      "Epoch: 1, Train_Loss: 15.989753723144531, Test_Loss: 16.005569458007812 *\n",
      "Epoch: 1, Train_Loss: 15.95724105834961, Test_Loss: 15.944746971130371 *\n",
      "Epoch: 1, Train_Loss: 15.929946899414062, Test_Loss: 15.966784477233887\n",
      "Epoch: 1, Train_Loss: 15.915369033813477, Test_Loss: 15.901910781860352 *\n",
      "Epoch: 1, Train_Loss: 16.006481170654297, Test_Loss: 16.11851692199707\n",
      "Epoch: 1, Train_Loss: 15.883719444274902, Test_Loss: 15.95173454284668 *\n",
      "Epoch: 1, Train_Loss: 15.84075927734375, Test_Loss: 15.824163436889648 *\n",
      "Epoch: 1, Train_Loss: 15.793965339660645, Test_Loss: 15.808595657348633 *\n",
      "Epoch: 1, Train_Loss: 15.780895233154297, Test_Loss: 18.598363876342773\n",
      "Epoch: 1, Train_Loss: 15.794682502746582, Test_Loss: 15.771214485168457 *\n",
      "Epoch: 1, Train_Loss: 15.772692680358887, Test_Loss: 15.719042778015137 *\n",
      "Epoch: 1, Train_Loss: 15.762308120727539, Test_Loss: 15.694682121276855 *\n",
      "Epoch: 1, Train_Loss: 15.772735595703125, Test_Loss: 15.674128532409668 *\n",
      "Epoch: 1, Train_Loss: 15.660333633422852, Test_Loss: 15.649164199829102 *\n",
      "Epoch: 1, Train_Loss: 15.624672889709473, Test_Loss: 15.629722595214844 *\n",
      "Epoch: 1, Train_Loss: 15.616621971130371, Test_Loss: 15.611886024475098 *\n",
      "Epoch: 1, Train_Loss: 15.605215072631836, Test_Loss: 15.577878952026367 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 15.593194007873535, Test_Loss: 15.552091598510742 *\n",
      "Epoch: 1, Train_Loss: 15.537323951721191, Test_Loss: 15.527772903442383 *\n",
      "Epoch: 1, Train_Loss: 15.510581970214844, Test_Loss: 15.50261402130127 *\n",
      "Epoch: 1, Train_Loss: 15.488000869750977, Test_Loss: 15.481365203857422 *\n",
      "Epoch: 1, Train_Loss: 15.508981704711914, Test_Loss: 15.463021278381348 *\n",
      "Epoch: 1, Train_Loss: 15.500788688659668, Test_Loss: 15.435944557189941 *\n",
      "Epoch: 1, Train_Loss: 15.413887977600098, Test_Loss: 15.450529098510742\n",
      "Epoch: 1, Train_Loss: 15.401970863342285, Test_Loss: 18.19193458557129\n",
      "Epoch: 1, Train_Loss: 15.373991966247559, Test_Loss: 15.364572525024414 *\n",
      "Epoch: 1, Train_Loss: 17.984312057495117, Test_Loss: 15.364299774169922 *\n",
      "Epoch: 1, Train_Loss: 15.418365478515625, Test_Loss: 15.361225128173828 *\n",
      "Epoch: 1, Train_Loss: 15.412849426269531, Test_Loss: 15.3639554977417\n",
      "Epoch: 1, Train_Loss: 15.311050415039062, Test_Loss: 15.298650741577148 *\n",
      "Epoch: 1, Train_Loss: 15.290034294128418, Test_Loss: 15.253080368041992 *\n",
      "Epoch: 1, Train_Loss: 15.244149208068848, Test_Loss: 15.288630485534668\n",
      "Epoch: 1, Train_Loss: 16.22797393798828, Test_Loss: 15.339764595031738\n",
      "Epoch: 1, Train_Loss: 15.842823028564453, Test_Loss: 15.19202709197998 *\n",
      "Epoch: 1, Train_Loss: 16.385961532592773, Test_Loss: 15.16649341583252 *\n",
      "Epoch: 1, Train_Loss: 15.625798225402832, Test_Loss: 15.146469116210938 *\n",
      "Epoch: 1, Train_Loss: 15.589073181152344, Test_Loss: 15.114509582519531 *\n",
      "Epoch: 1, Train_Loss: 16.382925033569336, Test_Loss: 15.101216316223145 *\n",
      "Epoch: 1, Train_Loss: 15.085576057434082, Test_Loss: 15.061087608337402 *\n",
      "Epoch: 1, Train_Loss: 15.399415016174316, Test_Loss: 15.082058906555176\n",
      "Epoch: 1, Train_Loss: 15.03942584991455, Test_Loss: 15.193181037902832\n",
      "Epoch: 1, Train_Loss: 15.11637020111084, Test_Loss: 15.068500518798828 *\n",
      "Epoch: 1, Train_Loss: 15.242928504943848, Test_Loss: 15.218046188354492\n",
      "Epoch: 1, Train_Loss: 15.0551118850708, Test_Loss: 15.093406677246094 *\n",
      "Epoch: 1, Train_Loss: 15.229904174804688, Test_Loss: 15.099610328674316\n",
      "Epoch: 1, Train_Loss: 15.214910507202148, Test_Loss: 14.929935455322266 *\n",
      "Epoch: 1, Train_Loss: 14.997705459594727, Test_Loss: 14.909943580627441 *\n",
      "Epoch: 1, Train_Loss: 14.906182289123535, Test_Loss: 15.392723083496094\n",
      "Epoch: 1, Train_Loss: 14.831436157226562, Test_Loss: 15.631429672241211\n",
      "Epoch: 1, Train_Loss: 14.810041427612305, Test_Loss: 15.227581024169922 *\n",
      "Epoch: 1, Train_Loss: 14.85889720916748, Test_Loss: 14.829428672790527 *\n",
      "Epoch: 1, Train_Loss: 14.873616218566895, Test_Loss: 14.847249984741211\n",
      "Epoch: 1, Train_Loss: 15.046661376953125, Test_Loss: 15.298104286193848\n",
      "Epoch: 1, Train_Loss: 14.785943984985352, Test_Loss: 15.952414512634277\n",
      "Epoch: 1, Train_Loss: 15.185075759887695, Test_Loss: 15.439441680908203 *\n",
      "Epoch: 1, Train_Loss: 14.685406684875488, Test_Loss: 14.693887710571289 *\n",
      "Epoch: 1, Train_Loss: 15.302216529846191, Test_Loss: 15.743139266967773\n",
      "Epoch: 1, Train_Loss: 14.718400001525879, Test_Loss: 14.714192390441895 *\n",
      "Epoch: 1, Train_Loss: 14.990936279296875, Test_Loss: 14.75113582611084\n",
      "Epoch: 1, Train_Loss: 15.501250267028809, Test_Loss: 15.09653091430664\n",
      "Epoch: 1, Train_Loss: 14.579083442687988, Test_Loss: 15.279765129089355\n",
      "Epoch: 1, Train_Loss: 14.772797584533691, Test_Loss: 14.555968284606934 *\n",
      "Epoch: 1, Train_Loss: 14.575697898864746, Test_Loss: 14.56344223022461\n",
      "Epoch: 1, Train_Loss: 14.506664276123047, Test_Loss: 14.974129676818848\n",
      "Epoch: 1, Train_Loss: 23.268089294433594, Test_Loss: 14.593506813049316 *\n",
      "Epoch: 1, Train_Loss: 16.603759765625, Test_Loss: 14.476462364196777 *\n",
      "Epoch: 1, Train_Loss: 14.475137710571289, Test_Loss: 15.043889999389648\n",
      "Epoch: 1, Train_Loss: 20.495437622070312, Test_Loss: 15.200348854064941\n",
      "Epoch: 1, Train_Loss: 14.484334945678711, Test_Loss: 14.436689376831055 *\n",
      "Epoch: 1, Train_Loss: 17.54196548461914, Test_Loss: 14.376925468444824 *\n",
      "Epoch: 1, Train_Loss: 14.364031791687012, Test_Loss: 14.393898963928223\n",
      "Epoch: 1, Train_Loss: 14.330475807189941, Test_Loss: 14.35423469543457 *\n",
      "Epoch: 1, Train_Loss: 14.308899879455566, Test_Loss: 14.52014446258545\n",
      "Epoch: 1, Train_Loss: 14.294341087341309, Test_Loss: 14.36868953704834 *\n",
      "Epoch: 1, Train_Loss: 14.267257690429688, Test_Loss: 14.262308120727539 *\n",
      "Epoch: 1, Train_Loss: 14.265439987182617, Test_Loss: 14.246543884277344 *\n",
      "Epoch: 1, Train_Loss: 14.281004905700684, Test_Loss: 17.069732666015625\n",
      "Epoch: 1, Train_Loss: 14.201160430908203, Test_Loss: 14.340124130249023 *\n",
      "Epoch: 1, Train_Loss: 14.17370319366455, Test_Loss: 14.173417091369629 *\n",
      "Epoch: 1, Train_Loss: 14.156493186950684, Test_Loss: 14.149300575256348 *\n",
      "Epoch: 1, Train_Loss: 14.141559600830078, Test_Loss: 14.143423080444336 *\n",
      "Epoch: 1, Train_Loss: 14.11236572265625, Test_Loss: 14.122283935546875 *\n",
      "Epoch: 1, Train_Loss: 14.088386535644531, Test_Loss: 14.09758186340332 *\n",
      "Epoch: 1, Train_Loss: 14.08491039276123, Test_Loss: 14.073190689086914 *\n",
      "Epoch: 1, Train_Loss: 14.18524169921875, Test_Loss: 14.056378364562988 *\n",
      "Epoch: 1, Train_Loss: 18.48952865600586, Test_Loss: 14.034907341003418 *\n",
      "Epoch: 1, Train_Loss: 14.0032320022583, Test_Loss: 14.016549110412598 *\n",
      "Epoch: 1, Train_Loss: 13.998738288879395, Test_Loss: 13.99023151397705 *\n",
      "Epoch: 1, Train_Loss: 13.973870277404785, Test_Loss: 13.975907325744629 *\n",
      "Epoch: 1, Train_Loss: 14.159687042236328, Test_Loss: 13.95639419555664 *\n",
      "Epoch: 1, Train_Loss: 14.005044937133789, Test_Loss: 13.929620742797852 *\n",
      "Epoch: 1, Train_Loss: 14.026655197143555, Test_Loss: 13.949275970458984\n",
      "Epoch: 1, Train_Loss: 14.037457466125488, Test_Loss: 16.86359214782715\n",
      "Epoch: 1, Train_Loss: 13.955202102661133, Test_Loss: 13.875372886657715 *\n",
      "Epoch: 1, Train_Loss: 13.866876602172852, Test_Loss: 13.85335922241211 *\n",
      "Epoch: 1, Train_Loss: 13.830777168273926, Test_Loss: 13.845151901245117 *\n",
      "Epoch: 1, Train_Loss: 13.803356170654297, Test_Loss: 13.907262802124023\n",
      "Epoch: 1, Train_Loss: 16.54530906677246, Test_Loss: 13.815174102783203 *\n",
      "Epoch: 1, Train_Loss: 13.784077644348145, Test_Loss: 13.769932746887207 *\n",
      "Epoch: 1, Train_Loss: 13.762947082519531, Test_Loss: 13.784236907958984\n",
      "Epoch: 1, Train_Loss: 13.730713844299316, Test_Loss: 13.869043350219727\n",
      "Epoch: 1, Train_Loss: 13.706442832946777, Test_Loss: 13.71983528137207 *\n",
      "Epoch: 1, Train_Loss: 13.682517051696777, Test_Loss: 13.67303466796875 *\n",
      "Epoch: 1, Train_Loss: 13.664615631103516, Test_Loss: 13.65552043914795 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 1\n",
      "Epoch: 1, Train_Loss: 13.648091316223145, Test_Loss: 13.636801719665527 *\n",
      "Epoch: 1, Train_Loss: 13.645991325378418, Test_Loss: 13.614861488342285 *\n",
      "Epoch: 1, Train_Loss: 13.648754119873047, Test_Loss: 13.593639373779297 *\n",
      "Epoch: 1, Train_Loss: 13.577810287475586, Test_Loss: 13.586865425109863 *\n",
      "Epoch: 1, Train_Loss: 13.618121147155762, Test_Loss: 13.776470184326172\n",
      "Epoch: 1, Train_Loss: 13.582422256469727, Test_Loss: 13.566569328308105 *\n",
      "Epoch: 1, Train_Loss: 13.588255882263184, Test_Loss: 13.678890228271484\n",
      "Epoch: 1, Train_Loss: 13.542850494384766, Test_Loss: 13.630987167358398 *\n",
      "Epoch: 1, Train_Loss: 13.539621353149414, Test_Loss: 13.725991249084473\n",
      "Epoch: 1, Train_Loss: 13.454258918762207, Test_Loss: 13.4903564453125 *\n",
      "Epoch: 1, Train_Loss: 13.445756912231445, Test_Loss: 13.437895774841309 *\n",
      "Epoch: 1, Train_Loss: 13.815046310424805, Test_Loss: 14.000969886779785\n",
      "Epoch: 1, Train_Loss: 15.603313446044922, Test_Loss: 14.068183898925781\n",
      "Epoch: 1, Train_Loss: 13.40698528289795, Test_Loss: 13.81163215637207 *\n",
      "Epoch: 1, Train_Loss: 13.349461555480957, Test_Loss: 13.4500093460083 *\n",
      "Epoch: 1, Train_Loss: 13.33415412902832, Test_Loss: 13.374855041503906 *\n",
      "Epoch: 1, Train_Loss: 13.314202308654785, Test_Loss: 13.943792343139648\n",
      "Epoch: 1, Train_Loss: 13.431365966796875, Test_Loss: 14.446495056152344\n",
      "Epoch: 1, Train_Loss: 13.399083137512207, Test_Loss: 14.060937881469727 *\n",
      "Epoch: 1, Train_Loss: 13.302230834960938, Test_Loss: 13.262184143066406 *\n",
      "Epoch: 1, Train_Loss: 13.385560035705566, Test_Loss: 14.217794418334961\n",
      "Epoch: 1, Train_Loss: 13.267881393432617, Test_Loss: 13.336030006408691 *\n",
      "Epoch: 1, Train_Loss: 13.193265914916992, Test_Loss: 13.265423774719238 *\n",
      "Epoch: 1, Train_Loss: 13.171435356140137, Test_Loss: 13.575887680053711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_Loss: 13.151519775390625, Test_Loss: 14.100236892700195\n",
      "Epoch: 1, Train_Loss: 13.134202003479004, Test_Loss: 13.148601531982422 *\n",
      "Epoch: 1, Train_Loss: 13.18185043334961, Test_Loss: 13.17525863647461\n",
      "Epoch: 2, Train_Loss: 13.222920417785645, Test_Loss: 13.472558975219727 *\n",
      "Epoch: 2, Train_Loss: 13.209760665893555, Test_Loss: 13.223509788513184 *\n",
      "Epoch: 2, Train_Loss: 13.175545692443848, Test_Loss: 13.077075958251953 *\n",
      "Epoch: 2, Train_Loss: 13.266229629516602, Test_Loss: 13.539132118225098\n",
      "Epoch: 2, Train_Loss: 13.028770446777344, Test_Loss: 13.971466064453125\n",
      "Epoch: 2, Train_Loss: 14.707237243652344, Test_Loss: 13.043878555297852 *\n",
      "Epoch: 2, Train_Loss: 13.003746032714844, Test_Loss: 12.989295959472656 *\n",
      "Epoch: 2, Train_Loss: 12.981438636779785, Test_Loss: 12.994961738586426\n",
      "Epoch: 2, Train_Loss: 12.98077392578125, Test_Loss: 12.974736213684082 *\n",
      "Epoch: 2, Train_Loss: 13.07520580291748, Test_Loss: 13.152544975280762\n",
      "Epoch: 2, Train_Loss: 13.016416549682617, Test_Loss: 13.025029182434082 *\n",
      "Epoch: 2, Train_Loss: 12.945981979370117, Test_Loss: 12.895237922668457 *\n",
      "Epoch: 2, Train_Loss: 12.876636505126953, Test_Loss: 12.884549140930176 *\n",
      "Epoch: 2, Train_Loss: 12.850602149963379, Test_Loss: 14.97734260559082\n",
      "Epoch: 2, Train_Loss: 12.881052017211914, Test_Loss: 13.481378555297852 *\n",
      "Epoch: 2, Train_Loss: 12.84795093536377, Test_Loss: 12.810954093933105 *\n",
      "Epoch: 2, Train_Loss: 12.789789199829102, Test_Loss: 12.7991361618042 *\n",
      "Epoch: 2, Train_Loss: 12.769536018371582, Test_Loss: 12.771398544311523 *\n",
      "Epoch: 2, Train_Loss: 12.749205589294434, Test_Loss: 12.755144119262695 *\n",
      "Epoch: 2, Train_Loss: 12.730528831481934, Test_Loss: 12.738627433776855 *\n",
      "Epoch: 2, Train_Loss: 12.715868949890137, Test_Loss: 12.733195304870605 *\n",
      "Epoch: 2, Train_Loss: 12.697664260864258, Test_Loss: 12.696919441223145 *\n",
      "Epoch: 2, Train_Loss: 12.681893348693848, Test_Loss: 12.678108215332031 *\n",
      "Epoch: 2, Train_Loss: 12.656387329101562, Test_Loss: 12.658398628234863 *\n",
      "Epoch: 2, Train_Loss: 12.638150215148926, Test_Loss: 12.637876510620117 *\n",
      "Epoch: 2, Train_Loss: 12.632219314575195, Test_Loss: 12.627196311950684 *\n",
      "Epoch: 2, Train_Loss: 12.601044654846191, Test_Loss: 12.60447883605957 *\n",
      "Epoch: 2, Train_Loss: 12.646817207336426, Test_Loss: 12.584905624389648 *\n",
      "Epoch: 2, Train_Loss: 12.594730377197266, Test_Loss: 12.593415260314941\n",
      "Epoch: 2, Train_Loss: 12.572681427001953, Test_Loss: 15.325761795043945\n",
      "Epoch: 2, Train_Loss: 12.539487838745117, Test_Loss: 12.547795295715332 *\n",
      "Epoch: 2, Train_Loss: 12.532927513122559, Test_Loss: 12.554743766784668\n",
      "Epoch: 2, Train_Loss: 12.624601364135742, Test_Loss: 12.513781547546387 *\n",
      "Epoch: 2, Train_Loss: 12.502554893493652, Test_Loss: 12.569161415100098\n",
      "Epoch: 2, Train_Loss: 12.468744277954102, Test_Loss: 12.478944778442383 *\n",
      "Epoch: 2, Train_Loss: 12.442901611328125, Test_Loss: 12.447136878967285 *\n",
      "Epoch: 2, Train_Loss: 12.43658447265625, Test_Loss: 12.502273559570312\n",
      "Epoch: 2, Train_Loss: 12.45039176940918, Test_Loss: 12.495092391967773 *\n",
      "Epoch: 2, Train_Loss: 12.425822257995605, Test_Loss: 12.42546272277832 *\n",
      "Epoch: 2, Train_Loss: 12.422857284545898, Test_Loss: 12.366403579711914 *\n",
      "Epoch: 2, Train_Loss: 12.460159301757812, Test_Loss: 12.356619834899902 *\n",
      "Epoch: 2, Train_Loss: 12.336494445800781, Test_Loss: 12.332013130187988 *\n",
      "Epoch: 2, Train_Loss: 12.306206703186035, Test_Loss: 12.325385093688965 *\n",
      "Epoch: 2, Train_Loss: 12.30239486694336, Test_Loss: 12.291769981384277 *\n",
      "Epoch: 2, Train_Loss: 12.298727989196777, Test_Loss: 12.291121482849121 *\n",
      "Epoch: 2, Train_Loss: 12.285212516784668, Test_Loss: 12.439696311950684\n",
      "Epoch: 2, Train_Loss: 12.237579345703125, Test_Loss: 12.275583267211914 *\n",
      "Epoch: 2, Train_Loss: 12.228228569030762, Test_Loss: 12.44976806640625\n",
      "Epoch: 2, Train_Loss: 12.206616401672363, Test_Loss: 12.306085586547852 *\n",
      "Epoch: 2, Train_Loss: 12.224761962890625, Test_Loss: 12.42402172088623\n",
      "Epoch: 2, Train_Loss: 12.2118558883667, Test_Loss: 12.192578315734863 *\n",
      "Epoch: 2, Train_Loss: 12.155391693115234, Test_Loss: 12.161823272705078 *\n",
      "Epoch: 2, Train_Loss: 12.14189338684082, Test_Loss: 12.646153450012207\n",
      "Epoch: 2, Train_Loss: 12.133520126342773, Test_Loss: 12.753674507141113\n",
      "Epoch: 2, Train_Loss: 14.817602157592773, Test_Loss: 12.520057678222656 *\n",
      "Epoch: 2, Train_Loss: 12.081929206848145, Test_Loss: 12.220368385314941 *\n",
      "Epoch: 2, Train_Loss: 12.190459251403809, Test_Loss: 12.086620330810547 *\n",
      "Epoch: 2, Train_Loss: 12.065829277038574, Test_Loss: 12.634156227111816\n",
      "Epoch: 2, Train_Loss: 12.067449569702148, Test_Loss: 13.186396598815918\n",
      "Epoch: 2, Train_Loss: 12.012764930725098, Test_Loss: 12.80154800415039 *\n",
      "Epoch: 2, Train_Loss: 13.109411239624023, Test_Loss: 12.006229400634766 *\n",
      "Epoch: 2, Train_Loss: 12.492203712463379, Test_Loss: 12.858572006225586\n",
      "Epoch: 2, Train_Loss: 13.312511444091797, Test_Loss: 12.201364517211914 *\n",
      "Epoch: 2, Train_Loss: 12.234928131103516, Test_Loss: 11.99797248840332 *\n",
      "Epoch: 2, Train_Loss: 12.60018539428711, Test_Loss: 12.245847702026367\n",
      "Epoch: 2, Train_Loss: 12.969873428344727, Test_Loss: 12.82963752746582\n",
      "Epoch: 2, Train_Loss: 11.9019193649292, Test_Loss: 11.885030746459961 *\n",
      "Epoch: 2, Train_Loss: 12.227002143859863, Test_Loss: 11.891523361206055\n",
      "Epoch: 2, Train_Loss: 11.865870475769043, Test_Loss: 12.21481990814209\n",
      "Epoch: 2, Train_Loss: 11.979686737060547, Test_Loss: 12.11484432220459 *\n",
      "Epoch: 2, Train_Loss: 12.07923698425293, Test_Loss: 11.838747024536133 *\n",
      "Epoch: 2, Train_Loss: 11.880570411682129, Test_Loss: 12.091543197631836\n",
      "Epoch: 2, Train_Loss: 12.104938507080078, Test_Loss: 12.795771598815918\n",
      "Epoch: 2, Train_Loss: 12.032255172729492, Test_Loss: 11.815866470336914 *\n",
      "Epoch: 2, Train_Loss: 11.864316940307617, Test_Loss: 11.74337100982666 *\n",
      "Epoch: 2, Train_Loss: 11.759319305419922, Test_Loss: 11.744659423828125\n",
      "Epoch: 2, Train_Loss: 11.702226638793945, Test_Loss: 11.730180740356445 *\n",
      "Epoch: 2, Train_Loss: 11.690276145935059, Test_Loss: 11.836223602294922\n",
      "Epoch: 2, Train_Loss: 11.739161491394043, Test_Loss: 11.926371574401855\n",
      "Epoch: 2, Train_Loss: 11.739130973815918, Test_Loss: 11.663304328918457 *\n",
      "Epoch: 2, Train_Loss: 11.934324264526367, Test_Loss: 11.654736518859863 *\n",
      "Epoch: 2, Train_Loss: 11.70327377319336, Test_Loss: 12.882844924926758\n",
      "Epoch: 2, Train_Loss: 12.058882713317871, Test_Loss: 13.126211166381836\n",
      "Epoch: 2, Train_Loss: 11.598505020141602, Test_Loss: 11.586167335510254 *\n",
      "Epoch: 2, Train_Loss: 12.257959365844727, Test_Loss: 11.571359634399414 *\n",
      "Epoch: 2, Train_Loss: 11.622149467468262, Test_Loss: 11.55113410949707 *\n",
      "Epoch: 2, Train_Loss: 12.042414665222168, Test_Loss: 11.53459644317627 *\n",
      "Epoch: 2, Train_Loss: 12.286070823669434, Test_Loss: 11.519021987915039 *\n",
      "Epoch: 2, Train_Loss: 11.515573501586914, Test_Loss: 11.519935607910156\n",
      "Epoch: 2, Train_Loss: 11.723702430725098, Test_Loss: 11.482401847839355 *\n",
      "Epoch: 2, Train_Loss: 11.512619018554688, Test_Loss: 11.466573715209961 *\n",
      "Epoch: 2, Train_Loss: 11.46240520477295, Test_Loss: 11.449287414550781 *\n",
      "Epoch: 2, Train_Loss: 20.23149871826172, Test_Loss: 11.432312965393066 *\n",
      "Epoch: 2, Train_Loss: 13.572600364685059, Test_Loss: 11.417573928833008 *\n",
      "Epoch: 2, Train_Loss: 11.438142776489258, Test_Loss: 11.399144172668457 *\n",
      "Epoch: 2, Train_Loss: 17.638689041137695, Test_Loss: 11.38694953918457 *\n",
      "Epoch: 2, Train_Loss: 11.40322208404541, Test_Loss: 11.389970779418945\n",
      "Epoch: 2, Train_Loss: 14.523094177246094, Test_Loss: 13.807662010192871\n",
      "Epoch: 2, Train_Loss: 11.351186752319336, Test_Loss: 11.738000869750977 *\n",
      "Epoch: 2, Train_Loss: 11.319073677062988, Test_Loss: 11.337965965270996 *\n",
      "Epoch: 2, Train_Loss: 11.307303428649902, Test_Loss: 11.31700611114502 *\n",
      "Epoch: 2, Train_Loss: 11.287243843078613, Test_Loss: 11.410533905029297\n",
      "Epoch: 2, Train_Loss: 11.272082328796387, Test_Loss: 11.30416202545166 *\n",
      "Epoch: 2, Train_Loss: 11.276762962341309, Test_Loss: 11.264348030090332 *\n",
      "Epoch: 2, Train_Loss: 11.277885437011719, Test_Loss: 11.277186393737793\n",
      "Epoch: 2, Train_Loss: 11.224884033203125, Test_Loss: 11.336389541625977\n",
      "Epoch: 2, Train_Loss: 11.199361801147461, Test_Loss: 11.2618408203125 *\n",
      "Epoch: 2, Train_Loss: 11.185522079467773, Test_Loss: 11.183696746826172 *\n",
      "Epoch: 2, Train_Loss: 11.176501274108887, Test_Loss: 11.164054870605469 *\n",
      "Epoch: 2, Train_Loss: 11.1545991897583, Test_Loss: 11.147884368896484 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 11.135392189025879, Test_Loss: 11.135358810424805 *\n",
      "Epoch: 2, Train_Loss: 11.129815101623535, Test_Loss: 11.117905616760254 *\n",
      "Epoch: 2, Train_Loss: 11.746606826782227, Test_Loss: 11.117321968078613 *\n",
      "Epoch: 2, Train_Loss: 15.040069580078125, Test_Loss: 11.300050735473633\n",
      "Epoch: 2, Train_Loss: 11.072242736816406, Test_Loss: 11.100519180297852 *\n",
      "Epoch: 2, Train_Loss: 11.076238632202148, Test_Loss: 11.24470329284668\n",
      "Epoch: 2, Train_Loss: 11.049771308898926, Test_Loss: 11.128019332885742 *\n",
      "Epoch: 2, Train_Loss: 11.256317138671875, Test_Loss: 11.370003700256348\n",
      "Epoch: 2, Train_Loss: 11.091723442077637, Test_Loss: 11.056768417358398 *\n",
      "Epoch: 2, Train_Loss: 11.131568908691406, Test_Loss: 10.998311042785645 *\n",
      "Epoch: 2, Train_Loss: 11.144840240478516, Test_Loss: 11.566431045532227\n",
      "Epoch: 2, Train_Loss: 11.037610054016113, Test_Loss: 11.441856384277344 *\n",
      "Epoch: 2, Train_Loss: 10.97834300994873, Test_Loss: 11.427695274353027 *\n",
      "Epoch: 2, Train_Loss: 10.940555572509766, Test_Loss: 11.196074485778809 *\n",
      "Epoch: 2, Train_Loss: 10.915862083435059, Test_Loss: 10.919652938842773 *\n",
      "Epoch: 2, Train_Loss: 13.69411849975586, Test_Loss: 11.486775398254395\n",
      "Epoch: 2, Train_Loss: 10.903348922729492, Test_Loss: 11.904208183288574\n",
      "Epoch: 2, Train_Loss: 10.892874717712402, Test_Loss: 11.798051834106445 *\n",
      "Epoch: 2, Train_Loss: 10.862434387207031, Test_Loss: 10.949603080749512 *\n",
      "Epoch: 2, Train_Loss: 10.846253395080566, Test_Loss: 11.438969612121582\n",
      "Epoch: 2, Train_Loss: 10.830516815185547, Test_Loss: 11.197535514831543 *\n",
      "Epoch: 2, Train_Loss: 10.814718246459961, Test_Loss: 10.854320526123047 *\n",
      "Epoch: 2, Train_Loss: 10.803116798400879, Test_Loss: 11.067855834960938\n",
      "Epoch: 2, Train_Loss: 10.811288833618164, Test_Loss: 11.904912948608398\n",
      "Epoch: 2, Train_Loss: 10.809867858886719, Test_Loss: 10.822351455688477 *\n",
      "Epoch: 2, Train_Loss: 10.74987506866455, Test_Loss: 10.792802810668945 *\n",
      "Epoch: 2, Train_Loss: 10.790671348571777, Test_Loss: 10.962318420410156\n",
      "Epoch: 2, Train_Loss: 10.759875297546387, Test_Loss: 10.95315170288086 *\n",
      "Epoch: 2, Train_Loss: 10.774483680725098, Test_Loss: 10.699772834777832 *\n",
      "Epoch: 2, Train_Loss: 10.72708511352539, Test_Loss: 10.898388862609863\n",
      "Epoch: 2, Train_Loss: 10.725044250488281, Test_Loss: 11.980019569396973\n",
      "Epoch: 2, Train_Loss: 10.651803016662598, Test_Loss: 10.681517601013184 *\n",
      "Epoch: 2, Train_Loss: 10.638809204101562, Test_Loss: 10.641590118408203 *\n",
      "Epoch: 2, Train_Loss: 11.570950508117676, Test_Loss: 10.64889144897461\n",
      "Epoch: 2, Train_Loss: 12.264622688293457, Test_Loss: 10.657758712768555\n",
      "Epoch: 2, Train_Loss: 10.620101928710938, Test_Loss: 10.66373348236084\n",
      "Epoch: 2, Train_Loss: 10.571883201599121, Test_Loss: 10.797101974487305\n",
      "Epoch: 2, Train_Loss: 10.5587739944458, Test_Loss: 10.555248260498047 *\n",
      "Epoch: 2, Train_Loss: 10.548730850219727, Test_Loss: 10.537853240966797 *\n",
      "Epoch: 2, Train_Loss: 10.67569351196289, Test_Loss: 11.069000244140625\n",
      "Epoch: 2, Train_Loss: 10.627135276794434, Test_Loss: 12.883304595947266\n",
      "Epoch: 2, Train_Loss: 10.553330421447754, Test_Loss: 10.493292808532715 *\n",
      "Epoch: 2, Train_Loss: 10.62106704711914, Test_Loss: 10.47693920135498 *\n",
      "Epoch: 2, Train_Loss: 10.500435829162598, Test_Loss: 10.470070838928223 *\n",
      "Epoch: 2, Train_Loss: 10.447110176086426, Test_Loss: 10.458915710449219 *\n",
      "Epoch: 2, Train_Loss: 10.426949501037598, Test_Loss: 10.440678596496582 *\n",
      "Epoch: 2, Train_Loss: 10.416064262390137, Test_Loss: 10.424798965454102 *\n",
      "Epoch: 2, Train_Loss: 10.402183532714844, Test_Loss: 10.408352851867676 *\n",
      "Epoch: 2, Train_Loss: 10.471573829650879, Test_Loss: 10.392050743103027 *\n",
      "Epoch: 2, Train_Loss: 10.493721961975098, Test_Loss: 10.37739372253418 *\n",
      "Epoch: 2, Train_Loss: 10.515986442565918, Test_Loss: 10.362208366394043 *\n",
      "Epoch: 2, Train_Loss: 10.460652351379395, Test_Loss: 10.349289894104004 *\n",
      "Epoch: 2, Train_Loss: 10.561784744262695, Test_Loss: 10.335929870605469 *\n",
      "Epoch: 2, Train_Loss: 10.361449241638184, Test_Loss: 10.32115650177002 *\n",
      "Epoch: 2, Train_Loss: 11.999469757080078, Test_Loss: 10.320585250854492 *\n",
      "Epoch: 2, Train_Loss: 10.312268257141113, Test_Loss: 12.016183853149414\n",
      "Epoch: 2, Train_Loss: 10.285420417785645, Test_Loss: 11.448169708251953 *\n",
      "Epoch: 2, Train_Loss: 10.31081771850586, Test_Loss: 10.275483131408691 *\n",
      "Epoch: 2, Train_Loss: 10.403059005737305, Test_Loss: 10.263801574707031 *\n",
      "Epoch: 2, Train_Loss: 10.35325813293457, Test_Loss: 10.334390640258789\n",
      "Epoch: 2, Train_Loss: 10.27234172821045, Test_Loss: 10.231184005737305 *\n",
      "Epoch: 2, Train_Loss: 10.199317932128906, Test_Loss: 10.212888717651367 *\n",
      "Epoch: 2, Train_Loss: 10.182860374450684, Test_Loss: 10.231770515441895\n",
      "Epoch: 2, Train_Loss: 10.229090690612793, Test_Loss: 10.261726379394531\n",
      "Epoch: 2, Train_Loss: 10.192435264587402, Test_Loss: 10.223938941955566 *\n",
      "Epoch: 2, Train_Loss: 10.137540817260742, Test_Loss: 10.148892402648926 *\n",
      "Epoch: 2, Train_Loss: 10.123239517211914, Test_Loss: 10.133398056030273 *\n",
      "Epoch: 2, Train_Loss: 10.10556697845459, Test_Loss: 10.11824893951416 *\n",
      "Epoch: 2, Train_Loss: 10.092154502868652, Test_Loss: 10.114188194274902 *\n",
      "Epoch: 2, Train_Loss: 10.079033851623535, Test_Loss: 10.091021537780762 *\n",
      "Epoch: 2, Train_Loss: 10.062804222106934, Test_Loss: 10.076456069946289 *\n",
      "Epoch: 2, Train_Loss: 10.052456855773926, Test_Loss: 10.2415771484375\n",
      "Epoch: 2, Train_Loss: 10.036710739135742, Test_Loss: 10.071951866149902 *\n",
      "Epoch: 2, Train_Loss: 10.022004127502441, Test_Loss: 10.226664543151855\n",
      "Epoch: 2, Train_Loss: 10.010939598083496, Test_Loss: 10.102853775024414 *\n",
      "Epoch: 2, Train_Loss: 9.99422550201416, Test_Loss: 10.284204483032227\n",
      "Epoch: 2, Train_Loss: 10.045655250549316, Test_Loss: 10.011800765991211 *\n",
      "Epoch: 2, Train_Loss: 9.982210159301758, Test_Loss: 9.981226921081543 *\n",
      "Epoch: 2, Train_Loss: 9.970311164855957, Test_Loss: 10.42693042755127\n",
      "Epoch: 2, Train_Loss: 9.948588371276855, Test_Loss: 10.424043655395508 *\n",
      "Epoch: 2, Train_Loss: 9.96020793914795, Test_Loss: 10.416993141174316 *\n",
      "Epoch: 2, Train_Loss: 10.034379959106445, Test_Loss: 10.19544506072998 *\n",
      "Epoch: 2, Train_Loss: 9.918974876403809, Test_Loss: 9.908663749694824 *\n",
      "Epoch: 2, Train_Loss: 9.89384937286377, Test_Loss: 10.27215576171875\n",
      "Epoch: 2, Train_Loss: 9.874174118041992, Test_Loss: 10.895498275756836\n",
      "Epoch: 2, Train_Loss: 9.875835418701172, Test_Loss: 10.865235328674316 *\n",
      "Epoch: 2, Train_Loss: 9.890592575073242, Test_Loss: 10.016658782958984 *\n",
      "Epoch: 2, Train_Loss: 9.860064506530762, Test_Loss: 10.35877513885498\n",
      "Epoch: 2, Train_Loss: 9.884486198425293, Test_Loss: 10.361008644104004\n",
      "Epoch: 2, Train_Loss: 9.876349449157715, Test_Loss: 9.844985961914062 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 2\n",
      "Epoch: 2, Train_Loss: 9.792374610900879, Test_Loss: 10.043968200683594\n",
      "Epoch: 2, Train_Loss: 9.76582145690918, Test_Loss: 10.736648559570312\n",
      "Epoch: 2, Train_Loss: 9.765697479248047, Test_Loss: 9.871822357177734 *\n",
      "Epoch: 2, Train_Loss: 9.767986297607422, Test_Loss: 9.768941879272461 *\n",
      "Epoch: 2, Train_Loss: 9.759349822998047, Test_Loss: 9.94515323638916\n",
      "Epoch: 2, Train_Loss: 9.711091041564941, Test_Loss: 10.058075904846191\n",
      "Epoch: 2, Train_Loss: 9.706161499023438, Test_Loss: 9.71890926361084 *\n",
      "Epoch: 2, Train_Loss: 9.695058822631836, Test_Loss: 9.78085994720459\n",
      "Epoch: 2, Train_Loss: 9.719465255737305, Test_Loss: 10.937921524047852\n",
      "Epoch: 2, Train_Loss: 9.696873664855957, Test_Loss: 9.719343185424805 *\n",
      "Epoch: 2, Train_Loss: 9.648893356323242, Test_Loss: 9.644673347473145 *\n",
      "Epoch: 2, Train_Loss: 9.639755249023438, Test_Loss: 9.644232749938965 *\n",
      "Epoch: 2, Train_Loss: 9.646953582763672, Test_Loss: 9.65466594696045\n",
      "Epoch: 2, Train_Loss: 12.317739486694336, Test_Loss: 9.658997535705566\n",
      "Epoch: 2, Train_Loss: 9.598790168762207, Test_Loss: 9.871962547302246\n",
      "Epoch: 2, Train_Loss: 9.705977439880371, Test_Loss: 9.594587326049805 *\n",
      "Epoch: 2, Train_Loss: 9.580827713012695, Test_Loss: 9.565611839294434 *\n",
      "Epoch: 2, Train_Loss: 9.598037719726562, Test_Loss: 9.640297889709473\n",
      "Epoch: 2, Train_Loss: 9.538674354553223, Test_Loss: 12.253966331481934\n",
      "Epoch: 2, Train_Loss: 10.792214393615723, Test_Loss: 9.5194091796875 *\n",
      "Epoch: 2, Train_Loss: 9.89322566986084, Test_Loss: 9.507482528686523 *\n",
      "Epoch: 2, Train_Loss: 11.00671672821045, Test_Loss: 9.493634223937988 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_Loss: 9.640816688537598, Test_Loss: 9.477554321289062 *\n",
      "Epoch: 2, Train_Loss: 10.325852394104004, Test_Loss: 9.465059280395508 *\n",
      "Epoch: 2, Train_Loss: 10.3468656539917, Test_Loss: 9.47387409210205\n",
      "Epoch: 2, Train_Loss: 9.461651802062988, Test_Loss: 9.444701194763184 *\n",
      "Epoch: 2, Train_Loss: 9.78126049041748, Test_Loss: 9.424753189086914 *\n",
      "Epoch: 3, Train_Loss: 9.430906295776367, Test_Loss: 9.411266326904297 *\n",
      "Epoch: 3, Train_Loss: 9.55737590789795, Test_Loss: 9.396008491516113 *\n",
      "Epoch: 3, Train_Loss: 9.668054580688477, Test_Loss: 9.383430480957031 *\n",
      "Epoch: 3, Train_Loss: 9.465123176574707, Test_Loss: 9.372551918029785 *\n",
      "Epoch: 3, Train_Loss: 9.688565254211426, Test_Loss: 9.35798168182373 *\n",
      "Epoch: 3, Train_Loss: 9.5810546875, Test_Loss: 9.347070693969727 *\n",
      "Epoch: 3, Train_Loss: 9.465267181396484, Test_Loss: 10.377713203430176\n",
      "Epoch: 3, Train_Loss: 9.34001636505127, Test_Loss: 11.056100845336914\n",
      "Epoch: 3, Train_Loss: 9.302139282226562, Test_Loss: 9.333185195922852 *\n",
      "Epoch: 3, Train_Loss: 9.292988777160645, Test_Loss: 9.335323333740234\n",
      "Epoch: 3, Train_Loss: 9.359735488891602, Test_Loss: 9.366984367370605\n",
      "Epoch: 3, Train_Loss: 9.361052513122559, Test_Loss: 9.277106285095215 *\n",
      "Epoch: 3, Train_Loss: 9.524728775024414, Test_Loss: 9.272917747497559 *\n",
      "Epoch: 3, Train_Loss: 9.350736618041992, Test_Loss: 9.285440444946289\n",
      "Epoch: 3, Train_Loss: 9.65882682800293, Test_Loss: 9.313854217529297\n",
      "Epoch: 3, Train_Loss: 9.275934219360352, Test_Loss: 9.286216735839844 *\n",
      "Epoch: 3, Train_Loss: 9.859857559204102, Test_Loss: 9.220768928527832 *\n",
      "Epoch: 3, Train_Loss: 9.233336448669434, Test_Loss: 9.20494270324707 *\n",
      "Epoch: 3, Train_Loss: 9.798299789428711, Test_Loss: 9.184690475463867 *\n",
      "Epoch: 3, Train_Loss: 9.772655487060547, Test_Loss: 9.183540344238281 *\n",
      "Epoch: 3, Train_Loss: 9.162491798400879, Test_Loss: 9.157800674438477 *\n",
      "Epoch: 3, Train_Loss: 9.362886428833008, Test_Loss: 9.138419151306152 *\n",
      "Epoch: 3, Train_Loss: 9.164815902709961, Test_Loss: 9.278753280639648\n",
      "Epoch: 3, Train_Loss: 9.116474151611328, Test_Loss: 9.17019271850586 *\n",
      "Epoch: 3, Train_Loss: 17.88736343383789, Test_Loss: 9.254682540893555\n",
      "Epoch: 3, Train_Loss: 11.26490592956543, Test_Loss: 9.202434539794922 *\n",
      "Epoch: 3, Train_Loss: 9.10607624053955, Test_Loss: 9.363921165466309\n",
      "Epoch: 3, Train_Loss: 15.313638687133789, Test_Loss: 9.097115516662598 *\n",
      "Epoch: 3, Train_Loss: 9.095390319824219, Test_Loss: 9.054512977600098 *\n",
      "Epoch: 3, Train_Loss: 12.212318420410156, Test_Loss: 9.445962905883789\n",
      "Epoch: 3, Train_Loss: 9.029449462890625, Test_Loss: 9.474752426147461\n",
      "Epoch: 3, Train_Loss: 9.006118774414062, Test_Loss: 9.468513488769531 *\n",
      "Epoch: 3, Train_Loss: 8.992274284362793, Test_Loss: 9.395514488220215 *\n",
      "Epoch: 3, Train_Loss: 8.980365753173828, Test_Loss: 8.980887413024902 *\n",
      "Epoch: 3, Train_Loss: 8.971735000610352, Test_Loss: 9.19473934173584\n",
      "Epoch: 3, Train_Loss: 8.977700233459473, Test_Loss: 9.854169845581055\n",
      "Epoch: 3, Train_Loss: 8.977374076843262, Test_Loss: 10.119305610656738\n",
      "Epoch: 3, Train_Loss: 8.930924415588379, Test_Loss: 9.255851745605469 *\n",
      "Epoch: 3, Train_Loss: 8.91156005859375, Test_Loss: 9.24361515045166 *\n",
      "Epoch: 3, Train_Loss: 8.902421951293945, Test_Loss: 9.546735763549805\n",
      "Epoch: 3, Train_Loss: 8.892352104187012, Test_Loss: 8.957703590393066 *\n",
      "Epoch: 3, Train_Loss: 8.878287315368652, Test_Loss: 9.083598136901855\n",
      "Epoch: 3, Train_Loss: 8.865520477294922, Test_Loss: 9.917900085449219\n",
      "Epoch: 3, Train_Loss: 8.863901138305664, Test_Loss: 9.050849914550781 *\n",
      "Epoch: 3, Train_Loss: 10.316683769226074, Test_Loss: 8.88836669921875 *\n",
      "Epoch: 3, Train_Loss: 11.955979347229004, Test_Loss: 8.981999397277832\n",
      "Epoch: 3, Train_Loss: 8.816119194030762, Test_Loss: 9.145142555236816\n",
      "Epoch: 3, Train_Loss: 8.82277774810791, Test_Loss: 8.825288772583008 *\n",
      "Epoch: 3, Train_Loss: 8.805315017700195, Test_Loss: 8.893211364746094\n",
      "Epoch: 3, Train_Loss: 9.008277893066406, Test_Loss: 10.114421844482422\n",
      "Epoch: 3, Train_Loss: 8.841775894165039, Test_Loss: 8.89216423034668 *\n",
      "Epoch: 3, Train_Loss: 8.8800630569458, Test_Loss: 8.760157585144043 *\n",
      "Epoch: 3, Train_Loss: 8.900323867797852, Test_Loss: 8.755946159362793 *\n",
      "Epoch: 3, Train_Loss: 8.78669548034668, Test_Loss: 8.800128936767578\n",
      "Epoch: 3, Train_Loss: 8.75519847869873, Test_Loss: 8.757816314697266 *\n",
      "Epoch: 3, Train_Loss: 8.712130546569824, Test_Loss: 8.906336784362793\n",
      "Epoch: 3, Train_Loss: 8.693634033203125, Test_Loss: 8.744841575622559 *\n",
      "Epoch: 3, Train_Loss: 11.480368614196777, Test_Loss: 8.678180694580078 *\n",
      "Epoch: 3, Train_Loss: 8.689393043518066, Test_Loss: 8.664690017700195 *\n",
      "Epoch: 3, Train_Loss: 8.684395790100098, Test_Loss: 11.616841316223145\n",
      "Epoch: 3, Train_Loss: 8.654135704040527, Test_Loss: 8.641021728515625 *\n",
      "Epoch: 3, Train_Loss: 8.643841743469238, Test_Loss: 8.625690460205078 *\n",
      "Epoch: 3, Train_Loss: 8.62735652923584, Test_Loss: 8.622912406921387 *\n",
      "Epoch: 3, Train_Loss: 8.622398376464844, Test_Loss: 8.61707878112793 *\n",
      "Epoch: 3, Train_Loss: 8.603907585144043, Test_Loss: 8.609588623046875 *\n",
      "Epoch: 3, Train_Loss: 8.628300666809082, Test_Loss: 8.585782051086426 *\n",
      "Epoch: 3, Train_Loss: 8.612045288085938, Test_Loss: 8.57714557647705 *\n",
      "Epoch: 3, Train_Loss: 8.571101188659668, Test_Loss: 8.562365531921387 *\n",
      "Epoch: 3, Train_Loss: 8.612918853759766, Test_Loss: 8.548478126525879 *\n",
      "Epoch: 3, Train_Loss: 8.58177375793457, Test_Loss: 8.54216194152832 *\n",
      "Epoch: 3, Train_Loss: 8.608906745910645, Test_Loss: 8.529214859008789 *\n",
      "Epoch: 3, Train_Loss: 8.568422317504883, Test_Loss: 8.517155647277832 *\n",
      "Epoch: 3, Train_Loss: 8.538650512695312, Test_Loss: 8.503595352172852 *\n",
      "Epoch: 3, Train_Loss: 8.492152214050293, Test_Loss: 8.498476028442383 *\n",
      "Epoch: 3, Train_Loss: 8.481107711791992, Test_Loss: 8.824018478393555\n",
      "Epoch: 3, Train_Loss: 10.000492095947266, Test_Loss: 11.11454963684082\n",
      "Epoch: 3, Train_Loss: 9.486944198608398, Test_Loss: 8.45885944366455 *\n",
      "Epoch: 3, Train_Loss: 8.473404884338379, Test_Loss: 8.462150573730469\n",
      "Epoch: 3, Train_Loss: 8.430846214294434, Test_Loss: 8.530112266540527\n",
      "Epoch: 3, Train_Loss: 8.417272567749023, Test_Loss: 8.439011573791504 *\n",
      "Epoch: 3, Train_Loss: 8.4214506149292, Test_Loss: 8.438278198242188 *\n",
      "Epoch: 3, Train_Loss: 8.537765502929688, Test_Loss: 8.41955280303955 *\n",
      "Epoch: 3, Train_Loss: 8.478931427001953, Test_Loss: 8.457584381103516\n",
      "Epoch: 3, Train_Loss: 8.437557220458984, Test_Loss: 8.479580879211426\n",
      "Epoch: 3, Train_Loss: 8.501602172851562, Test_Loss: 8.363005638122559 *\n",
      "Epoch: 3, Train_Loss: 8.36949634552002, Test_Loss: 8.342463493347168 *\n",
      "Epoch: 3, Train_Loss: 8.331612586975098, Test_Loss: 8.331419944763184 *\n",
      "Epoch: 3, Train_Loss: 8.314590454101562, Test_Loss: 8.321381568908691 *\n",
      "Epoch: 3, Train_Loss: 8.308769226074219, Test_Loss: 8.311683654785156 *\n",
      "Epoch: 3, Train_Loss: 8.295036315917969, Test_Loss: 8.298810958862305 *\n",
      "Epoch: 3, Train_Loss: 8.376371383666992, Test_Loss: 8.37641429901123\n",
      "Epoch: 3, Train_Loss: 8.383777618408203, Test_Loss: 8.409029006958008\n",
      "Epoch: 3, Train_Loss: 8.444122314453125, Test_Loss: 8.349544525146484 *\n",
      "Epoch: 3, Train_Loss: 8.355427742004395, Test_Loss: 8.397390365600586\n",
      "Epoch: 3, Train_Loss: 8.465181350708008, Test_Loss: 8.556199073791504\n",
      "Epoch: 3, Train_Loss: 8.450477600097656, Test_Loss: 8.308788299560547 *\n",
      "Epoch: 3, Train_Loss: 9.736211776733398, Test_Loss: 8.225465774536133 *\n",
      "Epoch: 3, Train_Loss: 8.234382629394531, Test_Loss: 8.544827461242676\n",
      "Epoch: 3, Train_Loss: 8.209451675415039, Test_Loss: 8.675026893615723\n",
      "Epoch: 3, Train_Loss: 8.25048542022705, Test_Loss: 8.651299476623535 *\n",
      "Epoch: 3, Train_Loss: 8.316476821899414, Test_Loss: 8.64790153503418 *\n",
      "Epoch: 3, Train_Loss: 8.268108367919922, Test_Loss: 8.164713859558105 *\n",
      "Epoch: 3, Train_Loss: 8.191376686096191, Test_Loss: 8.277044296264648\n",
      "Epoch: 3, Train_Loss: 8.146764755249023, Test_Loss: 8.890337944030762\n",
      "Epoch: 3, Train_Loss: 8.128056526184082, Test_Loss: 9.41568374633789\n",
      "Epoch: 3, Train_Loss: 8.176085472106934, Test_Loss: 8.563750267028809 *\n",
      "Epoch: 3, Train_Loss: 8.124993324279785, Test_Loss: 8.35669231414795 *\n",
      "Epoch: 3, Train_Loss: 8.090463638305664, Test_Loss: 8.935196876525879\n",
      "Epoch: 3, Train_Loss: 8.081645965576172, Test_Loss: 8.147010803222656 *\n",
      "Epoch: 3, Train_Loss: 8.069942474365234, Test_Loss: 8.318086624145508\n",
      "Epoch: 3, Train_Loss: 8.058188438415527, Test_Loss: 8.925390243530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 8.047368049621582, Test_Loss: 8.277873039245605 *\n",
      "Epoch: 3, Train_Loss: 8.043591499328613, Test_Loss: 8.054359436035156 *\n",
      "Epoch: 3, Train_Loss: 8.031806945800781, Test_Loss: 8.152259826660156\n",
      "Epoch: 3, Train_Loss: 8.01379108428955, Test_Loss: 8.506284713745117\n",
      "Epoch: 3, Train_Loss: 8.007747650146484, Test_Loss: 8.071637153625488 *\n",
      "Epoch: 3, Train_Loss: 7.996740818023682, Test_Loss: 8.042013168334961 *\n",
      "Epoch: 3, Train_Loss: 7.982694625854492, Test_Loss: 9.049306869506836\n",
      "Epoch: 3, Train_Loss: 8.02702522277832, Test_Loss: 8.221295356750488 *\n",
      "Epoch: 3, Train_Loss: 7.977680206298828, Test_Loss: 7.963640213012695 *\n",
      "Epoch: 3, Train_Loss: 7.978119373321533, Test_Loss: 7.953664779663086 *\n",
      "Epoch: 3, Train_Loss: 7.948803901672363, Test_Loss: 7.984461307525635\n",
      "Epoch: 3, Train_Loss: 7.977921962738037, Test_Loss: 7.973385334014893 *\n",
      "Epoch: 3, Train_Loss: 8.010188102722168, Test_Loss: 8.146419525146484\n",
      "Epoch: 3, Train_Loss: 7.925747394561768, Test_Loss: 7.9885711669921875 *\n",
      "Epoch: 3, Train_Loss: 7.905923366546631, Test_Loss: 7.894043445587158 *\n",
      "Epoch: 3, Train_Loss: 7.889586448669434, Test_Loss: 7.889978408813477 *\n",
      "Epoch: 3, Train_Loss: 7.913848876953125, Test_Loss: 10.641668319702148\n",
      "Epoch: 3, Train_Loss: 7.911496162414551, Test_Loss: 7.860114097595215 *\n",
      "Epoch: 3, Train_Loss: 7.871413707733154, Test_Loss: 7.849743366241455 *\n",
      "Epoch: 3, Train_Loss: 7.953985691070557, Test_Loss: 7.837154865264893 *\n",
      "Epoch: 3, Train_Loss: 7.8759589195251465, Test_Loss: 7.828749179840088 *\n",
      "Epoch: 3, Train_Loss: 7.824118614196777, Test_Loss: 7.817839622497559 *\n",
      "Epoch: 3, Train_Loss: 7.804200649261475, Test_Loss: 7.818753242492676\n",
      "Epoch: 3, Train_Loss: 7.807098388671875, Test_Loss: 7.803835391998291 *\n",
      "Epoch: 3, Train_Loss: 7.812580585479736, Test_Loss: 7.782492637634277 *\n",
      "Epoch: 3, Train_Loss: 7.7968430519104, Test_Loss: 7.7706298828125 *\n",
      "Epoch: 3, Train_Loss: 7.7628655433654785, Test_Loss: 7.761325359344482 *\n",
      "Epoch: 3, Train_Loss: 7.759566307067871, Test_Loss: 7.751258850097656 *\n",
      "Epoch: 3, Train_Loss: 7.753226280212402, Test_Loss: 7.741959095001221 *\n",
      "Epoch: 3, Train_Loss: 7.779708385467529, Test_Loss: 7.7295966148376465 *\n",
      "Epoch: 3, Train_Loss: 7.757110118865967, Test_Loss: 7.71898889541626 *\n",
      "Epoch: 3, Train_Loss: 7.718438625335693, Test_Loss: 7.744597434997559\n",
      "Epoch: 3, Train_Loss: 7.709259986877441, Test_Loss: 10.539037704467773\n",
      "Epoch: 3, Train_Loss: 7.720871925354004, Test_Loss: 7.69612979888916 *\n",
      "Epoch: 3, Train_Loss: 10.39394760131836, Test_Loss: 7.721588611602783\n",
      "Epoch: 3, Train_Loss: 7.674401760101318, Test_Loss: 7.730639457702637\n",
      "Epoch: 3, Train_Loss: 7.793785572052002, Test_Loss: 7.691976070404053 *\n",
      "Epoch: 3, Train_Loss: 7.660033702850342, Test_Loss: 7.673986911773682 *\n",
      "Epoch: 3, Train_Loss: 7.687309265136719, Test_Loss: 7.646997928619385 *\n",
      "Epoch: 3, Train_Loss: 7.629176616668701, Test_Loss: 7.698905944824219\n",
      "Epoch: 3, Train_Loss: 8.981359481811523, Test_Loss: 7.716446876525879\n",
      "Epoch: 3, Train_Loss: 7.85487699508667, Test_Loss: 7.630714416503906 *\n",
      "Epoch: 3, Train_Loss: 9.220436096191406, Test_Loss: 7.605585098266602 *\n",
      "Epoch: 3, Train_Loss: 7.648401737213135, Test_Loss: 7.590719699859619 *\n",
      "Epoch: 3, Train_Loss: 8.533114433288574, Test_Loss: 7.584505081176758 *\n",
      "Epoch: 3, Train_Loss: 8.350959777832031, Test_Loss: 7.581865310668945 *\n",
      "Epoch: 3, Train_Loss: 7.578243255615234, Test_Loss: 7.5533246994018555 *\n",
      "Epoch: 3, Train_Loss: 7.908262252807617, Test_Loss: 7.58441162109375\n",
      "Epoch: 3, Train_Loss: 7.558756351470947, Test_Loss: 7.685548782348633\n",
      "Epoch: 3, Train_Loss: 7.694922924041748, Test_Loss: 7.62539005279541 *\n",
      "Epoch: 3, Train_Loss: 7.778830051422119, Test_Loss: 7.74643087387085\n",
      "Epoch: 3, Train_Loss: 7.613901615142822, Test_Loss: 7.697242736816406 *\n",
      "Epoch: 3, Train_Loss: 7.861491680145264, Test_Loss: 7.600311279296875 *\n",
      "Epoch: 3, Train_Loss: 7.694454669952393, Test_Loss: 7.492475509643555 *\n",
      "Epoch: 3, Train_Loss: 7.598280906677246, Test_Loss: 7.637297630310059\n",
      "Epoch: 3, Train_Loss: 7.470566749572754, Test_Loss: 7.985843658447266\n",
      "Epoch: 3, Train_Loss: 7.444606304168701, Test_Loss: 8.040653228759766\n",
      "Epoch: 3, Train_Loss: 7.441082000732422, Test_Loss: 7.883541584014893 *\n",
      "Epoch: 3, Train_Loss: 7.506484508514404, Test_Loss: 7.437507152557373 *\n",
      "Epoch: 3, Train_Loss: 7.5262250900268555, Test_Loss: 7.514081001281738\n",
      "Epoch: 3, Train_Loss: 7.648082733154297, Test_Loss: 7.999482154846191\n",
      "Epoch: 3, Train_Loss: 7.527480125427246, Test_Loss: 8.734142303466797\n",
      "Epoch: 3, Train_Loss: 7.796070098876953, Test_Loss: 7.983035087585449 *\n",
      "Epoch: 3, Train_Loss: 7.499689102172852, Test_Loss: 7.494361400604248 *\n",
      "Epoch: 3, Train_Loss: 7.9665632247924805, Test_Loss: 8.326300621032715\n",
      "Epoch: 3, Train_Loss: 7.381178379058838, Test_Loss: 7.42492151260376 *\n",
      "Epoch: 3, Train_Loss: 8.11298656463623, Test_Loss: 7.57786750793457\n",
      "Epoch: 3, Train_Loss: 7.79418420791626, Test_Loss: 8.09053897857666\n",
      "Epoch: 3, Train_Loss: 7.340723991394043, Test_Loss: 7.671146869659424 *\n",
      "Epoch: 3, Train_Loss: 7.537026882171631, Test_Loss: 7.3294548988342285 *\n",
      "Epoch: 3, Train_Loss: 7.349565505981445, Test_Loss: 7.361076831817627\n",
      "Epoch: 3, Train_Loss: 7.377622604370117, Test_Loss: 7.820873260498047\n",
      "Epoch: 3, Train_Loss: 16.034360885620117, Test_Loss: 7.353659629821777 *\n",
      "Epoch: 3, Train_Loss: 9.409497261047363, Test_Loss: 7.315067291259766 *\n",
      "Epoch: 3, Train_Loss: 7.299959659576416, Test_Loss: 8.223103523254395\n",
      "Epoch: 3, Train_Loss: 13.483858108520508, Test_Loss: 7.656093120574951 *\n",
      "Epoch: 3, Train_Loss: 7.584063529968262, Test_Loss: 7.257082939147949 *\n",
      "Epoch: 3, Train_Loss: 10.116193771362305, Test_Loss: 7.238317966461182 *\n",
      "Epoch: 3, Train_Loss: 7.232077121734619, Test_Loss: 7.287029266357422\n",
      "Epoch: 3, Train_Loss: 7.216434955596924, Test_Loss: 7.23992395401001 *\n",
      "Epoch: 3, Train_Loss: 7.206379413604736, Test_Loss: 7.427945137023926\n",
      "Epoch: 3, Train_Loss: 7.198142051696777, Test_Loss: 7.277909755706787 *\n",
      "Epoch: 3, Train_Loss: 7.1906585693359375, Test_Loss: 7.183051586151123 *\n",
      "Epoch: 3, Train_Loss: 7.202820301055908, Test_Loss: 7.1726908683776855 *\n",
      "Epoch: 3, Train_Loss: 7.198078155517578, Test_Loss: 10.049570083618164\n",
      "Epoch: 3, Train_Loss: 7.158453941345215, Test_Loss: 7.158082962036133 *\n",
      "Epoch: 3, Train_Loss: 7.142670631408691, Test_Loss: 7.1416521072387695 *\n",
      "Epoch: 3, Train_Loss: 7.137479305267334, Test_Loss: 7.136376857757568 *\n",
      "Epoch: 3, Train_Loss: 7.127880096435547, Test_Loss: 7.135000228881836 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 3\n",
      "Epoch: 3, Train_Loss: 7.11668586730957, Test_Loss: 7.125186920166016 *\n",
      "Epoch: 3, Train_Loss: 7.109018802642822, Test_Loss: 7.115854740142822 *\n",
      "Epoch: 3, Train_Loss: 7.110071659088135, Test_Loss: 7.105291843414307 *\n",
      "Epoch: 3, Train_Loss: 9.968825340270996, Test_Loss: 7.090086936950684 *\n",
      "Epoch: 3, Train_Loss: 8.835220336914062, Test_Loss: 7.082753658294678 *\n",
      "Epoch: 3, Train_Loss: 7.071005821228027, Test_Loss: 7.072899341583252 *\n",
      "Epoch: 3, Train_Loss: 7.0832839012146, Test_Loss: 7.064901828765869 *\n",
      "Epoch: 3, Train_Loss: 7.07880163192749, Test_Loss: 7.055626392364502 *\n",
      "Epoch: 3, Train_Loss: 7.263125419616699, Test_Loss: 7.045073509216309 *\n",
      "Epoch: 3, Train_Loss: 7.0986008644104, Test_Loss: 7.037128925323486 *\n",
      "Epoch: 3, Train_Loss: 7.1508402824401855, Test_Loss: 7.059680461883545\n",
      "Epoch: 3, Train_Loss: 7.160091876983643, Test_Loss: 9.961462020874023\n",
      "Epoch: 3, Train_Loss: 7.04741096496582, Test_Loss: 7.007435321807861 *\n",
      "Epoch: 3, Train_Loss: 7.0374321937561035, Test_Loss: 7.019580841064453\n",
      "Epoch: 3, Train_Loss: 6.9886956214904785, Test_Loss: 7.0458455085754395\n",
      "Epoch: 3, Train_Loss: 6.981102466583252, Test_Loss: 7.040419101715088 *\n",
      "Epoch: 3, Train_Loss: 9.782733917236328, Test_Loss: 7.002135276794434 *\n",
      "Epoch: 3, Train_Loss: 6.974783420562744, Test_Loss: 6.968217372894287 *\n",
      "Epoch: 3, Train_Loss: 6.970749378204346, Test_Loss: 6.996255874633789\n",
      "Epoch: 3, Train_Loss: 6.945991039276123, Test_Loss: 7.0937018394470215\n",
      "Epoch: 3, Train_Loss: 6.94047212600708, Test_Loss: 6.927377223968506 *\n",
      "Epoch: 3, Train_Loss: 6.925807952880859, Test_Loss: 6.914113998413086 *\n",
      "Epoch: 3, Train_Loss: 6.929157257080078, Test_Loss: 6.90250301361084 *\n",
      "Epoch: 3, Train_Loss: 6.900274753570557, Test_Loss: 6.89337682723999 *\n",
      "Epoch: 3, Train_Loss: 6.935202121734619, Test_Loss: 6.890017509460449 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_Loss: 6.913906574249268, Test_Loss: 6.881026268005371 *\n",
      "Epoch: 3, Train_Loss: 6.889857769012451, Test_Loss: 6.889294624328613\n",
      "Epoch: 4, Train_Loss: 6.931304454803467, Test_Loss: 7.066605567932129 *\n",
      "Epoch: 4, Train_Loss: 6.8981451988220215, Test_Loss: 6.895231246948242 *\n",
      "Epoch: 4, Train_Loss: 6.930382251739502, Test_Loss: 7.013702869415283\n",
      "Epoch: 4, Train_Loss: 6.898916721343994, Test_Loss: 7.045437335968018\n",
      "Epoch: 4, Train_Loss: 6.8460917472839355, Test_Loss: 7.00156307220459 *\n",
      "Epoch: 4, Train_Loss: 6.818991184234619, Test_Loss: 6.84827995300293 *\n",
      "Epoch: 4, Train_Loss: 6.81186056137085, Test_Loss: 6.862082481384277\n",
      "Epoch: 4, Train_Loss: 8.858240127563477, Test_Loss: 7.373172283172607\n",
      "Epoch: 4, Train_Loss: 7.2949113845825195, Test_Loss: 7.4022626876831055\n",
      "Epoch: 4, Train_Loss: 6.805962562561035, Test_Loss: 7.285837650299072 *\n",
      "Epoch: 4, Train_Loss: 6.769931793212891, Test_Loss: 6.7876362800598145 *\n",
      "Epoch: 4, Train_Loss: 6.7615766525268555, Test_Loss: 6.828097343444824\n",
      "Epoch: 4, Train_Loss: 6.782205581665039, Test_Loss: 7.378986358642578\n",
      "Epoch: 4, Train_Loss: 6.890492916107178, Test_Loss: 7.924010276794434\n",
      "Epoch: 4, Train_Loss: 6.814590930938721, Test_Loss: 7.461845397949219 *\n",
      "Epoch: 4, Train_Loss: 6.806556224822998, Test_Loss: 6.765468120574951 *\n",
      "Epoch: 4, Train_Loss: 6.860431671142578, Test_Loss: 7.657961845397949\n",
      "Epoch: 4, Train_Loss: 6.717104434967041, Test_Loss: 6.793478965759277 *\n",
      "Epoch: 4, Train_Loss: 6.694396018981934, Test_Loss: 6.8932576179504395\n",
      "Epoch: 4, Train_Loss: 6.682063102722168, Test_Loss: 7.330538749694824\n",
      "Epoch: 4, Train_Loss: 6.6774749755859375, Test_Loss: 7.239152908325195 *\n",
      "Epoch: 4, Train_Loss: 6.665661811828613, Test_Loss: 6.694072246551514 *\n",
      "Epoch: 4, Train_Loss: 6.769207000732422, Test_Loss: 6.706969261169434\n",
      "Epoch: 4, Train_Loss: 6.756265640258789, Test_Loss: 7.091274738311768\n",
      "Epoch: 4, Train_Loss: 6.81745719909668, Test_Loss: 6.7118048667907715 *\n",
      "Epoch: 4, Train_Loss: 6.731991291046143, Test_Loss: 6.659943103790283 *\n",
      "Epoch: 4, Train_Loss: 6.837131977081299, Test_Loss: 7.485070705413818\n",
      "Epoch: 4, Train_Loss: 7.140580654144287, Test_Loss: 7.2059502601623535 *\n",
      "Epoch: 4, Train_Loss: 7.818612575531006, Test_Loss: 6.635902404785156 *\n",
      "Epoch: 4, Train_Loss: 6.623479843139648, Test_Loss: 6.605013847351074 *\n",
      "Epoch: 4, Train_Loss: 6.601334095001221, Test_Loss: 6.652154445648193\n",
      "Epoch: 4, Train_Loss: 6.666323184967041, Test_Loss: 6.591979503631592 *\n",
      "Epoch: 4, Train_Loss: 6.704464912414551, Test_Loss: 6.809933662414551\n",
      "Epoch: 4, Train_Loss: 6.674164772033691, Test_Loss: 6.66041374206543 *\n",
      "Epoch: 4, Train_Loss: 6.594457626342773, Test_Loss: 6.552639007568359 *\n",
      "Epoch: 4, Train_Loss: 6.551886558532715, Test_Loss: 6.548044681549072 *\n",
      "Epoch: 4, Train_Loss: 6.535383224487305, Test_Loss: 9.350322723388672\n",
      "Epoch: 4, Train_Loss: 6.601428508758545, Test_Loss: 6.551854610443115 *\n",
      "Epoch: 4, Train_Loss: 6.53599739074707, Test_Loss: 6.519742012023926 *\n",
      "Epoch: 4, Train_Loss: 6.508777618408203, Test_Loss: 6.508574962615967 *\n",
      "Epoch: 4, Train_Loss: 6.49996280670166, Test_Loss: 6.50548791885376 *\n",
      "Epoch: 4, Train_Loss: 6.490351676940918, Test_Loss: 6.494753837585449 *\n",
      "Epoch: 4, Train_Loss: 6.481382846832275, Test_Loss: 6.492214679718018 *\n",
      "Epoch: 4, Train_Loss: 6.474911689758301, Test_Loss: 6.489063262939453 *\n",
      "Epoch: 4, Train_Loss: 6.467865943908691, Test_Loss: 6.463928699493408 *\n",
      "Epoch: 4, Train_Loss: 6.462033271789551, Test_Loss: 6.456452369689941 *\n",
      "Epoch: 4, Train_Loss: 6.447338104248047, Test_Loss: 6.449066638946533 *\n",
      "Epoch: 4, Train_Loss: 6.4442458152771, Test_Loss: 6.439252853393555 *\n",
      "Epoch: 4, Train_Loss: 6.4332098960876465, Test_Loss: 6.431058883666992 *\n",
      "Epoch: 4, Train_Loss: 6.427823066711426, Test_Loss: 6.424760818481445 *\n",
      "Epoch: 4, Train_Loss: 6.476973533630371, Test_Loss: 6.413475036621094 *\n",
      "Epoch: 4, Train_Loss: 6.419522285461426, Test_Loss: 6.4396562576293945\n",
      "Epoch: 4, Train_Loss: 6.421018600463867, Test_Loss: 9.223033905029297\n",
      "Epoch: 4, Train_Loss: 6.402871131896973, Test_Loss: 6.389134883880615 *\n",
      "Epoch: 4, Train_Loss: 6.442426681518555, Test_Loss: 6.417993068695068\n",
      "Epoch: 4, Train_Loss: 6.46013879776001, Test_Loss: 6.418725490570068\n",
      "Epoch: 4, Train_Loss: 6.380342483520508, Test_Loss: 6.4296064376831055\n",
      "Epoch: 4, Train_Loss: 6.3677167892456055, Test_Loss: 6.380407333374023 *\n",
      "Epoch: 4, Train_Loss: 6.355373859405518, Test_Loss: 6.3557329177856445 *\n",
      "Epoch: 4, Train_Loss: 6.3919830322265625, Test_Loss: 6.4007248878479\n",
      "Epoch: 4, Train_Loss: 6.372821807861328, Test_Loss: 6.4564714431762695\n",
      "Epoch: 4, Train_Loss: 6.339762210845947, Test_Loss: 6.335455417633057 *\n",
      "Epoch: 4, Train_Loss: 6.458314895629883, Test_Loss: 6.318233966827393 *\n",
      "Epoch: 4, Train_Loss: 6.329304218292236, Test_Loss: 6.309314727783203 *\n",
      "Epoch: 4, Train_Loss: 6.295098781585693, Test_Loss: 6.298128128051758 *\n",
      "Epoch: 4, Train_Loss: 6.286776065826416, Test_Loss: 6.2950968742370605 *\n",
      "Epoch: 4, Train_Loss: 6.293242454528809, Test_Loss: 6.281462669372559 *\n",
      "Epoch: 4, Train_Loss: 6.293198108673096, Test_Loss: 6.30064058303833\n",
      "Epoch: 4, Train_Loss: 6.2824273109436035, Test_Loss: 6.435835838317871\n",
      "Epoch: 4, Train_Loss: 6.253547668457031, Test_Loss: 6.320024490356445 *\n",
      "Epoch: 4, Train_Loss: 6.254719257354736, Test_Loss: 6.462806224822998\n",
      "Epoch: 4, Train_Loss: 6.249934673309326, Test_Loss: 6.393338203430176 *\n",
      "Epoch: 4, Train_Loss: 6.28089714050293, Test_Loss: 6.406332492828369\n",
      "Epoch: 4, Train_Loss: 6.253973960876465, Test_Loss: 6.2468695640563965 *\n",
      "Epoch: 4, Train_Loss: 6.222826957702637, Test_Loss: 6.23618221282959 *\n",
      "Epoch: 4, Train_Loss: 6.215397834777832, Test_Loss: 6.744529724121094\n",
      "Epoch: 4, Train_Loss: 6.230978488922119, Test_Loss: 6.916719436645508\n",
      "Epoch: 4, Train_Loss: 8.904099464416504, Test_Loss: 6.6197614669799805 *\n",
      "Epoch: 4, Train_Loss: 6.189266681671143, Test_Loss: 6.218633651733398 *\n",
      "Epoch: 4, Train_Loss: 6.312733173370361, Test_Loss: 6.252382755279541\n",
      "Epoch: 4, Train_Loss: 6.175553321838379, Test_Loss: 6.746090412139893\n",
      "Epoch: 4, Train_Loss: 6.207002639770508, Test_Loss: 7.391480445861816\n",
      "Epoch: 4, Train_Loss: 6.192330360412598, Test_Loss: 6.856592655181885 *\n",
      "Epoch: 4, Train_Loss: 7.621517658233643, Test_Loss: 6.151819705963135 *\n",
      "Epoch: 4, Train_Loss: 6.267293453216553, Test_Loss: 7.198103904724121\n",
      "Epoch: 4, Train_Loss: 7.778713226318359, Test_Loss: 6.192630767822266 *\n",
      "Epoch: 4, Train_Loss: 6.148815631866455, Test_Loss: 6.266815185546875\n",
      "Epoch: 4, Train_Loss: 7.1761155128479, Test_Loss: 6.6007304191589355\n",
      "Epoch: 4, Train_Loss: 6.782552242279053, Test_Loss: 6.752213478088379\n",
      "Epoch: 4, Train_Loss: 6.147696495056152, Test_Loss: 6.098682880401611 *\n",
      "Epoch: 4, Train_Loss: 6.395357608795166, Test_Loss: 6.111590385437012\n",
      "Epoch: 4, Train_Loss: 6.111636638641357, Test_Loss: 6.606441974639893\n",
      "Epoch: 4, Train_Loss: 6.206667900085449, Test_Loss: 6.218238353729248 *\n",
      "Epoch: 4, Train_Loss: 6.335809707641602, Test_Loss: 6.086333751678467 *\n",
      "Epoch: 4, Train_Loss: 6.179513931274414, Test_Loss: 6.606695175170898\n",
      "Epoch: 4, Train_Loss: 6.421207427978516, Test_Loss: 6.7454447746276855\n",
      "Epoch: 4, Train_Loss: 6.2214202880859375, Test_Loss: 6.091725826263428 *\n",
      "Epoch: 4, Train_Loss: 6.145700931549072, Test_Loss: 6.029475212097168 *\n",
      "Epoch: 4, Train_Loss: 6.025343418121338, Test_Loss: 6.050271034240723\n",
      "Epoch: 4, Train_Loss: 6.008792400360107, Test_Loss: 6.028747081756592 *\n",
      "Epoch: 4, Train_Loss: 6.003005027770996, Test_Loss: 6.262956619262695\n",
      "Epoch: 4, Train_Loss: 6.081615924835205, Test_Loss: 6.101344585418701 *\n",
      "Epoch: 4, Train_Loss: 6.116031646728516, Test_Loss: 5.988712310791016 *\n",
      "Epoch: 4, Train_Loss: 6.208416938781738, Test_Loss: 5.980716705322266 *\n",
      "Epoch: 4, Train_Loss: 6.119668960571289, Test_Loss: 8.62621784210205\n",
      "Epoch: 4, Train_Loss: 6.331902503967285, Test_Loss: 6.071744918823242 *\n",
      "Epoch: 4, Train_Loss: 6.1425299644470215, Test_Loss: 5.953039646148682 *\n",
      "Epoch: 4, Train_Loss: 6.511352062225342, Test_Loss: 5.947629928588867 *\n",
      "Epoch: 4, Train_Loss: 5.960962295532227, Test_Loss: 5.939696788787842 *\n",
      "Epoch: 4, Train_Loss: 6.810000419616699, Test_Loss: 5.930937767028809 *\n",
      "Epoch: 4, Train_Loss: 6.264821529388428, Test_Loss: 5.928097724914551 *\n",
      "Epoch: 4, Train_Loss: 5.925745487213135, Test_Loss: 5.931480407714844\n",
      "Epoch: 4, Train_Loss: 6.143054008483887, Test_Loss: 5.905434608459473 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_Loss: 5.937314987182617, Test_Loss: 5.897262096405029 *\n",
      "Epoch: 4, Train_Loss: 8.519371032714844, Test_Loss: 5.8900299072265625 *\n",
      "Epoch: 4, Train_Loss: 12.27733325958252, Test_Loss: 5.879483222961426 *\n",
      "Epoch: 4, Train_Loss: 7.767241954803467, Test_Loss: 5.874902725219727 *\n",
      "Epoch: 4, Train_Loss: 5.9500250816345215, Test_Loss: 5.865877151489258 *\n",
      "Epoch: 4, Train_Loss: 12.041580200195312, Test_Loss: 5.8591461181640625 *\n",
      "Epoch: 4, Train_Loss: 7.063356876373291, Test_Loss: 5.8835296630859375\n",
      "Epoch: 4, Train_Loss: 7.859274864196777, Test_Loss: 8.681884765625\n",
      "Epoch: 4, Train_Loss: 5.8394975662231445, Test_Loss: 5.84162712097168 *\n",
      "Epoch: 4, Train_Loss: 5.829268455505371, Test_Loss: 5.85694694519043\n",
      "Epoch: 4, Train_Loss: 5.8214640617370605, Test_Loss: 5.850966453552246 *\n",
      "Epoch: 4, Train_Loss: 5.8150315284729, Test_Loss: 5.905767440795898\n",
      "Epoch: 4, Train_Loss: 5.80836820602417, Test_Loss: 5.837305545806885 *\n",
      "Epoch: 4, Train_Loss: 5.832531929016113, Test_Loss: 5.808876991271973 *\n",
      "Epoch: 4, Train_Loss: 5.817933559417725, Test_Loss: 5.845383167266846\n",
      "Epoch: 4, Train_Loss: 5.782533168792725, Test_Loss: 5.920775413513184\n",
      "Epoch: 4, Train_Loss: 5.770411968231201, Test_Loss: 5.795373916625977 *\n",
      "Epoch: 4, Train_Loss: 5.768428802490234, Test_Loss: 5.764819145202637 *\n",
      "Epoch: 4, Train_Loss: 5.761557579040527, Test_Loss: 5.757322788238525 *\n",
      "Epoch: 4, Train_Loss: 5.7510833740234375, Test_Loss: 5.749242782592773 *\n",
      "Epoch: 4, Train_Loss: 5.7473273277282715, Test_Loss: 5.745890140533447 *\n",
      "Epoch: 4, Train_Loss: 5.747892379760742, Test_Loss: 5.739013671875 *\n",
      "Epoch: 4, Train_Loss: 9.807779312133789, Test_Loss: 5.74962854385376\n",
      "Epoch: 4, Train_Loss: 6.310515403747559, Test_Loss: 5.921358108520508\n",
      "Epoch: 4, Train_Loss: 5.715620517730713, Test_Loss: 5.751621723175049 *\n",
      "Epoch: 4, Train_Loss: 5.730949401855469, Test_Loss: 5.889276027679443\n",
      "Epoch: 4, Train_Loss: 5.751016139984131, Test_Loss: 5.844224452972412 *\n",
      "Epoch: 4, Train_Loss: 5.90138053894043, Test_Loss: 5.937427043914795\n",
      "Epoch: 4, Train_Loss: 5.740878105163574, Test_Loss: 5.72196102142334 *\n",
      "Epoch: 4, Train_Loss: 5.805704116821289, Test_Loss: 5.684085845947266 *\n",
      "Epoch: 4, Train_Loss: 5.810047149658203, Test_Loss: 6.239039897918701\n",
      "Epoch: 4, Train_Loss: 5.700512886047363, Test_Loss: 6.309195041656494\n",
      "Epoch: 4, Train_Loss: 5.696069717407227, Test_Loss: 6.105039119720459 *\n",
      "Epoch: 4, Train_Loss: 5.651058197021484, Test_Loss: 5.741887092590332 *\n",
      "Epoch: 4, Train_Loss: 5.661623001098633, Test_Loss: 5.684828281402588 *\n",
      "Epoch: 4, Train_Loss: 8.436599731445312, Test_Loss: 6.276087760925293\n",
      "Epoch: 4, Train_Loss: 5.64587926864624, Test_Loss: 6.752257347106934\n",
      "Epoch: 4, Train_Loss: 5.640059947967529, Test_Loss: 6.419973850250244 *\n",
      "Epoch: 4, Train_Loss: 5.618535995483398, Test_Loss: 5.615975379943848 *\n",
      "Epoch: 4, Train_Loss: 5.618782043457031, Test_Loss: 6.535567283630371\n",
      "Epoch: 4, Train_Loss: 5.604866027832031, Test_Loss: 5.717323303222656 *\n",
      "Epoch: 4, Train_Loss: 5.60653018951416, Test_Loss: 5.653498649597168 *\n",
      "Epoch: 4, Train_Loss: 5.582315921783447, Test_Loss: 5.997828006744385\n",
      "Epoch: 4, Train_Loss: 5.628360748291016, Test_Loss: 6.524564743041992\n",
      "Epoch: 4, Train_Loss: 5.593539714813232, Test_Loss: 5.587063312530518 *\n",
      "Epoch: 4, Train_Loss: 5.583986759185791, Test_Loss: 5.6170148849487305\n",
      "Epoch: 4, Train_Loss: 5.626418590545654, Test_Loss: 5.905540466308594\n",
      "Epoch: 4, Train_Loss: 5.584996223449707, Test_Loss: 5.680428504943848 *\n",
      "Epoch: 4, Train_Loss: 5.629385471343994, Test_Loss: 5.554518699645996 *\n",
      "Epoch: 4, Train_Loss: 5.592107772827148, Test_Loss: 6.055581092834473\n",
      "Epoch: 4, Train_Loss: 5.544821739196777, Test_Loss: 6.479672431945801\n",
      "Epoch: 4, Train_Loss: 5.5194268226623535, Test_Loss: 5.552157402038574 *\n",
      "Epoch: 4, Train_Loss: 5.51323127746582, Test_Loss: 5.5190534591674805 *\n",
      "Epoch: 4, Train_Loss: 7.837475776672363, Test_Loss: 5.5403361320495605\n",
      "Epoch: 4, Train_Loss: 5.713825702667236, Test_Loss: 5.535369873046875 *\n",
      "Epoch: 4, Train_Loss: 5.511054992675781, Test_Loss: 5.706080913543701\n",
      "Epoch: 4, Train_Loss: 5.480264186859131, Test_Loss: 5.5791425704956055 *\n",
      "Epoch: 4, Train_Loss: 5.474179744720459, Test_Loss: 5.472562789916992 *\n",
      "Epoch: 4, Train_Loss: 5.5037102699279785, Test_Loss: 5.463897228240967 *\n",
      "Epoch: 4, Train_Loss: 5.60090970993042, Test_Loss: 7.736783504486084\n",
      "Epoch: 4, Train_Loss: 5.520593643188477, Test_Loss: 6.050673961639404 *\n",
      "Epoch: 4, Train_Loss: 5.542999744415283, Test_Loss: 5.439963340759277 *\n",
      "Epoch: 4, Train_Loss: 5.591278553009033, Test_Loss: 5.435860633850098 *\n",
      "Epoch: 4, Train_Loss: 5.433556079864502, Test_Loss: 5.434488773345947 *\n",
      "Epoch: 4, Train_Loss: 5.421302795410156, Test_Loss: 5.42997407913208 *\n",
      "Epoch: 4, Train_Loss: 5.412297248840332, Test_Loss: 5.420857906341553 *\n",
      "Epoch: 4, Train_Loss: 5.408778190612793, Test_Loss: 5.42111873626709\n",
      "Epoch: 4, Train_Loss: 5.399679660797119, Test_Loss: 5.403990268707275 *\n",
      "Epoch: 4, Train_Loss: 5.510819911956787, Test_Loss: 5.398229122161865 *\n",
      "Epoch: 4, Train_Loss: 5.488134384155273, Test_Loss: 5.392037391662598 *\n",
      "Epoch: 4, Train_Loss: 5.552121162414551, Test_Loss: 5.381782531738281 *\n",
      "Epoch: 4, Train_Loss: 5.489509582519531, Test_Loss: 5.379188537597656 *\n",
      "Epoch: 4, Train_Loss: 5.566518306732178, Test_Loss: 5.3687591552734375 *\n",
      "Epoch: 4, Train_Loss: 6.221465587615967, Test_Loss: 5.361898422241211 *\n",
      "Epoch: 4, Train_Loss: 6.21309232711792, Test_Loss: 5.386102199554443\n",
      "Epoch: 4, Train_Loss: 5.382580757141113, Test_Loss: 8.22543716430664\n",
      "Epoch: 4, Train_Loss: 5.353055000305176, Test_Loss: 5.354736328125 *\n",
      "Epoch: 4, Train_Loss: 5.42413330078125, Test_Loss: 5.366821765899658\n",
      "Epoch: 4, Train_Loss: 5.443076133728027, Test_Loss: 5.34855842590332 *\n",
      "Epoch: 4, Train_Loss: 5.423743724822998, Test_Loss: 5.412513256072998\n",
      "Epoch: 4, Train_Loss: 5.345959186553955, Test_Loss: 5.336548328399658 *\n",
      "Epoch: 4, Train_Loss: 5.317838191986084, Test_Loss: 5.31679630279541 *\n",
      "Epoch: 4, Train_Loss: 5.299661159515381, Test_Loss: 5.3828511238098145\n",
      "Epoch: 4, Train_Loss: 5.359633445739746, Test_Loss: 5.381279468536377 *\n",
      "Epoch: 4, Train_Loss: 5.295557022094727, Test_Loss: 5.330364227294922 *\n",
      "Epoch: 4, Train_Loss: 5.2764081954956055, Test_Loss: 5.300003528594971 *\n",
      "Epoch: 4, Train_Loss: 5.269775867462158, Test_Loss: 5.292076587677002 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 4\n",
      "Epoch: 4, Train_Loss: 5.262414932250977, Test_Loss: 5.291244983673096 *\n",
      "Epoch: 4, Train_Loss: 5.255147457122803, Test_Loss: 5.291292667388916\n",
      "Epoch: 4, Train_Loss: 5.251763820648193, Test_Loss: 5.262659549713135 *\n",
      "Epoch: 4, Train_Loss: 5.245886325836182, Test_Loss: 5.2792229652404785\n",
      "Epoch: 4, Train_Loss: 5.243626594543457, Test_Loss: 5.411862850189209\n",
      "Epoch: 4, Train_Loss: 5.229053020477295, Test_Loss: 5.2861809730529785 *\n",
      "Epoch: 4, Train_Loss: 5.231870174407959, Test_Loss: 5.4868059158325195\n",
      "Epoch: 4, Train_Loss: 5.221370220184326, Test_Loss: 5.3179473876953125 *\n",
      "Epoch: 4, Train_Loss: 5.217913627624512, Test_Loss: 5.412065029144287\n",
      "Epoch: 4, Train_Loss: 5.261918544769287, Test_Loss: 5.230041980743408 *\n",
      "Epoch: 4, Train_Loss: 5.213005542755127, Test_Loss: 5.21884822845459 *\n",
      "Epoch: 4, Train_Loss: 5.218148231506348, Test_Loss: 5.68308687210083\n",
      "Epoch: 4, Train_Loss: 5.1969122886657715, Test_Loss: 5.859398365020752\n",
      "Epoch: 4, Train_Loss: 5.2504096031188965, Test_Loss: 5.590749263763428 *\n",
      "Epoch: 4, Train_Loss: 5.252805233001709, Test_Loss: 5.292159080505371 *\n",
      "Epoch: 4, Train_Loss: 5.177160263061523, Test_Loss: 5.199214458465576 *\n",
      "Epoch: 4, Train_Loss: 5.167699813842773, Test_Loss: 5.735422134399414\n",
      "Epoch: 4, Train_Loss: 5.1596784591674805, Test_Loss: 6.3476948738098145\n",
      "Epoch: 4, Train_Loss: 5.193955421447754, Test_Loss: 5.8975043296813965 *\n",
      "Epoch: 4, Train_Loss: 5.178617477416992, Test_Loss: 5.154651641845703 *\n",
      "Epoch: 4, Train_Loss: 5.146669387817383, Test_Loss: 6.035350322723389\n",
      "Epoch: 4, Train_Loss: 5.280575752258301, Test_Loss: 5.3587727546691895 *\n",
      "Epoch: 4, Train_Loss: 5.133120536804199, Test_Loss: 5.179372787475586 *\n",
      "Epoch: 4, Train_Loss: 5.110305309295654, Test_Loss: 5.444278240203857\n",
      "Epoch: 4, Train_Loss: 5.102987766265869, Test_Loss: 6.078400135040283\n",
      "Epoch: 4, Train_Loss: 5.114867210388184, Test_Loss: 5.105452060699463 *\n",
      "Epoch: 4, Train_Loss: 5.114687919616699, Test_Loss: 5.121244430541992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 5.106346607208252, Test_Loss: 5.436412811279297 *\n",
      "Epoch: 5, Train_Loss: 5.079785346984863, Test_Loss: 5.32220458984375 *\n",
      "Epoch: 5, Train_Loss: 5.083869934082031, Test_Loss: 5.090025424957275 *\n",
      "Epoch: 5, Train_Loss: 5.0755462646484375, Test_Loss: 5.38615608215332\n",
      "Epoch: 5, Train_Loss: 5.120812892913818, Test_Loss: 6.095430374145508\n",
      "Epoch: 5, Train_Loss: 5.0764288902282715, Test_Loss: 5.102754592895508 *\n",
      "Epoch: 5, Train_Loss: 5.058230400085449, Test_Loss: 5.049021244049072 *\n",
      "Epoch: 5, Train_Loss: 5.051456928253174, Test_Loss: 5.061883926391602\n",
      "Epoch: 5, Train_Loss: 5.1503376960754395, Test_Loss: 5.062173843383789\n",
      "Epoch: 5, Train_Loss: 7.660604000091553, Test_Loss: 5.177276611328125\n",
      "Epoch: 5, Train_Loss: 5.030740261077881, Test_Loss: 5.230134963989258\n",
      "Epoch: 5, Train_Loss: 5.15157413482666, Test_Loss: 5.014708995819092 *\n",
      "Epoch: 5, Train_Loss: 5.017414093017578, Test_Loss: 5.011685371398926 *\n",
      "Epoch: 5, Train_Loss: 5.049732685089111, Test_Loss: 6.364387512207031\n",
      "Epoch: 5, Train_Loss: 5.105504512786865, Test_Loss: 6.445518493652344\n",
      "Epoch: 5, Train_Loss: 6.373627662658691, Test_Loss: 4.987080097198486 *\n",
      "Epoch: 5, Train_Loss: 5.093906879425049, Test_Loss: 4.983685493469238 *\n",
      "Epoch: 5, Train_Loss: 6.650397777557373, Test_Loss: 4.974887847900391 *\n",
      "Epoch: 5, Train_Loss: 4.999510288238525, Test_Loss: 4.970006465911865 *\n",
      "Epoch: 5, Train_Loss: 6.117063045501709, Test_Loss: 4.964407920837402 *\n",
      "Epoch: 5, Train_Loss: 5.489429473876953, Test_Loss: 4.989844799041748\n",
      "Epoch: 5, Train_Loss: 5.0649919509887695, Test_Loss: 4.951033115386963 *\n",
      "Epoch: 5, Train_Loss: 5.204898834228516, Test_Loss: 4.943819999694824 *\n",
      "Epoch: 5, Train_Loss: 5.002403259277344, Test_Loss: 4.936685085296631 *\n",
      "Epoch: 5, Train_Loss: 5.068180561065674, Test_Loss: 4.928574562072754 *\n",
      "Epoch: 5, Train_Loss: 5.218876838684082, Test_Loss: 4.9252400398254395 *\n",
      "Epoch: 5, Train_Loss: 5.071054935455322, Test_Loss: 4.918020248413086 *\n",
      "Epoch: 5, Train_Loss: 5.311614990234375, Test_Loss: 4.911539077758789 *\n",
      "Epoch: 5, Train_Loss: 5.0791802406311035, Test_Loss: 4.921146392822266\n",
      "Epoch: 5, Train_Loss: 5.013968467712402, Test_Loss: 7.328860282897949\n",
      "Epoch: 5, Train_Loss: 4.9020795822143555, Test_Loss: 5.20355224609375 *\n",
      "Epoch: 5, Train_Loss: 4.8858561515808105, Test_Loss: 4.936123371124268 *\n",
      "Epoch: 5, Train_Loss: 4.889289379119873, Test_Loss: 4.918590545654297 *\n",
      "Epoch: 5, Train_Loss: 4.965052127838135, Test_Loss: 4.954318523406982\n",
      "Epoch: 5, Train_Loss: 5.032132148742676, Test_Loss: 4.88995361328125 *\n",
      "Epoch: 5, Train_Loss: 5.0526556968688965, Test_Loss: 4.877285480499268 *\n",
      "Epoch: 5, Train_Loss: 5.059022426605225, Test_Loss: 4.930680274963379\n",
      "Epoch: 5, Train_Loss: 5.171116828918457, Test_Loss: 4.935670852661133\n",
      "Epoch: 5, Train_Loss: 5.080159664154053, Test_Loss: 4.897190570831299 *\n",
      "Epoch: 5, Train_Loss: 5.347390174865723, Test_Loss: 4.8538498878479 *\n",
      "Epoch: 5, Train_Loss: 4.8529534339904785, Test_Loss: 4.848663330078125 *\n",
      "Epoch: 5, Train_Loss: 5.849977016448975, Test_Loss: 4.844794750213623 *\n",
      "Epoch: 5, Train_Loss: 5.018607139587402, Test_Loss: 4.834517002105713 *\n",
      "Epoch: 5, Train_Loss: 4.820220947265625, Test_Loss: 4.818827152252197 *\n",
      "Epoch: 5, Train_Loss: 5.046752452850342, Test_Loss: 4.831141948699951\n",
      "Epoch: 5, Train_Loss: 4.837467193603516, Test_Loss: 4.9830732345581055\n",
      "Epoch: 5, Train_Loss: 11.979452133178711, Test_Loss: 4.830208778381348 *\n",
      "Epoch: 5, Train_Loss: 6.881325721740723, Test_Loss: 5.013772964477539\n",
      "Epoch: 5, Train_Loss: 6.407109260559082, Test_Loss: 4.86618185043335 *\n",
      "Epoch: 5, Train_Loss: 5.220522403717041, Test_Loss: 5.030181884765625\n",
      "Epoch: 5, Train_Loss: 10.568513870239258, Test_Loss: 4.801926136016846 *\n",
      "Epoch: 5, Train_Loss: 7.058566570281982, Test_Loss: 4.78025484085083 *\n",
      "Epoch: 5, Train_Loss: 5.680985927581787, Test_Loss: 5.277307510375977\n",
      "Epoch: 5, Train_Loss: 4.755930423736572, Test_Loss: 5.282057762145996\n",
      "Epoch: 5, Train_Loss: 4.747915744781494, Test_Loss: 5.214234828948975 *\n",
      "Epoch: 5, Train_Loss: 4.740051746368408, Test_Loss: 4.959277629852295 *\n",
      "Epoch: 5, Train_Loss: 4.739562034606934, Test_Loss: 4.747523784637451 *\n",
      "Epoch: 5, Train_Loss: 4.7326202392578125, Test_Loss: 5.301811695098877\n",
      "Epoch: 5, Train_Loss: 4.7627787590026855, Test_Loss: 5.782038688659668\n",
      "Epoch: 5, Train_Loss: 4.738927841186523, Test_Loss: 5.621127128601074 *\n",
      "Epoch: 5, Train_Loss: 4.710827350616455, Test_Loss: 4.785164833068848 *\n",
      "Epoch: 5, Train_Loss: 4.701230525970459, Test_Loss: 5.362682342529297\n",
      "Epoch: 5, Train_Loss: 4.699210166931152, Test_Loss: 5.083371162414551 *\n",
      "Epoch: 5, Train_Loss: 4.692001819610596, Test_Loss: 4.7441840171813965 *\n",
      "Epoch: 5, Train_Loss: 4.685036659240723, Test_Loss: 4.975303649902344\n",
      "Epoch: 5, Train_Loss: 4.688620090484619, Test_Loss: 5.7353668212890625\n",
      "Epoch: 5, Train_Loss: 4.684434413909912, Test_Loss: 4.724844932556152 *\n",
      "Epoch: 5, Train_Loss: 9.184551239013672, Test_Loss: 4.714198112487793 *\n",
      "Epoch: 5, Train_Loss: 4.817576885223389, Test_Loss: 4.921432018280029\n",
      "Epoch: 5, Train_Loss: 4.66222620010376, Test_Loss: 4.926492214202881\n",
      "Epoch: 5, Train_Loss: 4.676098823547363, Test_Loss: 4.6633477210998535 *\n",
      "Epoch: 5, Train_Loss: 4.721191883087158, Test_Loss: 4.869838237762451\n",
      "Epoch: 5, Train_Loss: 4.83368444442749, Test_Loss: 5.899341583251953\n",
      "Epoch: 5, Train_Loss: 4.6959028244018555, Test_Loss: 4.6714653968811035 *\n",
      "Epoch: 5, Train_Loss: 4.759634017944336, Test_Loss: 4.635900497436523 *\n",
      "Epoch: 5, Train_Loss: 4.759624004364014, Test_Loss: 4.653193950653076\n",
      "Epoch: 5, Train_Loss: 4.646124839782715, Test_Loss: 4.6684346199035645\n",
      "Epoch: 5, Train_Loss: 4.653439998626709, Test_Loss: 4.6900200843811035\n",
      "Epoch: 5, Train_Loss: 4.610086917877197, Test_Loss: 4.836718559265137\n",
      "Epoch: 5, Train_Loss: 4.655477523803711, Test_Loss: 4.597763538360596 *\n",
      "Epoch: 5, Train_Loss: 7.385066509246826, Test_Loss: 4.591243743896484 *\n",
      "Epoch: 5, Train_Loss: 4.609145641326904, Test_Loss: 5.196964740753174\n",
      "Epoch: 5, Train_Loss: 4.602160930633545, Test_Loss: 6.896781921386719\n",
      "Epoch: 5, Train_Loss: 4.584562301635742, Test_Loss: 4.575522422790527 *\n",
      "Epoch: 5, Train_Loss: 4.587421417236328, Test_Loss: 4.569106578826904 *\n",
      "Epoch: 5, Train_Loss: 4.574350833892822, Test_Loss: 4.571632385253906\n",
      "Epoch: 5, Train_Loss: 4.578738212585449, Test_Loss: 4.5691728591918945 *\n",
      "Epoch: 5, Train_Loss: 4.556309223175049, Test_Loss: 4.563032627105713 *\n",
      "Epoch: 5, Train_Loss: 4.608251094818115, Test_Loss: 4.553119659423828 *\n",
      "Epoch: 5, Train_Loss: 4.5641632080078125, Test_Loss: 4.549168109893799 *\n",
      "Epoch: 5, Train_Loss: 4.569505214691162, Test_Loss: 4.539550304412842 *\n",
      "Epoch: 5, Train_Loss: 4.612715721130371, Test_Loss: 4.536804676055908 *\n",
      "Epoch: 5, Train_Loss: 4.563168048858643, Test_Loss: 4.5304741859436035 *\n",
      "Epoch: 5, Train_Loss: 4.614014148712158, Test_Loss: 4.524807453155518 *\n",
      "Epoch: 5, Train_Loss: 4.575868606567383, Test_Loss: 4.52122688293457 *\n",
      "Epoch: 5, Train_Loss: 4.52171516418457, Test_Loss: 4.5147857666015625 *\n",
      "Epoch: 5, Train_Loss: 4.508033275604248, Test_Loss: 4.527987480163574\n",
      "Epoch: 5, Train_Loss: 4.505493640899658, Test_Loss: 6.316176891326904\n",
      "Epoch: 5, Train_Loss: 6.992738723754883, Test_Loss: 5.634791374206543 *\n",
      "Epoch: 5, Train_Loss: 4.551151275634766, Test_Loss: 4.501326084136963 *\n",
      "Epoch: 5, Train_Loss: 4.501476287841797, Test_Loss: 4.503305435180664\n",
      "Epoch: 5, Train_Loss: 4.476770401000977, Test_Loss: 4.597838878631592\n",
      "Epoch: 5, Train_Loss: 4.472419261932373, Test_Loss: 4.494989395141602 *\n",
      "Epoch: 5, Train_Loss: 4.517141819000244, Test_Loss: 4.481769561767578 *\n",
      "Epoch: 5, Train_Loss: 4.604107856750488, Test_Loss: 4.500973224639893\n",
      "Epoch: 5, Train_Loss: 4.511739253997803, Test_Loss: 4.552468299865723\n",
      "Epoch: 5, Train_Loss: 4.556652545928955, Test_Loss: 4.524373531341553 *\n",
      "Epoch: 5, Train_Loss: 4.580955982208252, Test_Loss: 4.443217754364014 *\n",
      "Epoch: 5, Train_Loss: 4.440882682800293, Test_Loss: 4.435551643371582 *\n",
      "Epoch: 5, Train_Loss: 4.428992748260498, Test_Loss: 4.429950714111328 *\n",
      "Epoch: 5, Train_Loss: 4.421854496002197, Test_Loss: 4.428861141204834 *\n",
      "Epoch: 5, Train_Loss: 4.421294689178467, Test_Loss: 4.42287015914917 *\n",
      "Epoch: 5, Train_Loss: 4.413392066955566, Test_Loss: 4.42493200302124\n",
      "Epoch: 5, Train_Loss: 4.536722183227539, Test_Loss: 4.610807418823242\n",
      "Epoch: 5, Train_Loss: 4.506867408752441, Test_Loss: 4.433609962463379 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_Loss: 4.558664321899414, Test_Loss: 4.575973987579346\n",
      "Epoch: 5, Train_Loss: 4.5135979652404785, Test_Loss: 4.472909927368164 *\n",
      "Epoch: 5, Train_Loss: 4.57719087600708, Test_Loss: 4.705770969390869\n",
      "Epoch: 5, Train_Loss: 5.591757774353027, Test_Loss: 4.41984748840332 *\n",
      "Epoch: 5, Train_Loss: 4.886711597442627, Test_Loss: 4.3870320320129395 *\n",
      "Epoch: 5, Train_Loss: 4.401640892028809, Test_Loss: 4.868264198303223\n",
      "Epoch: 5, Train_Loss: 4.3769989013671875, Test_Loss: 4.834517478942871 *\n",
      "Epoch: 5, Train_Loss: 4.466875076293945, Test_Loss: 4.8528008460998535\n",
      "Epoch: 5, Train_Loss: 4.473368167877197, Test_Loss: 4.653048038482666 *\n",
      "Epoch: 5, Train_Loss: 4.449813365936279, Test_Loss: 4.3584794998168945 *\n",
      "Epoch: 5, Train_Loss: 4.379715919494629, Test_Loss: 4.757016181945801\n",
      "Epoch: 5, Train_Loss: 4.346206188201904, Test_Loss: 5.369884967803955\n",
      "Epoch: 5, Train_Loss: 4.338606834411621, Test_Loss: 5.332326889038086 *\n",
      "Epoch: 5, Train_Loss: 4.399648666381836, Test_Loss: 4.492440223693848 *\n",
      "Epoch: 5, Train_Loss: 4.329303741455078, Test_Loss: 4.877424716949463\n",
      "Epoch: 5, Train_Loss: 4.3163676261901855, Test_Loss: 4.856378078460693 *\n",
      "Epoch: 5, Train_Loss: 4.3116021156311035, Test_Loss: 4.362065315246582 *\n",
      "Epoch: 5, Train_Loss: 4.305884838104248, Test_Loss: 4.57554292678833\n",
      "Epoch: 5, Train_Loss: 4.300263404846191, Test_Loss: 5.257338047027588\n",
      "Epoch: 5, Train_Loss: 4.299979209899902, Test_Loss: 4.400693893432617 *\n",
      "Epoch: 5, Train_Loss: 4.2943525314331055, Test_Loss: 4.316891670227051 *\n",
      "Epoch: 5, Train_Loss: 4.291835784912109, Test_Loss: 4.517794609069824\n",
      "Epoch: 5, Train_Loss: 4.279717922210693, Test_Loss: 4.681026458740234\n",
      "Epoch: 5, Train_Loss: 4.285203456878662, Test_Loss: 4.311165809631348 *\n",
      "Epoch: 5, Train_Loss: 4.269298076629639, Test_Loss: 4.366905212402344\n",
      "Epoch: 5, Train_Loss: 4.2806396484375, Test_Loss: 5.49595832824707\n",
      "Epoch: 5, Train_Loss: 4.315826416015625, Test_Loss: 4.327626705169678 *\n",
      "Epoch: 5, Train_Loss: 4.262741565704346, Test_Loss: 4.257907390594482 *\n",
      "Epoch: 5, Train_Loss: 4.279621601104736, Test_Loss: 4.264060020446777\n",
      "Epoch: 5, Train_Loss: 4.2659502029418945, Test_Loss: 4.285672187805176\n",
      "Epoch: 5, Train_Loss: 4.317018032073975, Test_Loss: 4.297052383422852\n",
      "Epoch: 5, Train_Loss: 4.306550979614258, Test_Loss: 4.526042938232422\n",
      "Epoch: 5, Train_Loss: 4.241157054901123, Test_Loss: 4.240259170532227 *\n",
      "Epoch: 5, Train_Loss: 4.237095832824707, Test_Loss: 4.22584342956543 *\n",
      "Epoch: 5, Train_Loss: 4.227767467498779, Test_Loss: 4.333870887756348\n",
      "Epoch: 5, Train_Loss: 4.271033763885498, Test_Loss: 6.89388370513916\n",
      "Epoch: 5, Train_Loss: 4.254189968109131, Test_Loss: 4.209603786468506 *\n",
      "Epoch: 5, Train_Loss: 4.222813606262207, Test_Loss: 4.205739498138428 *\n",
      "Epoch: 5, Train_Loss: 4.357978343963623, Test_Loss: 4.198822975158691 *\n",
      "Epoch: 5, Train_Loss: 4.2123613357543945, Test_Loss: 4.1933393478393555 *\n",
      "Epoch: 5, Train_Loss: 4.188005447387695, Test_Loss: 4.191731929779053 *\n",
      "Epoch: 5, Train_Loss: 4.181626319885254, Test_Loss: 4.197966575622559\n",
      "Epoch: 5, Train_Loss: 4.1960954666137695, Test_Loss: 4.1838812828063965 *\n",
      "Epoch: 5, Train_Loss: 4.198071002960205, Test_Loss: 4.171757698059082 *\n",
      "Epoch: 5, Train_Loss: 4.183165550231934, Test_Loss: 4.166626930236816 *\n",
      "Epoch: 5, Train_Loss: 4.163374423980713, Test_Loss: 4.161306858062744 *\n",
      "Epoch: 5, Train_Loss: 4.176451683044434, Test_Loss: 4.156992435455322 *\n",
      "Epoch: 5, Train_Loss: 4.162552356719971, Test_Loss: 4.151564598083496 *\n",
      "Epoch: 5, Train_Loss: 4.213744640350342, Test_Loss: 4.14669132232666 *\n",
      "Epoch: 5, Train_Loss: 4.165219306945801, Test_Loss: 4.146533012390137 *\n",
      "Epoch: 5, Train_Loss: 4.148446559906006, Test_Loss: 5.261358261108398\n",
      "Epoch: 5, Train_Loss: 4.146364212036133, Test_Loss: 5.856590747833252\n",
      "Epoch: 5, Train_Loss: 4.685150146484375, Test_Loss: 4.146605014801025 *\n",
      "Epoch: 5, Train_Loss: 6.318181037902832, Test_Loss: 4.159820556640625\n",
      "Epoch: 5, Train_Loss: 4.132934093475342, Test_Loss: 4.218717575073242\n",
      "Epoch: 5, Train_Loss: 4.2486796379089355, Test_Loss: 4.123884677886963 *\n",
      "Epoch: 5, Train_Loss: 4.11516809463501, Test_Loss: 4.126287937164307\n",
      "Epoch: 5, Train_Loss: 4.1510419845581055, Test_Loss: 4.139949321746826\n",
      "Epoch: 5, Train_Loss: 4.290812015533447, Test_Loss: 4.186086177825928\n",
      "Epoch: 5, Train_Loss: 5.435887336730957, Test_Loss: 4.167458534240723 *\n",
      "Epoch: 5, Train_Loss: 4.24962854385376, Test_Loss: 4.1056108474731445 *\n",
      "Epoch: 5, Train_Loss: 5.729156970977783, Test_Loss: 4.090895175933838 *\n",
      "Epoch: 5, Train_Loss: 4.110722064971924, Test_Loss: 4.088797569274902 *\n",
      "Epoch: 5, Train_Loss: 5.394096374511719, Test_Loss: 4.09337043762207\n",
      "Epoch: 5, Train_Loss: 4.4913811683654785, Test_Loss: 4.083689212799072 *\n",
      "Epoch: 5, Train_Loss: 4.234439849853516, Test_Loss: 4.073856353759766 *\n",
      "Epoch: 5, Train_Loss: 4.237760543823242, Test_Loss: 4.219211578369141\n",
      "Epoch: 5, Train_Loss: 4.12484073638916, Test_Loss: 4.1112165451049805 *\n",
      "Epoch: 5, Train_Loss: 4.175795078277588, Test_Loss: 4.222348213195801\n",
      "Epoch: 5, Train_Loss: 4.327972888946533, Test_Loss: 4.178323745727539 *\n",
      "Epoch: 5, Train_Loss: 4.2012104988098145, Test_Loss: 4.285851001739502\n",
      "Epoch: 5, Train_Loss: 4.450165748596191, Test_Loss: 4.0669846534729 *\n",
      "Epoch: 5, Train_Loss: 4.180582523345947, Test_Loss: 4.0498809814453125 *\n",
      "Epoch: 5, Train_Loss: 4.128115653991699, Test_Loss: 4.4259419441223145\n",
      "Epoch: 5, Train_Loss: 4.027987957000732, Test_Loss: 4.504789352416992\n",
      "Epoch: 5, Train_Loss: 4.01379919052124, Test_Loss: 4.507409572601318\n",
      "Epoch: 5, Train_Loss: 4.023768424987793, Test_Loss: 4.349347114562988 *\n",
      "Epoch: 5, Train_Loss: 4.104113578796387, Test_Loss: 4.015909194946289 *\n",
      "Epoch: 5, Train_Loss: 4.2410664558410645, Test_Loss: 4.24390983581543\n",
      "Epoch: 5, Train_Loss: 4.0942254066467285, Test_Loss: 4.90597677230835\n",
      "Epoch: 5, Train_Loss: 4.247175693511963, Test_Loss: 5.169595241546631\n",
      "Epoch: 5, Train_Loss: 4.259978294372559, Test_Loss: 4.257890701293945 *\n",
      "Epoch: 5, Train_Loss: 4.260854721069336, Test_Loss: 4.3873138427734375\n",
      "Epoch: 5, Train_Loss: 4.423557758331299, Test_Loss: 4.679326057434082\n",
      "Epoch: 5, Train_Loss: 3.992887496948242, Test_Loss: 4.020977020263672 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 5\n",
      "Epoch: 5, Train_Loss: 5.111006259918213, Test_Loss: 4.212979793548584\n",
      "Epoch: 5, Train_Loss: 4.040551662445068, Test_Loss: 4.880350589752197\n",
      "Epoch: 5, Train_Loss: 3.960707187652588, Test_Loss: 4.11407995223999 *\n",
      "Epoch: 5, Train_Loss: 4.196843147277832, Test_Loss: 3.974630832672119 *\n",
      "Epoch: 5, Train_Loss: 3.978827476501465, Test_Loss: 4.123655796051025\n",
      "Epoch: 5, Train_Loss: 12.605226516723633, Test_Loss: 4.376160621643066\n",
      "Epoch: 5, Train_Loss: 4.777151584625244, Test_Loss: 3.987229585647583 *\n",
      "Epoch: 5, Train_Loss: 5.31166410446167, Test_Loss: 3.9969818592071533\n",
      "Epoch: 5, Train_Loss: 4.985241413116455, Test_Loss: 5.1232829093933105\n",
      "Epoch: 5, Train_Loss: 9.172330856323242, Test_Loss: 4.042280673980713 *\n",
      "Epoch: 5, Train_Loss: 6.943023681640625, Test_Loss: 3.919463872909546 *\n",
      "Epoch: 5, Train_Loss: 4.109601974487305, Test_Loss: 3.9233663082122803\n",
      "Epoch: 5, Train_Loss: 3.910050392150879, Test_Loss: 3.961358070373535\n",
      "Epoch: 5, Train_Loss: 3.90407133102417, Test_Loss: 3.953172445297241 *\n",
      "Epoch: 5, Train_Loss: 3.8979992866516113, Test_Loss: 4.120255947113037\n",
      "Epoch: 5, Train_Loss: 3.8979570865631104, Test_Loss: 3.9529991149902344 *\n",
      "Epoch: 5, Train_Loss: 3.892874002456665, Test_Loss: 3.886324405670166 *\n",
      "Epoch: 5, Train_Loss: 3.9286916255950928, Test_Loss: 3.887676954269409\n",
      "Epoch: 5, Train_Loss: 3.895522117614746, Test_Loss: 6.7628703117370605\n",
      "Epoch: 5, Train_Loss: 3.874831199645996, Test_Loss: 3.8746049404144287 *\n",
      "Epoch: 5, Train_Loss: 3.867823839187622, Test_Loss: 3.868950366973877 *\n",
      "Epoch: 5, Train_Loss: 3.8656554222106934, Test_Loss: 3.868806838989258 *\n",
      "Epoch: 5, Train_Loss: 3.8599026203155518, Test_Loss: 3.8686015605926514 *\n",
      "Epoch: 5, Train_Loss: 3.8554115295410156, Test_Loss: 3.8688316345214844\n",
      "Epoch: 5, Train_Loss: 3.8623361587524414, Test_Loss: 3.8594272136688232 *\n",
      "Epoch: 5, Train_Loss: 3.8551182746887207, Test_Loss: 3.854621171951294 *\n",
      "Epoch: 5, Train_Loss: 8.453822135925293, Test_Loss: 3.846014976501465 *\n",
      "Epoch: 6, Train_Loss: 3.9003188610076904, Test_Loss: 3.8391363620758057 *\n",
      "Epoch: 6, Train_Loss: 3.8411340713500977, Test_Loss: 3.839986562728882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 3.853144407272339, Test_Loss: 3.835555076599121 *\n",
      "Epoch: 6, Train_Loss: 3.9153881072998047, Test_Loss: 3.829055070877075 *\n",
      "Epoch: 6, Train_Loss: 3.9917988777160645, Test_Loss: 3.82495379447937 *\n",
      "Epoch: 6, Train_Loss: 3.879920482635498, Test_Loss: 3.825199604034424\n",
      "Epoch: 6, Train_Loss: 3.9326934814453125, Test_Loss: 4.214987754821777\n",
      "Epoch: 6, Train_Loss: 3.932079792022705, Test_Loss: 6.3689470291137695\n",
      "Epoch: 6, Train_Loss: 3.8269026279449463, Test_Loss: 3.8128750324249268 *\n",
      "Epoch: 6, Train_Loss: 3.8358805179595947, Test_Loss: 3.824467658996582\n",
      "Epoch: 6, Train_Loss: 3.7962100505828857, Test_Loss: 3.8950421810150146\n",
      "Epoch: 6, Train_Loss: 4.068911552429199, Test_Loss: 3.8094425201416016 *\n",
      "Epoch: 6, Train_Loss: 6.357515335083008, Test_Loss: 3.8178658485412598\n",
      "Epoch: 6, Train_Loss: 3.801875591278076, Test_Loss: 3.809459686279297 *\n",
      "Epoch: 6, Train_Loss: 3.793980836868286, Test_Loss: 3.8556151390075684\n",
      "Epoch: 6, Train_Loss: 3.777707815170288, Test_Loss: 3.877810478210449\n",
      "Epoch: 6, Train_Loss: 3.7842462062835693, Test_Loss: 3.7710490226745605 *\n",
      "Epoch: 6, Train_Loss: 3.7720086574554443, Test_Loss: 3.759704351425171 *\n",
      "Epoch: 6, Train_Loss: 3.7733821868896484, Test_Loss: 3.7565910816192627 *\n",
      "Epoch: 6, Train_Loss: 3.7543911933898926, Test_Loss: 3.753960609436035 *\n",
      "Epoch: 6, Train_Loss: 3.8146069049835205, Test_Loss: 3.7507498264312744 *\n",
      "Epoch: 6, Train_Loss: 3.7577075958251953, Test_Loss: 3.7470810413360596 *\n",
      "Epoch: 6, Train_Loss: 3.78145694732666, Test_Loss: 3.8381834030151367\n",
      "Epoch: 6, Train_Loss: 3.8110997676849365, Test_Loss: 3.8651235103607178\n",
      "Epoch: 6, Train_Loss: 3.778099536895752, Test_Loss: 3.8126070499420166 *\n",
      "Epoch: 6, Train_Loss: 3.8074865341186523, Test_Loss: 3.861715078353882\n",
      "Epoch: 6, Train_Loss: 3.7815237045288086, Test_Loss: 4.044216156005859\n",
      "Epoch: 6, Train_Loss: 3.7265799045562744, Test_Loss: 3.793877363204956 *\n",
      "Epoch: 6, Train_Loss: 3.7177412509918213, Test_Loss: 3.721404790878296 *\n",
      "Epoch: 6, Train_Loss: 3.7137253284454346, Test_Loss: 4.0622735023498535\n",
      "Epoch: 6, Train_Loss: 6.251441478729248, Test_Loss: 4.182532787322998\n",
      "Epoch: 6, Train_Loss: 3.71730637550354, Test_Loss: 4.147894382476807 *\n",
      "Epoch: 6, Train_Loss: 3.710178852081299, Test_Loss: 4.164449691772461\n",
      "Epoch: 6, Train_Loss: 3.691383123397827, Test_Loss: 3.695115327835083 *\n",
      "Epoch: 6, Train_Loss: 3.6865651607513428, Test_Loss: 3.8172969818115234\n",
      "Epoch: 6, Train_Loss: 3.7431092262268066, Test_Loss: 4.4625067710876465\n",
      "Epoch: 6, Train_Loss: 3.8045108318328857, Test_Loss: 4.959731578826904\n",
      "Epoch: 6, Train_Loss: 3.718797445297241, Test_Loss: 4.12541389465332 *\n",
      "Epoch: 6, Train_Loss: 3.807474374771118, Test_Loss: 3.9200799465179443 *\n",
      "Epoch: 6, Train_Loss: 3.8056797981262207, Test_Loss: 4.432250022888184\n",
      "Epoch: 6, Train_Loss: 3.6621451377868652, Test_Loss: 3.733787775039673 *\n",
      "Epoch: 6, Train_Loss: 3.654505491256714, Test_Loss: 3.8675527572631836\n",
      "Epoch: 6, Train_Loss: 3.651385545730591, Test_Loss: 4.588482856750488\n",
      "Epoch: 6, Train_Loss: 3.649348497390747, Test_Loss: 3.8906326293945312 *\n",
      "Epoch: 6, Train_Loss: 3.646484375, Test_Loss: 3.6722660064697266 *\n",
      "Epoch: 6, Train_Loss: 3.7850661277770996, Test_Loss: 3.764842987060547\n",
      "Epoch: 6, Train_Loss: 3.7473292350769043, Test_Loss: 4.052887916564941\n",
      "Epoch: 6, Train_Loss: 3.7673370838165283, Test_Loss: 3.6831560134887695 *\n",
      "Epoch: 6, Train_Loss: 3.752673387527466, Test_Loss: 3.6876060962677\n",
      "Epoch: 6, Train_Loss: 3.7926230430603027, Test_Loss: 4.756424903869629\n",
      "Epoch: 6, Train_Loss: 5.025171279907227, Test_Loss: 3.8615305423736572 *\n",
      "Epoch: 6, Train_Loss: 3.8912901878356934, Test_Loss: 3.6182103157043457 *\n",
      "Epoch: 6, Train_Loss: 3.6553902626037598, Test_Loss: 3.6173295974731445 *\n",
      "Epoch: 6, Train_Loss: 3.616007089614868, Test_Loss: 3.650723457336426\n",
      "Epoch: 6, Train_Loss: 3.6977970600128174, Test_Loss: 3.653961420059204\n",
      "Epoch: 6, Train_Loss: 3.6949777603149414, Test_Loss: 3.830965042114258\n",
      "Epoch: 6, Train_Loss: 3.665092706680298, Test_Loss: 3.677025318145752 *\n",
      "Epoch: 6, Train_Loss: 3.6143276691436768, Test_Loss: 3.596923828125 *\n",
      "Epoch: 6, Train_Loss: 3.612121820449829, Test_Loss: 3.5982651710510254\n",
      "Epoch: 6, Train_Loss: 3.587029218673706, Test_Loss: 6.302206039428711\n",
      "Epoch: 6, Train_Loss: 3.6275644302368164, Test_Loss: 3.585592746734619 *\n",
      "Epoch: 6, Train_Loss: 3.575603485107422, Test_Loss: 3.583005666732788 *\n",
      "Epoch: 6, Train_Loss: 3.569784641265869, Test_Loss: 3.572652578353882 *\n",
      "Epoch: 6, Train_Loss: 3.5657405853271484, Test_Loss: 3.5653936862945557 *\n",
      "Epoch: 6, Train_Loss: 3.5612881183624268, Test_Loss: 3.56123948097229 *\n",
      "Epoch: 6, Train_Loss: 3.5598676204681396, Test_Loss: 3.583970308303833\n",
      "Epoch: 6, Train_Loss: 3.560178279876709, Test_Loss: 3.570753335952759 *\n",
      "Epoch: 6, Train_Loss: 3.5594282150268555, Test_Loss: 3.547950029373169 *\n",
      "Epoch: 6, Train_Loss: 3.5563223361968994, Test_Loss: 3.544100761413574 *\n",
      "Epoch: 6, Train_Loss: 3.5393576622009277, Test_Loss: 3.539076566696167 *\n",
      "Epoch: 6, Train_Loss: 3.5498032569885254, Test_Loss: 3.5352678298950195 *\n",
      "Epoch: 6, Train_Loss: 3.5312414169311523, Test_Loss: 3.530924081802368 *\n",
      "Epoch: 6, Train_Loss: 3.552426815032959, Test_Loss: 3.5264298915863037 *\n",
      "Epoch: 6, Train_Loss: 3.5707626342773438, Test_Loss: 3.523761510848999 *\n",
      "Epoch: 6, Train_Loss: 3.5240519046783447, Test_Loss: 3.557391881942749\n",
      "Epoch: 6, Train_Loss: 3.5415451526641846, Test_Loss: 6.322108745574951\n",
      "Epoch: 6, Train_Loss: 3.5338170528411865, Test_Loss: 3.52227520942688 *\n",
      "Epoch: 6, Train_Loss: 3.5858688354492188, Test_Loss: 3.5625016689300537\n",
      "Epoch: 6, Train_Loss: 3.5652198791503906, Test_Loss: 3.5656168460845947\n",
      "Epoch: 6, Train_Loss: 3.5127604007720947, Test_Loss: 3.5282745361328125 *\n",
      "Epoch: 6, Train_Loss: 3.5026495456695557, Test_Loss: 3.523189067840576 *\n",
      "Epoch: 6, Train_Loss: 3.499446153640747, Test_Loss: 3.5059728622436523 *\n",
      "Epoch: 6, Train_Loss: 3.5376968383789062, Test_Loss: 3.5663769245147705\n",
      "Epoch: 6, Train_Loss: 3.528527021408081, Test_Loss: 3.585718870162964\n",
      "Epoch: 6, Train_Loss: 3.490819215774536, Test_Loss: 3.497664451599121 *\n",
      "Epoch: 6, Train_Loss: 3.632509469985962, Test_Loss: 3.480557441711426 *\n",
      "Epoch: 6, Train_Loss: 3.482316017150879, Test_Loss: 3.478088855743408 *\n",
      "Epoch: 6, Train_Loss: 3.466036796569824, Test_Loss: 3.4739162921905518 *\n",
      "Epoch: 6, Train_Loss: 3.4616525173187256, Test_Loss: 3.475426435470581\n",
      "Epoch: 6, Train_Loss: 3.481689929962158, Test_Loss: 3.4615485668182373 *\n",
      "Epoch: 6, Train_Loss: 3.4785006046295166, Test_Loss: 3.492408037185669\n",
      "Epoch: 6, Train_Loss: 3.468740940093994, Test_Loss: 3.622649669647217\n",
      "Epoch: 6, Train_Loss: 3.447406768798828, Test_Loss: 3.525263547897339 *\n",
      "Epoch: 6, Train_Loss: 3.455922842025757, Test_Loss: 3.6323626041412354\n",
      "Epoch: 6, Train_Loss: 3.448513984680176, Test_Loss: 3.6609857082366943\n",
      "Epoch: 6, Train_Loss: 3.4939191341400146, Test_Loss: 3.551003932952881 *\n",
      "Epoch: 6, Train_Loss: 3.4484479427337646, Test_Loss: 3.441596508026123 *\n",
      "Epoch: 6, Train_Loss: 3.4393489360809326, Test_Loss: 3.6133623123168945\n",
      "Epoch: 6, Train_Loss: 3.4402735233306885, Test_Loss: 3.952293872833252\n",
      "Epoch: 6, Train_Loss: 4.407711982727051, Test_Loss: 3.9814133644104004\n",
      "Epoch: 6, Train_Loss: 5.179197311401367, Test_Loss: 3.8741159439086914 *\n",
      "Epoch: 6, Train_Loss: 3.4338161945343018, Test_Loss: 3.4203710556030273 *\n",
      "Epoch: 6, Train_Loss: 3.5341386795043945, Test_Loss: 3.5022053718566895\n",
      "Epoch: 6, Train_Loss: 3.40883469581604, Test_Loss: 4.0094451904296875\n",
      "Epoch: 6, Train_Loss: 3.449341297149658, Test_Loss: 4.720822334289551\n",
      "Epoch: 6, Train_Loss: 3.735130548477173, Test_Loss: 3.9748194217681885 *\n",
      "Epoch: 6, Train_Loss: 4.597606182098389, Test_Loss: 3.519507646560669 *\n",
      "Epoch: 6, Train_Loss: 3.677755117416382, Test_Loss: 4.35321044921875\n",
      "Epoch: 6, Train_Loss: 4.868863582611084, Test_Loss: 3.4495134353637695 *\n",
      "Epoch: 6, Train_Loss: 3.4031615257263184, Test_Loss: 3.622011661529541\n",
      "Epoch: 6, Train_Loss: 4.8251118659973145, Test_Loss: 4.125539779663086\n",
      "Epoch: 6, Train_Loss: 3.633737087249756, Test_Loss: 3.699467182159424 *\n",
      "Epoch: 6, Train_Loss: 3.617945671081543, Test_Loss: 3.3825137615203857 *\n",
      "Epoch: 6, Train_Loss: 3.4775476455688477, Test_Loss: 3.4237022399902344\n",
      "Epoch: 6, Train_Loss: 3.443091869354248, Test_Loss: 3.9287548065185547\n",
      "Epoch: 6, Train_Loss: 3.50500226020813, Test_Loss: 3.447726249694824 *\n",
      "Epoch: 6, Train_Loss: 3.614018440246582, Test_Loss: 3.387307643890381 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_Loss: 3.5197951793670654, Test_Loss: 4.263965129852295\n",
      "Epoch: 6, Train_Loss: 3.759438991546631, Test_Loss: 3.7221078872680664 *\n",
      "Epoch: 6, Train_Loss: 3.4673917293548584, Test_Loss: 3.3571863174438477 *\n",
      "Epoch: 6, Train_Loss: 3.4340388774871826, Test_Loss: 3.340691328048706 *\n",
      "Epoch: 6, Train_Loss: 3.34080171585083, Test_Loss: 3.375373601913452\n",
      "Epoch: 6, Train_Loss: 3.3308889865875244, Test_Loss: 3.3606696128845215 *\n",
      "Epoch: 6, Train_Loss: 3.3516898155212402, Test_Loss: 3.5750656127929688\n",
      "Epoch: 6, Train_Loss: 3.4113101959228516, Test_Loss: 3.422015428543091 *\n",
      "Epoch: 6, Train_Loss: 3.6216442584991455, Test_Loss: 3.320848226547241 *\n",
      "Epoch: 6, Train_Loss: 3.338019847869873, Test_Loss: 3.3218793869018555\n",
      "Epoch: 6, Train_Loss: 3.6163406372070312, Test_Loss: 6.045598983764648\n",
      "Epoch: 6, Train_Loss: 3.534644365310669, Test_Loss: 3.3107733726501465 *\n",
      "Epoch: 6, Train_Loss: 3.6419517993927, Test_Loss: 3.3064873218536377 *\n",
      "Epoch: 6, Train_Loss: 3.7136647701263428, Test_Loss: 3.2994086742401123 *\n",
      "Epoch: 6, Train_Loss: 3.3165111541748047, Test_Loss: 3.2976415157318115 *\n",
      "Epoch: 6, Train_Loss: 4.481122016906738, Test_Loss: 3.293668270111084 *\n",
      "Epoch: 6, Train_Loss: 3.3261098861694336, Test_Loss: 3.300119161605835\n",
      "Epoch: 6, Train_Loss: 3.2871527671813965, Test_Loss: 3.296710729598999 *\n",
      "Epoch: 6, Train_Loss: 3.5311734676361084, Test_Loss: 3.279144763946533 *\n",
      "Epoch: 6, Train_Loss: 3.3045623302459717, Test_Loss: 3.2761309146881104 *\n",
      "Epoch: 6, Train_Loss: 12.026233673095703, Test_Loss: 3.2727487087249756 *\n",
      "Epoch: 6, Train_Loss: 4.279171943664551, Test_Loss: 3.2699472904205322 *\n",
      "Epoch: 6, Train_Loss: 4.400078773498535, Test_Loss: 3.265986442565918 *\n",
      "Epoch: 6, Train_Loss: 5.123323917388916, Test_Loss: 3.261547327041626 *\n",
      "Epoch: 6, Train_Loss: 7.675612926483154, Test_Loss: 3.2589468955993652 *\n",
      "Epoch: 6, Train_Loss: 6.450503826141357, Test_Loss: 3.289090394973755\n",
      "Epoch: 6, Train_Loss: 3.2840776443481445, Test_Loss: 6.111238479614258\n",
      "Epoch: 6, Train_Loss: 3.248626232147217, Test_Loss: 3.251483678817749 *\n",
      "Epoch: 6, Train_Loss: 3.2441470623016357, Test_Loss: 3.284350633621216\n",
      "Epoch: 6, Train_Loss: 3.2409465312957764, Test_Loss: 3.2978768348693848\n",
      "Epoch: 6, Train_Loss: 3.2391586303710938, Test_Loss: 3.2932722568511963 *\n",
      "Epoch: 6, Train_Loss: 3.236417055130005, Test_Loss: 3.2653393745422363 *\n",
      "Epoch: 6, Train_Loss: 3.27878737449646, Test_Loss: 3.2412288188934326 *\n",
      "Epoch: 6, Train_Loss: 3.236355781555176, Test_Loss: 3.284575939178467\n",
      "Epoch: 6, Train_Loss: 3.2217774391174316, Test_Loss: 3.3596298694610596\n",
      "Epoch: 6, Train_Loss: 3.2148966789245605, Test_Loss: 3.2230544090270996 *\n",
      "Epoch: 6, Train_Loss: 3.214566946029663, Test_Loss: 3.2137372493743896 *\n",
      "Epoch: 6, Train_Loss: 3.2095587253570557, Test_Loss: 3.210170269012451 *\n",
      "Epoch: 6, Train_Loss: 3.2049779891967773, Test_Loss: 3.2060539722442627 *\n",
      "Epoch: 6, Train_Loss: 3.2155373096466064, Test_Loss: 3.2091310024261475\n",
      "Epoch: 6, Train_Loss: 3.2073209285736084, Test_Loss: 3.2035024166107178 *\n",
      "Epoch: 6, Train_Loss: 7.8431830406188965, Test_Loss: 3.2201099395751953\n",
      "Epoch: 6, Train_Loss: 3.2176475524902344, Test_Loss: 3.388366937637329\n",
      "Epoch: 6, Train_Loss: 3.1986396312713623, Test_Loss: 3.2369511127471924 *\n",
      "Epoch: 6, Train_Loss: 3.208207130432129, Test_Loss: 3.36240816116333\n",
      "Epoch: 6, Train_Loss: 3.2914462089538574, Test_Loss: 3.4004132747650146\n",
      "Epoch: 6, Train_Loss: 3.3424031734466553, Test_Loss: 3.347663402557373 *\n",
      "Epoch: 6, Train_Loss: 3.25089955329895, Test_Loss: 3.207324266433716 *\n",
      "Epoch: 6, Train_Loss: 3.3034465312957764, Test_Loss: 3.2354888916015625\n",
      "Epoch: 6, Train_Loss: 3.297102451324463, Test_Loss: 3.736072063446045\n",
      "Epoch: 6, Train_Loss: 3.185854911804199, Test_Loss: 3.7876288890838623\n",
      "Epoch: 6, Train_Loss: 3.1933231353759766, Test_Loss: 3.6588549613952637 *\n",
      "Epoch: 6, Train_Loss: 3.159291982650757, Test_Loss: 3.1744325160980225 *\n",
      "Epoch: 6, Train_Loss: 3.8854737281799316, Test_Loss: 3.2191293239593506\n",
      "Epoch: 6, Train_Loss: 5.2661590576171875, Test_Loss: 3.7854092121124268\n",
      "Epoch: 6, Train_Loss: 3.169003486633301, Test_Loss: 4.338831901550293\n",
      "Epoch: 6, Train_Loss: 3.1606552600860596, Test_Loss: 3.877387046813965 *\n",
      "Epoch: 6, Train_Loss: 3.1478235721588135, Test_Loss: 3.1832144260406494 *\n",
      "Epoch: 6, Train_Loss: 3.1538963317871094, Test_Loss: 4.032745838165283\n",
      "Epoch: 6, Train_Loss: 3.1411516666412354, Test_Loss: 3.2292592525482178 *\n",
      "Epoch: 6, Train_Loss: 3.1448814868927, Test_Loss: 3.317904472351074\n",
      "Epoch: 6, Train_Loss: 3.128737211227417, Test_Loss: 3.8011012077331543\n",
      "Epoch: 6, Train_Loss: 3.194826364517212, Test_Loss: 3.680438756942749 *\n",
      "Epoch: 6, Train_Loss: 3.12844181060791, Test_Loss: 3.146804094314575 *\n",
      "Epoch: 6, Train_Loss: 3.164402484893799, Test_Loss: 3.1653966903686523\n",
      "Epoch: 6, Train_Loss: 3.184518575668335, Test_Loss: 3.546013355255127\n",
      "Epoch: 6, Train_Loss: 3.163282871246338, Test_Loss: 3.1700499057769775 *\n",
      "Epoch: 6, Train_Loss: 3.182840347290039, Test_Loss: 3.1353273391723633 *\n",
      "Epoch: 6, Train_Loss: 3.159924030303955, Test_Loss: 3.992873430252075\n",
      "Epoch: 6, Train_Loss: 3.108544111251831, Test_Loss: 3.671872615814209 *\n",
      "Epoch: 6, Train_Loss: 3.0999886989593506, Test_Loss: 3.1192467212677 *\n",
      "Epoch: 6, Train_Loss: 3.0965042114257812, Test_Loss: 3.100517988204956 *\n",
      "Epoch: 6, Train_Loss: 5.641251087188721, Test_Loss: 3.1570351123809814\n",
      "Epoch: 6, Train_Loss: 3.099426507949829, Test_Loss: 3.099133253097534 *\n",
      "Epoch: 6, Train_Loss: 3.0912349224090576, Test_Loss: 3.304053544998169\n",
      "Epoch: 6, Train_Loss: 3.078691244125366, Test_Loss: 3.1624622344970703 *\n",
      "Epoch: 6, Train_Loss: 3.074855327606201, Test_Loss: 3.070491313934326 *\n",
      "Epoch: 6, Train_Loss: 3.1433587074279785, Test_Loss: 3.0669445991516113 *\n",
      "Epoch: 6, Train_Loss: 3.204941511154175, Test_Loss: 5.910726547241211\n",
      "Epoch: 6, Train_Loss: 3.0988569259643555, Test_Loss: 3.0845460891723633 *\n",
      "Epoch: 6, Train_Loss: 3.1984689235687256, Test_Loss: 3.0574698448181152 *\n",
      "Epoch: 6, Train_Loss: 3.174929141998291, Test_Loss: 3.056060552597046 *\n",
      "Epoch: 6, Train_Loss: 3.0544614791870117, Test_Loss: 3.0611400604248047\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 6\n",
      "Epoch: 6, Train_Loss: 3.0468506813049316, Test_Loss: 3.054548978805542 *\n",
      "Epoch: 6, Train_Loss: 3.044240951538086, Test_Loss: 3.0557401180267334\n",
      "Epoch: 6, Train_Loss: 3.045081853866577, Test_Loss: 3.0539662837982178 *\n",
      "Epoch: 6, Train_Loss: 3.0518884658813477, Test_Loss: 3.041201114654541 *\n",
      "Epoch: 6, Train_Loss: 3.1848902702331543, Test_Loss: 3.038998603820801 *\n",
      "Epoch: 6, Train_Loss: 3.148015260696411, Test_Loss: 3.0362815856933594 *\n",
      "Epoch: 6, Train_Loss: 3.156381368637085, Test_Loss: 3.031949043273926 *\n",
      "Epoch: 6, Train_Loss: 3.1623404026031494, Test_Loss: 3.029388666152954 *\n",
      "Epoch: 6, Train_Loss: 3.184988498687744, Test_Loss: 3.0255064964294434 *\n",
      "Epoch: 6, Train_Loss: 4.577434539794922, Test_Loss: 3.021785259246826 *\n",
      "Epoch: 6, Train_Loss: 3.163801908493042, Test_Loss: 3.052854537963867\n",
      "Epoch: 6, Train_Loss: 3.0557608604431152, Test_Loss: 5.86335563659668\n",
      "Epoch: 6, Train_Loss: 3.015716314315796, Test_Loss: 3.011537790298462 *\n",
      "Epoch: 6, Train_Loss: 3.1162686347961426, Test_Loss: 3.0471551418304443\n",
      "Epoch: 6, Train_Loss: 3.110111951828003, Test_Loss: 3.0527231693267822\n",
      "Epoch: 6, Train_Loss: 3.074151039123535, Test_Loss: 3.0634164810180664\n",
      "Epoch: 6, Train_Loss: 3.02451491355896, Test_Loss: 3.0219268798828125 *\n",
      "Epoch: 6, Train_Loss: 3.0085394382476807, Test_Loss: 3.005213499069214 *\n",
      "Epoch: 6, Train_Loss: 3.0009164810180664, Test_Loss: 3.065408706665039\n",
      "Epoch: 6, Train_Loss: 3.04023814201355, Test_Loss: 3.0971457958221436\n",
      "Epoch: 6, Train_Loss: 2.9824280738830566, Test_Loss: 3.00407338142395 *\n",
      "Epoch: 6, Train_Loss: 2.979156732559204, Test_Loss: 2.995983362197876 *\n",
      "Epoch: 6, Train_Loss: 2.9767258167266846, Test_Loss: 2.9950389862060547 *\n",
      "Epoch: 6, Train_Loss: 2.972712516784668, Test_Loss: 2.9975621700286865\n",
      "Epoch: 6, Train_Loss: 2.9694933891296387, Test_Loss: 2.9923460483551025 *\n",
      "Epoch: 6, Train_Loss: 2.9715538024902344, Test_Loss: 2.976969003677368 *\n",
      "Epoch: 6, Train_Loss: 2.97613525390625, Test_Loss: 3.0071585178375244\n",
      "Epoch: 7, Train_Loss: 2.965806722640991, Test_Loss: 3.128704309463501 *\n",
      "Epoch: 7, Train_Loss: 2.9568755626678467, Test_Loss: 3.032172441482544 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 2.9668033123016357, Test_Loss: 3.1889123916625977\n",
      "Epoch: 7, Train_Loss: 2.950949192047119, Test_Loss: 3.086329936981201 *\n",
      "Epoch: 7, Train_Loss: 2.986315965652466, Test_Loss: 3.1118149757385254\n",
      "Epoch: 7, Train_Loss: 2.9831652641296387, Test_Loss: 2.970348358154297 *\n",
      "Epoch: 7, Train_Loss: 2.9480838775634766, Test_Loss: 2.972825288772583\n",
      "Epoch: 7, Train_Loss: 2.9662506580352783, Test_Loss: 3.4518802165985107\n",
      "Epoch: 7, Train_Loss: 2.960753917694092, Test_Loss: 3.686156749725342\n",
      "Epoch: 7, Train_Loss: 3.017122983932495, Test_Loss: 3.345513343811035 *\n",
      "Epoch: 7, Train_Loss: 2.9865870475769043, Test_Loss: 2.9671788215637207 *\n",
      "Epoch: 7, Train_Loss: 2.9426560401916504, Test_Loss: 3.0144526958465576\n",
      "Epoch: 7, Train_Loss: 2.92574143409729, Test_Loss: 3.4829294681549072\n",
      "Epoch: 7, Train_Loss: 2.9287164211273193, Test_Loss: 4.166855812072754\n",
      "Epoch: 7, Train_Loss: 2.9732744693756104, Test_Loss: 3.6329169273376465 *\n",
      "Epoch: 7, Train_Loss: 2.956845760345459, Test_Loss: 2.9314725399017334 *\n",
      "Epoch: 7, Train_Loss: 2.9253134727478027, Test_Loss: 3.9522056579589844\n",
      "Epoch: 7, Train_Loss: 3.06911039352417, Test_Loss: 2.9834580421447754 *\n",
      "Epoch: 7, Train_Loss: 2.916771173477173, Test_Loss: 3.068150281906128\n",
      "Epoch: 7, Train_Loss: 2.900156259536743, Test_Loss: 3.409623861312866\n",
      "Epoch: 7, Train_Loss: 2.8977622985839844, Test_Loss: 3.566157341003418\n",
      "Epoch: 7, Train_Loss: 2.918194532394409, Test_Loss: 2.9062960147857666 *\n",
      "Epoch: 7, Train_Loss: 2.907979965209961, Test_Loss: 2.9257426261901855\n",
      "Epoch: 7, Train_Loss: 2.9039366245269775, Test_Loss: 3.356330633163452\n",
      "Epoch: 7, Train_Loss: 2.885772705078125, Test_Loss: 3.0090179443359375 *\n",
      "Epoch: 7, Train_Loss: 2.9003543853759766, Test_Loss: 2.9049267768859863 *\n",
      "Epoch: 7, Train_Loss: 2.8922133445739746, Test_Loss: 3.4910809993743896\n",
      "Epoch: 7, Train_Loss: 2.9346282482147217, Test_Loss: 3.5795555114746094\n",
      "Epoch: 7, Train_Loss: 2.888784408569336, Test_Loss: 2.922422170639038 *\n",
      "Epoch: 7, Train_Loss: 2.8822333812713623, Test_Loss: 2.8751158714294434 *\n",
      "Epoch: 7, Train_Loss: 2.883925199508667, Test_Loss: 2.90268874168396\n",
      "Epoch: 7, Train_Loss: 4.011446952819824, Test_Loss: 2.880866527557373 *\n",
      "Epoch: 7, Train_Loss: 4.45226526260376, Test_Loss: 3.1240172386169434\n",
      "Epoch: 7, Train_Loss: 2.887983798980713, Test_Loss: 2.957995891571045 *\n",
      "Epoch: 7, Train_Loss: 2.9739248752593994, Test_Loss: 2.8530540466308594 *\n",
      "Epoch: 7, Train_Loss: 2.858893394470215, Test_Loss: 2.8504409790039062 *\n",
      "Epoch: 7, Train_Loss: 2.8962113857269287, Test_Loss: 5.509546756744385\n",
      "Epoch: 7, Train_Loss: 3.314469337463379, Test_Loss: 2.939937114715576 *\n",
      "Epoch: 7, Train_Loss: 3.9067654609680176, Test_Loss: 2.8424010276794434 *\n",
      "Epoch: 7, Train_Loss: 3.3153867721557617, Test_Loss: 2.839850664138794 *\n",
      "Epoch: 7, Train_Loss: 4.143415927886963, Test_Loss: 2.83760929107666 *\n",
      "Epoch: 7, Train_Loss: 2.85528302192688, Test_Loss: 2.8335635662078857 *\n",
      "Epoch: 7, Train_Loss: 4.438002586364746, Test_Loss: 2.837615489959717\n",
      "Epoch: 7, Train_Loss: 2.933180570602417, Test_Loss: 2.850466012954712\n",
      "Epoch: 7, Train_Loss: 3.125685930252075, Test_Loss: 2.822070598602295 *\n",
      "Epoch: 7, Train_Loss: 2.886411666870117, Test_Loss: 2.8194141387939453 *\n",
      "Epoch: 7, Train_Loss: 2.8999500274658203, Test_Loss: 2.816771984100342 *\n",
      "Epoch: 7, Train_Loss: 2.998410940170288, Test_Loss: 2.813297748565674 *\n",
      "Epoch: 7, Train_Loss: 3.0328149795532227, Test_Loss: 2.8115358352661133 *\n",
      "Epoch: 7, Train_Loss: 2.9800057411193848, Test_Loss: 2.8079962730407715 *\n",
      "Epoch: 7, Train_Loss: 3.249967575073242, Test_Loss: 2.8044991493225098 *\n",
      "Epoch: 7, Train_Loss: 2.888843297958374, Test_Loss: 2.8334007263183594\n",
      "Epoch: 7, Train_Loss: 2.8843533992767334, Test_Loss: 5.537593841552734\n",
      "Epoch: 7, Train_Loss: 2.803528308868408, Test_Loss: 2.7990033626556396 *\n",
      "Epoch: 7, Train_Loss: 2.794196605682373, Test_Loss: 2.8451943397521973\n",
      "Epoch: 7, Train_Loss: 2.825594663619995, Test_Loss: 2.8284685611724854 *\n",
      "Epoch: 7, Train_Loss: 2.870349884033203, Test_Loss: 2.856933355331421\n",
      "Epoch: 7, Train_Loss: 3.1012659072875977, Test_Loss: 2.808009386062622 *\n",
      "Epoch: 7, Train_Loss: 2.791205644607544, Test_Loss: 2.7939209938049316 *\n",
      "Epoch: 7, Train_Loss: 3.108426809310913, Test_Loss: 2.8577663898468018\n",
      "Epoch: 7, Train_Loss: 2.959719181060791, Test_Loss: 2.8725028038024902\n",
      "Epoch: 7, Train_Loss: 3.1685855388641357, Test_Loss: 2.8048036098480225 *\n",
      "Epoch: 7, Train_Loss: 3.1351869106292725, Test_Loss: 2.778527021408081 *\n",
      "Epoch: 7, Train_Loss: 2.783820390701294, Test_Loss: 2.7788078784942627\n",
      "Epoch: 7, Train_Loss: 3.9399094581604004, Test_Loss: 2.773836851119995 *\n",
      "Epoch: 7, Train_Loss: 2.7808568477630615, Test_Loss: 2.774784803390503\n",
      "Epoch: 7, Train_Loss: 2.7675087451934814, Test_Loss: 2.7627885341644287 *\n",
      "Epoch: 7, Train_Loss: 3.002734899520874, Test_Loss: 2.79077410697937\n",
      "Epoch: 7, Train_Loss: 2.779057502746582, Test_Loss: 2.919478178024292\n",
      "Epoch: 7, Train_Loss: 11.50086498260498, Test_Loss: 2.807966709136963 *\n",
      "Epoch: 7, Train_Loss: 4.017240524291992, Test_Loss: 2.9679114818573\n",
      "Epoch: 7, Train_Loss: 3.60764217376709, Test_Loss: 2.8607966899871826 *\n",
      "Epoch: 7, Train_Loss: 5.479499816894531, Test_Loss: 2.924790143966675\n",
      "Epoch: 7, Train_Loss: 6.276852130889893, Test_Loss: 2.7634437084198 *\n",
      "Epoch: 7, Train_Loss: 5.958735466003418, Test_Loss: 2.7521538734436035 *\n",
      "Epoch: 7, Train_Loss: 2.7477378845214844, Test_Loss: 3.269641876220703\n",
      "Epoch: 7, Train_Loss: 2.7270398139953613, Test_Loss: 3.423290491104126\n",
      "Epoch: 7, Train_Loss: 2.7256979942321777, Test_Loss: 3.1364095211029053 *\n",
      "Epoch: 7, Train_Loss: 2.7226929664611816, Test_Loss: 2.800034523010254 *\n",
      "Epoch: 7, Train_Loss: 2.722893476486206, Test_Loss: 2.7798733711242676 *\n",
      "Epoch: 7, Train_Loss: 2.720479726791382, Test_Loss: 3.322916269302368\n",
      "Epoch: 7, Train_Loss: 2.7617225646972656, Test_Loss: 3.8798184394836426\n",
      "Epoch: 7, Train_Loss: 2.718426465988159, Test_Loss: 3.4524166584014893 *\n",
      "Epoch: 7, Train_Loss: 2.7065277099609375, Test_Loss: 2.7124574184417725 *\n",
      "Epoch: 7, Train_Loss: 2.70266056060791, Test_Loss: 3.6815590858459473\n",
      "Epoch: 7, Train_Loss: 2.7043325901031494, Test_Loss: 2.803651809692383 *\n",
      "Epoch: 7, Train_Loss: 2.6977176666259766, Test_Loss: 2.7755157947540283 *\n",
      "Epoch: 7, Train_Loss: 2.6942567825317383, Test_Loss: 3.1154706478118896\n",
      "Epoch: 7, Train_Loss: 2.7087788581848145, Test_Loss: 3.5883753299713135\n",
      "Epoch: 7, Train_Loss: 2.6981425285339355, Test_Loss: 2.703204393386841 *\n",
      "Epoch: 7, Train_Loss: 7.372320175170898, Test_Loss: 2.7371158599853516\n",
      "Epoch: 7, Train_Loss: 2.700946807861328, Test_Loss: 3.0652425289154053\n",
      "Epoch: 7, Train_Loss: 2.694850444793701, Test_Loss: 2.823984146118164 *\n",
      "Epoch: 7, Train_Loss: 2.702127456665039, Test_Loss: 2.6989364624023438 *\n",
      "Epoch: 7, Train_Loss: 2.798758029937744, Test_Loss: 3.1949715614318848\n",
      "Epoch: 7, Train_Loss: 2.8189706802368164, Test_Loss: 3.6018261909484863\n",
      "Epoch: 7, Train_Loss: 2.754871368408203, Test_Loss: 2.712204933166504 *\n",
      "Epoch: 7, Train_Loss: 2.797673225402832, Test_Loss: 2.6785073280334473 *\n",
      "Epoch: 7, Train_Loss: 2.77960205078125, Test_Loss: 2.7061705589294434\n",
      "Epoch: 7, Train_Loss: 2.6896936893463135, Test_Loss: 2.7018942832946777 *\n",
      "Epoch: 7, Train_Loss: 2.6886768341064453, Test_Loss: 2.8779592514038086\n",
      "Epoch: 7, Train_Loss: 2.65840744972229, Test_Loss: 2.751600742340088 *\n",
      "Epoch: 7, Train_Loss: 3.983356475830078, Test_Loss: 2.651820659637451 *\n",
      "Epoch: 7, Train_Loss: 4.183773994445801, Test_Loss: 2.649226427078247 *\n",
      "Epoch: 7, Train_Loss: 2.670193672180176, Test_Loss: 5.017012596130371\n",
      "Epoch: 7, Train_Loss: 2.6619367599487305, Test_Loss: 3.185044527053833 *\n",
      "Epoch: 7, Train_Loss: 2.653744697570801, Test_Loss: 2.6401379108428955 *\n",
      "Epoch: 7, Train_Loss: 2.659857988357544, Test_Loss: 2.639712333679199 *\n",
      "Epoch: 7, Train_Loss: 2.6453261375427246, Test_Loss: 2.646130084991455\n",
      "Epoch: 7, Train_Loss: 2.6498031616210938, Test_Loss: 2.6440465450286865 *\n",
      "Epoch: 7, Train_Loss: 2.6351447105407715, Test_Loss: 2.639554500579834 *\n",
      "Epoch: 7, Train_Loss: 2.7011067867279053, Test_Loss: 2.639482259750366 *\n",
      "Epoch: 7, Train_Loss: 2.632439613342285, Test_Loss: 2.630317211151123 *\n",
      "Epoch: 7, Train_Loss: 2.682183027267456, Test_Loss: 2.6279549598693848 *\n",
      "Epoch: 7, Train_Loss: 2.6939592361450195, Test_Loss: 2.6277639865875244 *\n",
      "Epoch: 7, Train_Loss: 2.6779592037200928, Test_Loss: 2.6208090782165527 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_Loss: 2.681333065032959, Test_Loss: 2.622694730758667\n",
      "Epoch: 7, Train_Loss: 2.671661853790283, Test_Loss: 2.617966890335083 *\n",
      "Epoch: 7, Train_Loss: 2.6212642192840576, Test_Loss: 2.6157758235931396 *\n",
      "Epoch: 7, Train_Loss: 2.6124870777130127, Test_Loss: 2.6432955265045166\n",
      "Epoch: 7, Train_Loss: 2.6084635257720947, Test_Loss: 5.52033805847168\n",
      "Epoch: 7, Train_Loss: 5.146976470947266, Test_Loss: 2.6180152893066406 *\n",
      "Epoch: 7, Train_Loss: 2.6172537803649902, Test_Loss: 2.625014305114746\n",
      "Epoch: 7, Train_Loss: 2.6064658164978027, Test_Loss: 2.616044044494629 *\n",
      "Epoch: 7, Train_Loss: 2.595214366912842, Test_Loss: 2.7067065238952637\n",
      "Epoch: 7, Train_Loss: 2.591867685317993, Test_Loss: 2.623861074447632 *\n",
      "Epoch: 7, Train_Loss: 2.6717889308929443, Test_Loss: 2.6013195514678955 *\n",
      "Epoch: 7, Train_Loss: 2.721893072128296, Test_Loss: 2.64324951171875\n",
      "Epoch: 7, Train_Loss: 2.607780933380127, Test_Loss: 2.6959826946258545\n",
      "Epoch: 7, Train_Loss: 2.736825704574585, Test_Loss: 2.620270013809204 *\n",
      "Epoch: 7, Train_Loss: 2.6874454021453857, Test_Loss: 2.580684185028076 *\n",
      "Epoch: 7, Train_Loss: 2.576137065887451, Test_Loss: 2.5768468379974365 *\n",
      "Epoch: 7, Train_Loss: 2.5698249340057373, Test_Loss: 2.5744333267211914 *\n",
      "Epoch: 7, Train_Loss: 2.5682523250579834, Test_Loss: 2.5782108306884766\n",
      "Epoch: 7, Train_Loss: 2.568709373474121, Test_Loss: 2.5731213092803955 *\n",
      "Epoch: 7, Train_Loss: 2.5873656272888184, Test_Loss: 2.5886590480804443\n",
      "Epoch: 7, Train_Loss: 2.7178189754486084, Test_Loss: 2.750488519668579\n",
      "Epoch: 7, Train_Loss: 2.6759278774261475, Test_Loss: 2.600928544998169 *\n",
      "Epoch: 7, Train_Loss: 2.664402723312378, Test_Loss: 2.756978750228882\n",
      "Epoch: 7, Train_Loss: 2.6953110694885254, Test_Loss: 2.6656694412231445 *\n",
      "Epoch: 7, Train_Loss: 2.711709976196289, Test_Loss: 2.790811538696289\n",
      "Epoch: 7, Train_Loss: 4.197190284729004, Test_Loss: 2.5806055068969727 *\n",
      "Epoch: 7, Train_Loss: 2.613002061843872, Test_Loss: 2.5603132247924805 *\n",
      "Epoch: 7, Train_Loss: 2.5804150104522705, Test_Loss: 3.0696232318878174\n",
      "Epoch: 7, Train_Loss: 2.5442004203796387, Test_Loss: 3.1897709369659424\n",
      "Epoch: 7, Train_Loss: 2.6610567569732666, Test_Loss: 2.9606516361236572 *\n",
      "Epoch: 7, Train_Loss: 2.641362428665161, Test_Loss: 2.664155960083008 *\n",
      "Epoch: 7, Train_Loss: 2.608250856399536, Test_Loss: 2.572521686553955 *\n",
      "Epoch: 7, Train_Loss: 2.556674003601074, Test_Loss: 3.1305558681488037\n",
      "Epoch: 7, Train_Loss: 2.5396037101745605, Test_Loss: 3.7197468280792236\n",
      "Epoch: 7, Train_Loss: 2.5431649684906006, Test_Loss: 3.266376256942749 *\n",
      "Epoch: 7, Train_Loss: 2.577397108078003, Test_Loss: 2.5389766693115234 *\n",
      "Epoch: 7, Train_Loss: 2.5188002586364746, Test_Loss: 3.4519522190093994\n",
      "Epoch: 7, Train_Loss: 2.5164172649383545, Test_Loss: 2.736870765686035 *\n",
      "Epoch: 7, Train_Loss: 2.514897346496582, Test_Loss: 2.585282564163208 *\n",
      "Epoch: 7, Train_Loss: 2.510995864868164, Test_Loss: 2.8512213230133057\n",
      "Epoch: 7, Train_Loss: 2.5091536045074463, Test_Loss: 3.4304988384246826\n",
      "Epoch: 7, Train_Loss: 2.512075185775757, Test_Loss: 2.514104127883911 *\n",
      "Epoch: 7, Train_Loss: 2.5157418251037598, Test_Loss: 2.5286965370178223\n",
      "Epoch: 7, Train_Loss: 2.5050950050354004, Test_Loss: 2.862312078475952\n",
      "Epoch: 7, Train_Loss: 2.498419761657715, Test_Loss: 2.759371519088745 *\n",
      "Epoch: 7, Train_Loss: 2.511094331741333, Test_Loss: 2.5209226608276367 *\n",
      "Epoch: 7, Train_Loss: 2.4949023723602295, Test_Loss: 2.81136417388916\n",
      "Epoch: 7, Train_Loss: 2.5393245220184326, Test_Loss: 3.460491180419922\n",
      "Epoch: 7, Train_Loss: 2.5196502208709717, Test_Loss: 2.546196937561035 *\n",
      "Epoch: 7, Train_Loss: 2.4931797981262207, Test_Loss: 2.490860939025879 *\n",
      "Epoch: 7, Train_Loss: 2.5122568607330322, Test_Loss: 2.5075881481170654\n",
      "Epoch: 7, Train_Loss: 2.507871389389038, Test_Loss: 2.5118420124053955\n",
      "Epoch: 7, Train_Loss: 2.5699663162231445, Test_Loss: 2.639618158340454\n",
      "Epoch: 7, Train_Loss: 2.5319395065307617, Test_Loss: 2.684936285018921\n",
      "Epoch: 7, Train_Loss: 2.489628314971924, Test_Loss: 2.475447654724121 *\n",
      "Epoch: 7, Train_Loss: 2.4754128456115723, Test_Loss: 2.4728519916534424 *\n",
      "Epoch: 7, Train_Loss: 2.4792444705963135, Test_Loss: 3.9011459350585938\n",
      "Epoch: 7, Train_Loss: 2.5248658657073975, Test_Loss: 3.8192410469055176 *\n",
      "Epoch: 7, Train_Loss: 2.5068140029907227, Test_Loss: 2.4647531509399414 *\n",
      "Epoch: 7, Train_Loss: 2.4775476455688477, Test_Loss: 2.464381217956543 *\n",
      "Epoch: 7, Train_Loss: 2.624904155731201, Test_Loss: 2.4616994857788086 *\n",
      "Epoch: 7, Train_Loss: 2.473447322845459, Test_Loss: 2.4607598781585693 *\n",
      "Epoch: 7, Train_Loss: 2.4543533325195312, Test_Loss: 2.458134412765503 *\n",
      "Epoch: 7, Train_Loss: 2.453325033187866, Test_Loss: 2.473385810852051\n",
      "Epoch: 7, Train_Loss: 2.469773530960083, Test_Loss: 2.4506847858428955 *\n",
      "Epoch: 7, Train_Loss: 2.463926076889038, Test_Loss: 2.4466376304626465 *\n",
      "Epoch: 7, Train_Loss: 2.4593584537506104, Test_Loss: 2.4452905654907227 *\n",
      "Epoch: 7, Train_Loss: 2.442561388015747, Test_Loss: 2.4410386085510254 *\n",
      "Epoch: 7, Train_Loss: 2.4656617641448975, Test_Loss: 2.4412341117858887\n",
      "Epoch: 7, Train_Loss: 2.454580783843994, Test_Loss: 2.4377450942993164 *\n",
      "Epoch: 7, Train_Loss: 2.494119882583618, Test_Loss: 2.4350152015686035 *\n",
      "Epoch: 7, Train_Loss: 2.445403575897217, Test_Loss: 2.4534783363342285\n",
      "Epoch: 7, Train_Loss: 2.4428904056549072, Test_Loss: 4.989767074584961\n",
      "Epoch: 7, Train_Loss: 2.4422309398651123, Test_Loss: 2.6947762966156006 *\n",
      "Epoch: 7, Train_Loss: 3.649289608001709, Test_Loss: 2.4606635570526123 *\n",
      "Epoch: 7, Train_Loss: 3.9651055335998535, Test_Loss: 2.4514410495758057 *\n",
      "Epoch: 7, Train_Loss: 2.4643287658691406, Test_Loss: 2.5158450603485107\n",
      "Epoch: 7, Train_Loss: 2.5183520317077637, Test_Loss: 2.441038131713867 *\n",
      "Epoch: 7, Train_Loss: 2.429513692855835, Test_Loss: 2.4270718097686768 *\n",
      "Epoch: 7, Train_Loss: 2.4543635845184326, Test_Loss: 2.481031656265259\n",
      "Epoch: 7, Train_Loss: 2.988147735595703, Test_Loss: 2.4997129440307617\n",
      "Epoch: 7, Train_Loss: 3.379455089569092, Test_Loss: 2.4599928855895996 *\n",
      "Epoch: 7, Train_Loss: 3.0584540367126465, Test_Loss: 2.416440725326538 *\n",
      "Epoch: 7, Train_Loss: 3.5147409439086914, Test_Loss: 2.415656089782715 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 7\n",
      "Epoch: 7, Train_Loss: 2.4518167972564697, Test_Loss: 2.415318250656128 *\n",
      "Epoch: 7, Train_Loss: 4.008942604064941, Test_Loss: 2.4143402576446533 *\n",
      "Epoch: 7, Train_Loss: 2.4329817295074463, Test_Loss: 2.412980556488037 *\n",
      "Epoch: 7, Train_Loss: 2.720327377319336, Test_Loss: 2.4265904426574707\n",
      "Epoch: 7, Train_Loss: 2.4426345825195312, Test_Loss: 2.5658156871795654\n",
      "Epoch: 7, Train_Loss: 2.4830472469329834, Test_Loss: 2.4372658729553223 *\n",
      "Epoch: 7, Train_Loss: 2.6016812324523926, Test_Loss: 2.650527000427246\n",
      "Epoch: 7, Train_Loss: 2.5675158500671387, Test_Loss: 2.4710302352905273 *\n",
      "Epoch: 7, Train_Loss: 2.57192063331604, Test_Loss: 2.582705020904541\n",
      "Epoch: 7, Train_Loss: 2.8393375873565674, Test_Loss: 2.4048194885253906 *\n",
      "Epoch: 7, Train_Loss: 2.4440720081329346, Test_Loss: 2.4036402702331543 *\n",
      "Epoch: 7, Train_Loss: 2.448091745376587, Test_Loss: 2.841705322265625\n",
      "Epoch: 7, Train_Loss: 2.378051996231079, Test_Loss: 2.9692981243133545\n",
      "Epoch: 7, Train_Loss: 2.371119499206543, Test_Loss: 2.8213233947753906 *\n",
      "Epoch: 7, Train_Loss: 2.404362201690674, Test_Loss: 2.5426697731018066 *\n",
      "Epoch: 7, Train_Loss: 2.4462995529174805, Test_Loss: 2.400115728378296 *\n",
      "Epoch: 7, Train_Loss: 2.672842025756836, Test_Loss: 2.9036340713500977\n",
      "Epoch: 7, Train_Loss: 2.376620292663574, Test_Loss: 3.493424415588379\n",
      "Epoch: 7, Train_Loss: 2.7157790660858154, Test_Loss: 3.1847054958343506 *\n",
      "Epoch: 7, Train_Loss: 2.4910616874694824, Test_Loss: 2.412766933441162 *\n",
      "Epoch: 7, Train_Loss: 2.7935335636138916, Test_Loss: 3.1160390377044678\n",
      "Epoch: 7, Train_Loss: 2.6701948642730713, Test_Loss: 2.7535877227783203 *\n",
      "Epoch: 7, Train_Loss: 2.3684475421905518, Test_Loss: 2.412541389465332 *\n",
      "Epoch: 7, Train_Loss: 3.512301445007324, Test_Loss: 2.6598637104034424\n",
      "Epoch: 7, Train_Loss: 2.358826160430908, Test_Loss: 3.257326364517212\n",
      "Epoch: 7, Train_Loss: 2.3894550800323486, Test_Loss: 2.3717472553253174 *\n",
      "Epoch: 7, Train_Loss: 2.558356761932373, Test_Loss: 2.354555130004883 *\n",
      "Epoch: 8, Train_Loss: 2.3636703491210938, Test_Loss: 2.6487908363342285 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 11.144567489624023, Test_Loss: 2.677917003631592\n",
      "Epoch: 8, Train_Loss: 3.8420205116271973, Test_Loss: 2.3592002391815186 *\n",
      "Epoch: 8, Train_Loss: 2.917048931121826, Test_Loss: 2.5013484954833984\n",
      "Epoch: 8, Train_Loss: 6.17934513092041, Test_Loss: 3.383535385131836\n",
      "Epoch: 8, Train_Loss: 4.775739669799805, Test_Loss: 2.37910532951355 *\n",
      "Epoch: 8, Train_Loss: 5.561348915100098, Test_Loss: 2.3270821571350098 *\n",
      "Epoch: 8, Train_Loss: 2.334730386734009, Test_Loss: 2.3428893089294434\n",
      "Epoch: 8, Train_Loss: 2.318957567214966, Test_Loss: 2.3579862117767334\n",
      "Epoch: 8, Train_Loss: 2.3157689571380615, Test_Loss: 2.3989920616149902\n",
      "Epoch: 8, Train_Loss: 2.3142662048339844, Test_Loss: 2.5809991359710693\n",
      "Epoch: 8, Train_Loss: 2.3149499893188477, Test_Loss: 2.3123996257781982 *\n",
      "Epoch: 8, Train_Loss: 2.311985731124878, Test_Loss: 2.309803009033203 *\n",
      "Epoch: 8, Train_Loss: 2.3595919609069824, Test_Loss: 2.967177629470825\n",
      "Epoch: 8, Train_Loss: 2.310807228088379, Test_Loss: 4.511810302734375\n",
      "Epoch: 8, Train_Loss: 2.3011651039123535, Test_Loss: 2.302187204360962 *\n",
      "Epoch: 8, Train_Loss: 2.29880690574646, Test_Loss: 2.300287961959839 *\n",
      "Epoch: 8, Train_Loss: 2.299489974975586, Test_Loss: 2.3049349784851074\n",
      "Epoch: 8, Train_Loss: 2.294220209121704, Test_Loss: 2.3052804470062256\n",
      "Epoch: 8, Train_Loss: 2.291879653930664, Test_Loss: 2.301121234893799 *\n",
      "Epoch: 8, Train_Loss: 2.3084070682525635, Test_Loss: 2.3017311096191406\n",
      "Epoch: 8, Train_Loss: 2.295008420944214, Test_Loss: 2.2947816848754883 *\n",
      "Epoch: 8, Train_Loss: 6.956562519073486, Test_Loss: 2.2909328937530518 *\n",
      "Epoch: 8, Train_Loss: 2.2933294773101807, Test_Loss: 2.2908079624176025 *\n",
      "Epoch: 8, Train_Loss: 2.2961409091949463, Test_Loss: 2.2865498065948486 *\n",
      "Epoch: 8, Train_Loss: 2.30072283744812, Test_Loss: 2.2867507934570312\n",
      "Epoch: 8, Train_Loss: 2.416153907775879, Test_Loss: 2.283594846725464 *\n",
      "Epoch: 8, Train_Loss: 2.3909311294555664, Test_Loss: 2.283431053161621 *\n",
      "Epoch: 8, Train_Loss: 2.3583953380584717, Test_Loss: 2.3014395236968994\n",
      "Epoch: 8, Train_Loss: 2.3928418159484863, Test_Loss: 4.133486270904541\n",
      "Epoch: 8, Train_Loss: 2.360203981399536, Test_Loss: 3.3272805213928223 *\n",
      "Epoch: 8, Train_Loss: 2.3036272525787354, Test_Loss: 2.286581039428711 *\n",
      "Epoch: 8, Train_Loss: 2.2867133617401123, Test_Loss: 2.28944993019104\n",
      "Epoch: 8, Train_Loss: 2.263859987258911, Test_Loss: 2.379255533218384\n",
      "Epoch: 8, Train_Loss: 4.413732528686523, Test_Loss: 2.283979654312134 *\n",
      "Epoch: 8, Train_Loss: 2.951887369155884, Test_Loss: 2.274387836456299 *\n",
      "Epoch: 8, Train_Loss: 2.2806060314178467, Test_Loss: 2.299593448638916\n",
      "Epoch: 8, Train_Loss: 2.2676706314086914, Test_Loss: 2.3545830249786377\n",
      "Epoch: 8, Train_Loss: 2.264413595199585, Test_Loss: 2.323404312133789 *\n",
      "Epoch: 8, Train_Loss: 2.2655677795410156, Test_Loss: 2.2498514652252197 *\n",
      "Epoch: 8, Train_Loss: 2.2558422088623047, Test_Loss: 2.2478370666503906 *\n",
      "Epoch: 8, Train_Loss: 2.2590649127960205, Test_Loss: 2.2464358806610107 *\n",
      "Epoch: 8, Train_Loss: 2.2494661808013916, Test_Loss: 2.2455601692199707 *\n",
      "Epoch: 8, Train_Loss: 2.3026463985443115, Test_Loss: 2.2421369552612305 *\n",
      "Epoch: 8, Train_Loss: 2.2418642044067383, Test_Loss: 2.2493538856506348\n",
      "Epoch: 8, Train_Loss: 2.3004250526428223, Test_Loss: 2.4432008266448975\n",
      "Epoch: 8, Train_Loss: 2.300147771835327, Test_Loss: 2.263782262802124 *\n",
      "Epoch: 8, Train_Loss: 2.2933874130249023, Test_Loss: 2.4022390842437744\n",
      "Epoch: 8, Train_Loss: 2.287903308868408, Test_Loss: 2.308192014694214 *\n",
      "Epoch: 8, Train_Loss: 2.2874796390533447, Test_Loss: 2.5330960750579834\n",
      "Epoch: 8, Train_Loss: 2.2356226444244385, Test_Loss: 2.2677652835845947 *\n",
      "Epoch: 8, Train_Loss: 2.2259624004364014, Test_Loss: 2.2323808670043945 *\n",
      "Epoch: 8, Train_Loss: 2.2223093509674072, Test_Loss: 2.7312850952148438\n",
      "Epoch: 8, Train_Loss: 4.766237258911133, Test_Loss: 2.6717047691345215 *\n",
      "Epoch: 8, Train_Loss: 2.2348792552948, Test_Loss: 2.7075538635253906\n",
      "Epoch: 8, Train_Loss: 2.220649480819702, Test_Loss: 2.503537654876709 *\n",
      "Epoch: 8, Train_Loss: 2.213484048843384, Test_Loss: 2.220515251159668 *\n",
      "Epoch: 8, Train_Loss: 2.209423065185547, Test_Loss: 2.644063949584961\n",
      "Epoch: 8, Train_Loss: 2.288846731185913, Test_Loss: 3.242274284362793\n",
      "Epoch: 8, Train_Loss: 2.3280739784240723, Test_Loss: 3.1868908405303955 *\n",
      "Epoch: 8, Train_Loss: 2.2237296104431152, Test_Loss: 2.3643388748168945 *\n",
      "Epoch: 8, Train_Loss: 2.3669049739837646, Test_Loss: 2.7452945709228516\n",
      "Epoch: 8, Train_Loss: 2.298727035522461, Test_Loss: 2.7090134620666504 *\n",
      "Epoch: 8, Train_Loss: 2.197976589202881, Test_Loss: 2.249526262283325 *\n",
      "Epoch: 8, Train_Loss: 2.1933541297912598, Test_Loss: 2.456386089324951\n",
      "Epoch: 8, Train_Loss: 2.192103862762451, Test_Loss: 3.1761035919189453\n",
      "Epoch: 8, Train_Loss: 2.19268798828125, Test_Loss: 2.298513650894165 *\n",
      "Epoch: 8, Train_Loss: 2.227321147918701, Test_Loss: 2.221547842025757 *\n",
      "Epoch: 8, Train_Loss: 2.342898368835449, Test_Loss: 2.408817768096924\n",
      "Epoch: 8, Train_Loss: 2.300225257873535, Test_Loss: 2.5258917808532715\n",
      "Epoch: 8, Train_Loss: 2.2775299549102783, Test_Loss: 2.2028253078460693 *\n",
      "Epoch: 8, Train_Loss: 2.3334200382232666, Test_Loss: 2.2866287231445312\n",
      "Epoch: 8, Train_Loss: 2.323991060256958, Test_Loss: 3.39328670501709\n",
      "Epoch: 8, Train_Loss: 3.8077893257141113, Test_Loss: 2.236008644104004 *\n",
      "Epoch: 8, Train_Loss: 2.2148211002349854, Test_Loss: 2.1785125732421875 *\n",
      "Epoch: 8, Train_Loss: 2.210191249847412, Test_Loss: 2.1919405460357666\n",
      "Epoch: 8, Train_Loss: 2.172919988632202, Test_Loss: 2.211552381515503\n",
      "Epoch: 8, Train_Loss: 2.2854905128479004, Test_Loss: 2.227315902709961\n",
      "Epoch: 8, Train_Loss: 2.2608304023742676, Test_Loss: 2.4681742191314697\n",
      "Epoch: 8, Train_Loss: 2.221184015274048, Test_Loss: 2.1711955070495605 *\n",
      "Epoch: 8, Train_Loss: 2.186695098876953, Test_Loss: 2.164531946182251 *\n",
      "Epoch: 8, Train_Loss: 2.1730706691741943, Test_Loss: 2.301699161529541\n",
      "Epoch: 8, Train_Loss: 2.179372787475586, Test_Loss: 4.818572998046875\n",
      "Epoch: 8, Train_Loss: 2.196305990219116, Test_Loss: 2.158508777618408 *\n",
      "Epoch: 8, Train_Loss: 2.155041456222534, Test_Loss: 2.156157970428467 *\n",
      "Epoch: 8, Train_Loss: 2.151137590408325, Test_Loss: 2.154426097869873 *\n",
      "Epoch: 8, Train_Loss: 2.1525747776031494, Test_Loss: 2.153671979904175 *\n",
      "Epoch: 8, Train_Loss: 2.149482250213623, Test_Loss: 2.1527397632598877 *\n",
      "Epoch: 8, Train_Loss: 2.1466853618621826, Test_Loss: 2.162874460220337\n",
      "Epoch: 8, Train_Loss: 2.1520440578460693, Test_Loss: 2.150568962097168 *\n",
      "Epoch: 8, Train_Loss: 2.153900623321533, Test_Loss: 2.141768455505371 *\n",
      "Epoch: 8, Train_Loss: 2.1428234577178955, Test_Loss: 2.1393251419067383 *\n",
      "Epoch: 8, Train_Loss: 2.1373701095581055, Test_Loss: 2.139244318008423 *\n",
      "Epoch: 8, Train_Loss: 2.152174234390259, Test_Loss: 2.137080192565918 *\n",
      "Epoch: 8, Train_Loss: 2.134723424911499, Test_Loss: 2.1350769996643066 *\n",
      "Epoch: 8, Train_Loss: 2.1807310581207275, Test_Loss: 2.131885290145874 *\n",
      "Epoch: 8, Train_Loss: 2.159067153930664, Test_Loss: 2.135087490081787\n",
      "Epoch: 8, Train_Loss: 2.143580675125122, Test_Loss: 3.304574966430664\n",
      "Epoch: 8, Train_Loss: 2.1468539237976074, Test_Loss: 3.7524819374084473\n",
      "Epoch: 8, Train_Loss: 2.1499154567718506, Test_Loss: 2.1508305072784424 *\n",
      "Epoch: 8, Train_Loss: 2.2134156227111816, Test_Loss: 2.166565179824829\n",
      "Epoch: 8, Train_Loss: 2.1762101650238037, Test_Loss: 2.2134411334991455\n",
      "Epoch: 8, Train_Loss: 2.1323392391204834, Test_Loss: 2.1300933361053467 *\n",
      "Epoch: 8, Train_Loss: 2.119839906692505, Test_Loss: 2.1357784271240234\n",
      "Epoch: 8, Train_Loss: 2.1228904724121094, Test_Loss: 2.1589484214782715\n",
      "Epoch: 8, Train_Loss: 2.1678736209869385, Test_Loss: 2.201565980911255\n",
      "Epoch: 8, Train_Loss: 2.1504967212677, Test_Loss: 2.177980661392212 *\n",
      "Epoch: 8, Train_Loss: 2.1272456645965576, Test_Loss: 2.125601291656494 *\n",
      "Epoch: 8, Train_Loss: 2.2668333053588867, Test_Loss: 2.113117218017578 *\n",
      "Epoch: 8, Train_Loss: 2.119194984436035, Test_Loss: 2.115994691848755\n",
      "Epoch: 8, Train_Loss: 2.1022531986236572, Test_Loss: 2.114392042160034 *\n",
      "Epoch: 8, Train_Loss: 2.1034605503082275, Test_Loss: 2.1074106693267822 *\n",
      "Epoch: 8, Train_Loss: 2.1187403202056885, Test_Loss: 2.1057708263397217 *\n",
      "Epoch: 8, Train_Loss: 2.1109704971313477, Test_Loss: 2.2648122310638428\n",
      "Epoch: 8, Train_Loss: 2.108036994934082, Test_Loss: 2.150301218032837 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_Loss: 2.09481143951416, Test_Loss: 2.252528667449951\n",
      "Epoch: 8, Train_Loss: 2.1146016120910645, Test_Loss: 2.203174114227295 *\n",
      "Epoch: 8, Train_Loss: 2.1099050045013428, Test_Loss: 2.3645706176757812\n",
      "Epoch: 8, Train_Loss: 2.1395621299743652, Test_Loss: 2.1243202686309814 *\n",
      "Epoch: 8, Train_Loss: 2.0968828201293945, Test_Loss: 2.1010847091674805 *\n",
      "Epoch: 8, Train_Loss: 2.09403133392334, Test_Loss: 2.4854307174682617\n",
      "Epoch: 8, Train_Loss: 2.0926318168640137, Test_Loss: 2.5566961765289307\n",
      "Epoch: 8, Train_Loss: 3.372338056564331, Test_Loss: 2.5738906860351562\n",
      "Epoch: 8, Train_Loss: 3.5441818237304688, Test_Loss: 2.405705690383911 *\n",
      "Epoch: 8, Train_Loss: 2.132105827331543, Test_Loss: 2.08551025390625 *\n",
      "Epoch: 8, Train_Loss: 2.1562604904174805, Test_Loss: 2.3279383182525635\n",
      "Epoch: 8, Train_Loss: 2.0955100059509277, Test_Loss: 3.0004076957702637\n",
      "Epoch: 8, Train_Loss: 2.1009671688079834, Test_Loss: 3.233637809753418\n",
      "Epoch: 8, Train_Loss: 2.7483277320861816, Test_Loss: 2.3245620727539062 *\n",
      "Epoch: 8, Train_Loss: 2.9438164234161377, Test_Loss: 2.4812965393066406\n",
      "Epoch: 8, Train_Loss: 2.8922595977783203, Test_Loss: 2.759345769882202\n",
      "Epoch: 8, Train_Loss: 2.9605164527893066, Test_Loss: 2.107534885406494 *\n",
      "Epoch: 8, Train_Loss: 2.1892919540405273, Test_Loss: 2.3208487033843994\n",
      "Epoch: 8, Train_Loss: 3.620093584060669, Test_Loss: 2.9287056922912598\n",
      "Epoch: 8, Train_Loss: 2.0804483890533447, Test_Loss: 2.204195499420166 *\n",
      "Epoch: 8, Train_Loss: 2.4031178951263428, Test_Loss: 2.0700831413269043 *\n",
      "Epoch: 8, Train_Loss: 2.08329176902771, Test_Loss: 2.2422664165496826\n",
      "Epoch: 8, Train_Loss: 2.1478517055511475, Test_Loss: 2.508577823638916\n",
      "Epoch: 8, Train_Loss: 2.285832643508911, Test_Loss: 2.113847017288208 *\n",
      "Epoch: 8, Train_Loss: 2.198587417602539, Test_Loss: 2.0979299545288086 *\n",
      "Epoch: 8, Train_Loss: 2.241384506225586, Test_Loss: 3.1046323776245117\n",
      "Epoch: 8, Train_Loss: 2.4634463787078857, Test_Loss: 2.160832405090332 *\n",
      "Epoch: 8, Train_Loss: 2.1185853481292725, Test_Loss: 2.04367733001709 *\n",
      "Epoch: 8, Train_Loss: 2.101226806640625, Test_Loss: 2.048497200012207\n",
      "Epoch: 8, Train_Loss: 2.041417360305786, Test_Loss: 2.0701184272766113\n",
      "Epoch: 8, Train_Loss: 2.036653757095337, Test_Loss: 2.1010401248931885\n",
      "Epoch: 8, Train_Loss: 2.078669786453247, Test_Loss: 2.296865940093994\n",
      "Epoch: 8, Train_Loss: 2.1170854568481445, Test_Loss: 2.0985867977142334 *\n",
      "Epoch: 8, Train_Loss: 2.3246726989746094, Test_Loss: 2.031614303588867 *\n",
      "Epoch: 8, Train_Loss: 2.0512537956237793, Test_Loss: 2.0449814796447754\n",
      "Epoch: 8, Train_Loss: 2.4024927616119385, Test_Loss: 4.734685897827148\n",
      "Epoch: 8, Train_Loss: 2.116284132003784, Test_Loss: 2.0274620056152344 *\n",
      "Epoch: 8, Train_Loss: 2.5197064876556396, Test_Loss: 2.025277853012085 *\n",
      "Epoch: 8, Train_Loss: 2.272937059402466, Test_Loss: 2.0214412212371826 *\n",
      "Epoch: 8, Train_Loss: 2.077763557434082, Test_Loss: 2.0195422172546387 *\n",
      "Epoch: 8, Train_Loss: 3.104489326477051, Test_Loss: 2.0215513706207275\n",
      "Epoch: 8, Train_Loss: 2.027207851409912, Test_Loss: 2.029935121536255\n",
      "Epoch: 8, Train_Loss: 2.1158599853515625, Test_Loss: 2.021432876586914 *\n",
      "Epoch: 8, Train_Loss: 2.172640562057495, Test_Loss: 2.0103602409362793 *\n",
      "Epoch: 8, Train_Loss: 2.0357322692871094, Test_Loss: 2.007750988006592 *\n",
      "Epoch: 8, Train_Loss: 10.80943489074707, Test_Loss: 2.0079057216644287\n",
      "Epoch: 8, Train_Loss: 3.7796127796173096, Test_Loss: 2.0067248344421387 *\n",
      "Epoch: 8, Train_Loss: 2.3405423164367676, Test_Loss: 2.003472328186035 *\n",
      "Epoch: 8, Train_Loss: 6.827417373657227, Test_Loss: 2.001775026321411 *\n",
      "Epoch: 8, Train_Loss: 3.4290881156921387, Test_Loss: 2.0045876502990723\n",
      "Epoch: 8, Train_Loss: 5.225225925445557, Test_Loss: 2.442789077758789\n",
      "Epoch: 8, Train_Loss: 2.010481119155884, Test_Loss: 4.438665390014648\n",
      "Epoch: 8, Train_Loss: 1.9942588806152344, Test_Loss: 2.0113115310668945 *\n",
      "Epoch: 8, Train_Loss: 1.9919666051864624, Test_Loss: 2.0272369384765625\n",
      "Epoch: 8, Train_Loss: 1.991113305091858, Test_Loss: 2.085360050201416\n",
      "Epoch: 8, Train_Loss: 1.992842197418213, Test_Loss: 2.005128860473633 *\n",
      "Epoch: 8, Train_Loss: 1.9924399852752686, Test_Loss: 2.0154995918273926\n",
      "Epoch: 8, Train_Loss: 2.034789800643921, Test_Loss: 2.0165932178497314\n",
      "Epoch: 8, Train_Loss: 1.9871881008148193, Test_Loss: 2.066767930984497\n",
      "Epoch: 8, Train_Loss: 1.9796521663665771, Test_Loss: 2.0684282779693604\n",
      "Epoch: 8, Train_Loss: 1.9781713485717773, Test_Loss: 1.989067792892456 *\n",
      "Epoch: 8, Train_Loss: 1.9795342683792114, Test_Loss: 1.979028344154358 *\n",
      "Epoch: 8, Train_Loss: 1.9743539094924927, Test_Loss: 1.978061556816101 *\n",
      "Epoch: 8, Train_Loss: 1.973006010055542, Test_Loss: 1.9798156023025513\n",
      "Epoch: 8, Train_Loss: 1.9925358295440674, Test_Loss: 1.9785478115081787 *\n",
      "Epoch: 8, Train_Loss: 1.9768961668014526, Test_Loss: 1.9755737781524658 *\n",
      "Epoch: 8, Train_Loss: 6.6461310386657715, Test_Loss: 2.0753557682037354\n",
      "Epoch: 8, Train_Loss: 1.9745121002197266, Test_Loss: 2.087477684020996\n",
      "Epoch: 8, Train_Loss: 1.9809796810150146, Test_Loss: 2.057286500930786 *\n",
      "Epoch: 8, Train_Loss: 1.9842127561569214, Test_Loss: 2.101574182510376\n",
      "Epoch: 8, Train_Loss: 2.1168646812438965, Test_Loss: 2.273470878601074\n",
      "Epoch: 8, Train_Loss: 2.0689032077789307, Test_Loss: 2.0323903560638428 *\n",
      "Epoch: 8, Train_Loss: 2.05135178565979, Test_Loss: 1.967700719833374 *\n",
      "Epoch: 8, Train_Loss: 2.096741199493408, Test_Loss: 2.3273372650146484\n",
      "Epoch: 8, Train_Loss: 2.0446112155914307, Test_Loss: 2.4313955307006836\n",
      "Epoch: 8, Train_Loss: 1.9947144985198975, Test_Loss: 2.3949763774871826 *\n",
      "Epoch: 8, Train_Loss: 1.9669686555862427, Test_Loss: 2.414921522140503\n",
      "Epoch: 8, Train_Loss: 1.9507172107696533, Test_Loss: 1.9535061120986938 *\n",
      "Epoch: 8, Train_Loss: 4.721701145172119, Test_Loss: 2.085767984390259\n",
      "Epoch: 8, Train_Loss: 2.0326497554779053, Test_Loss: 2.7382569313049316\n",
      "Epoch: 8, Train_Loss: 1.9681318998336792, Test_Loss: 3.201659679412842\n",
      "Epoch: 8, Train_Loss: 1.9561059474945068, Test_Loss: 2.388059139251709 *\n",
      "Epoch: 8, Train_Loss: 1.955479621887207, Test_Loss: 2.184230327606201 *\n",
      "Epoch: 8, Train_Loss: 1.9536861181259155, Test_Loss: 2.6809375286102295\n",
      "Epoch: 8, Train_Loss: 1.945766806602478, Test_Loss: 2.0182042121887207 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 8\n",
      "Epoch: 8, Train_Loss: 1.9502835273742676, Test_Loss: 2.1333484649658203\n",
      "Epoch: 8, Train_Loss: 1.9471819400787354, Test_Loss: 2.912661552429199\n",
      "Epoch: 8, Train_Loss: 1.9913575649261475, Test_Loss: 2.1750433444976807 *\n",
      "Epoch: 8, Train_Loss: 1.9349870681762695, Test_Loss: 1.9648139476776123 *\n",
      "Epoch: 8, Train_Loss: 1.9951181411743164, Test_Loss: 2.0585196018218994\n",
      "Epoch: 8, Train_Loss: 1.9917548894882202, Test_Loss: 2.2999327182769775\n",
      "Epoch: 8, Train_Loss: 1.9850409030914307, Test_Loss: 1.9693354368209839 *\n",
      "Epoch: 8, Train_Loss: 1.976834774017334, Test_Loss: 1.9912272691726685\n",
      "Epoch: 8, Train_Loss: 1.9809930324554443, Test_Loss: 3.0811476707458496\n",
      "Epoch: 8, Train_Loss: 1.931911826133728, Test_Loss: 2.15675950050354 *\n",
      "Epoch: 8, Train_Loss: 1.9243321418762207, Test_Loss: 1.9233404397964478 *\n",
      "Epoch: 8, Train_Loss: 1.9203020334243774, Test_Loss: 1.9280081987380981\n",
      "Epoch: 8, Train_Loss: 4.467458724975586, Test_Loss: 1.973556637763977\n",
      "Epoch: 8, Train_Loss: 1.9319804906845093, Test_Loss: 1.9612220525741577 *\n",
      "Epoch: 8, Train_Loss: 1.9143296480178833, Test_Loss: 2.1286749839782715\n",
      "Epoch: 8, Train_Loss: 1.9106754064559937, Test_Loss: 1.9815829992294312 *\n",
      "Epoch: 8, Train_Loss: 1.9075865745544434, Test_Loss: 1.90605890750885 *\n",
      "Epoch: 8, Train_Loss: 1.9972832202911377, Test_Loss: 1.9092100858688354\n",
      "Epoch: 8, Train_Loss: 2.0182912349700928, Test_Loss: 4.760303974151611\n",
      "Epoch: 8, Train_Loss: 1.9257394075393677, Test_Loss: 1.9041104316711426 *\n",
      "Epoch: 8, Train_Loss: 2.075976848602295, Test_Loss: 1.9009214639663696 *\n",
      "Epoch: 8, Train_Loss: 1.9930269718170166, Test_Loss: 1.9039366245269775\n",
      "Epoch: 8, Train_Loss: 1.897572636604309, Test_Loss: 1.904191255569458\n",
      "Epoch: 8, Train_Loss: 1.8962398767471313, Test_Loss: 1.904346227645874\n",
      "Epoch: 8, Train_Loss: 1.8949165344238281, Test_Loss: 1.9054605960845947\n",
      "Epoch: 8, Train_Loss: 1.8934630155563354, Test_Loss: 1.9023475646972656 *\n",
      "Epoch: 8, Train_Loss: 1.9484702348709106, Test_Loss: 1.892652153968811 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 2.052203416824341, Test_Loss: 1.89090895652771 *\n",
      "Epoch: 9, Train_Loss: 1.9988638162612915, Test_Loss: 1.8925554752349854\n",
      "Epoch: 9, Train_Loss: 1.9792611598968506, Test_Loss: 1.8910123109817505 *\n",
      "Epoch: 9, Train_Loss: 2.052649974822998, Test_Loss: 1.8885799646377563 *\n",
      "Epoch: 9, Train_Loss: 2.012730836868286, Test_Loss: 1.885482907295227 *\n",
      "Epoch: 9, Train_Loss: 3.526167869567871, Test_Loss: 1.886293649673462\n",
      "Epoch: 9, Train_Loss: 1.9135810136795044, Test_Loss: 1.927680492401123\n",
      "Epoch: 9, Train_Loss: 1.9121237993240356, Test_Loss: 4.7200727462768555\n",
      "Epoch: 9, Train_Loss: 1.8868350982666016, Test_Loss: 1.888134479522705 *\n",
      "Epoch: 9, Train_Loss: 1.9973994493484497, Test_Loss: 1.9196968078613281\n",
      "Epoch: 9, Train_Loss: 1.9725112915039062, Test_Loss: 1.9420064687728882\n",
      "Epoch: 9, Train_Loss: 1.918666958808899, Test_Loss: 1.9004756212234497 *\n",
      "Epoch: 9, Train_Loss: 1.8911467790603638, Test_Loss: 1.899953007698059 *\n",
      "Epoch: 9, Train_Loss: 1.8812857866287231, Test_Loss: 1.887532114982605 *\n",
      "Epoch: 9, Train_Loss: 1.8903560638427734, Test_Loss: 1.9429974555969238\n",
      "Epoch: 9, Train_Loss: 1.899699330329895, Test_Loss: 1.9524173736572266\n",
      "Epoch: 9, Train_Loss: 1.8633809089660645, Test_Loss: 1.888860821723938 *\n",
      "Epoch: 9, Train_Loss: 1.8609009981155396, Test_Loss: 1.8707184791564941 *\n",
      "Epoch: 9, Train_Loss: 1.8612616062164307, Test_Loss: 1.877044677734375\n",
      "Epoch: 9, Train_Loss: 1.8589603900909424, Test_Loss: 1.8739737272262573 *\n",
      "Epoch: 9, Train_Loss: 1.8581842184066772, Test_Loss: 1.8751397132873535\n",
      "Epoch: 9, Train_Loss: 1.864740252494812, Test_Loss: 1.859875202178955 *\n",
      "Epoch: 9, Train_Loss: 1.8675649166107178, Test_Loss: 1.8989511728286743\n",
      "Epoch: 9, Train_Loss: 1.856012225151062, Test_Loss: 2.012584686279297\n",
      "Epoch: 9, Train_Loss: 1.8509633541107178, Test_Loss: 1.9400966167449951 *\n",
      "Epoch: 9, Train_Loss: 1.8633754253387451, Test_Loss: 2.0496091842651367\n",
      "Epoch: 9, Train_Loss: 1.8485736846923828, Test_Loss: 2.048283338546753 *\n",
      "Epoch: 9, Train_Loss: 1.8948097229003906, Test_Loss: 1.9556964635849 *\n",
      "Epoch: 9, Train_Loss: 1.874732255935669, Test_Loss: 1.8555394411087036 *\n",
      "Epoch: 9, Train_Loss: 1.86481511592865, Test_Loss: 2.044625759124756\n",
      "Epoch: 9, Train_Loss: 1.8527600765228271, Test_Loss: 2.367525339126587\n",
      "Epoch: 9, Train_Loss: 1.8625459671020508, Test_Loss: 2.3961310386657715\n",
      "Epoch: 9, Train_Loss: 1.9374470710754395, Test_Loss: 2.2511682510375977 *\n",
      "Epoch: 9, Train_Loss: 1.8831099271774292, Test_Loss: 1.8479139804840088 *\n",
      "Epoch: 9, Train_Loss: 1.8489465713500977, Test_Loss: 1.9340877532958984\n",
      "Epoch: 9, Train_Loss: 1.8358224630355835, Test_Loss: 2.4424312114715576\n",
      "Epoch: 9, Train_Loss: 1.8415156602859497, Test_Loss: 3.1677017211914062\n",
      "Epoch: 9, Train_Loss: 1.886743187904358, Test_Loss: 2.387686252593994 *\n",
      "Epoch: 9, Train_Loss: 1.8721269369125366, Test_Loss: 1.9634184837341309 *\n",
      "Epoch: 9, Train_Loss: 1.8583705425262451, Test_Loss: 2.7748284339904785\n",
      "Epoch: 9, Train_Loss: 1.9750168323516846, Test_Loss: 1.8928724527359009 *\n",
      "Epoch: 9, Train_Loss: 1.8385788202285767, Test_Loss: 2.060983896255493\n",
      "Epoch: 9, Train_Loss: 1.8226890563964844, Test_Loss: 2.600893497467041\n",
      "Epoch: 9, Train_Loss: 1.8260279893875122, Test_Loss: 2.1465210914611816 *\n",
      "Epoch: 9, Train_Loss: 1.838678240776062, Test_Loss: 1.8340139389038086 *\n",
      "Epoch: 9, Train_Loss: 1.8320856094360352, Test_Loss: 1.88321852684021\n",
      "Epoch: 9, Train_Loss: 1.8262792825698853, Test_Loss: 2.3072237968444824\n",
      "Epoch: 9, Train_Loss: 1.819440245628357, Test_Loss: 1.8874945640563965 *\n",
      "Epoch: 9, Train_Loss: 1.8324679136276245, Test_Loss: 1.8524101972579956 *\n",
      "Epoch: 9, Train_Loss: 1.8351271152496338, Test_Loss: 2.7353999614715576\n",
      "Epoch: 9, Train_Loss: 1.860695719718933, Test_Loss: 2.1807422637939453 *\n",
      "Epoch: 9, Train_Loss: 1.8218261003494263, Test_Loss: 1.8200548887252808 *\n",
      "Epoch: 9, Train_Loss: 1.816856861114502, Test_Loss: 1.8107422590255737 *\n",
      "Epoch: 9, Train_Loss: 1.8158332109451294, Test_Loss: 1.8490664958953857\n",
      "Epoch: 9, Train_Loss: 3.5890448093414307, Test_Loss: 1.8365579843521118 *\n",
      "Epoch: 9, Train_Loss: 2.769439697265625, Test_Loss: 2.0316641330718994\n",
      "Epoch: 9, Train_Loss: 1.8771865367889404, Test_Loss: 1.8952269554138184 *\n",
      "Epoch: 9, Train_Loss: 1.8656721115112305, Test_Loss: 1.7987730503082275 *\n",
      "Epoch: 9, Train_Loss: 1.8272255659103394, Test_Loss: 1.8021080493927002\n",
      "Epoch: 9, Train_Loss: 1.819013237953186, Test_Loss: 4.549537181854248\n",
      "Epoch: 9, Train_Loss: 2.5637075901031494, Test_Loss: 1.797497034072876 *\n",
      "Epoch: 9, Train_Loss: 2.588759183883667, Test_Loss: 1.7946968078613281 *\n",
      "Epoch: 9, Train_Loss: 2.7832019329071045, Test_Loss: 1.7917900085449219 *\n",
      "Epoch: 9, Train_Loss: 2.4937803745269775, Test_Loss: 1.7922555208206177\n",
      "Epoch: 9, Train_Loss: 2.0181469917297363, Test_Loss: 1.79119873046875 *\n",
      "Epoch: 9, Train_Loss: 3.2270383834838867, Test_Loss: 1.7988272905349731\n",
      "Epoch: 9, Train_Loss: 1.812529444694519, Test_Loss: 1.7990175485610962\n",
      "Epoch: 9, Train_Loss: 2.141059398651123, Test_Loss: 1.7826284170150757 *\n",
      "Epoch: 9, Train_Loss: 1.8078590631484985, Test_Loss: 1.7817378044128418 *\n",
      "Epoch: 9, Train_Loss: 1.8885648250579834, Test_Loss: 1.7808725833892822 *\n",
      "Epoch: 9, Train_Loss: 2.0262715816497803, Test_Loss: 1.7794774770736694 *\n",
      "Epoch: 9, Train_Loss: 1.9027345180511475, Test_Loss: 1.7760287523269653 *\n",
      "Epoch: 9, Train_Loss: 2.0088517665863037, Test_Loss: 1.7746456861495972 *\n",
      "Epoch: 9, Train_Loss: 2.1794846057891846, Test_Loss: 1.773072600364685 *\n",
      "Epoch: 9, Train_Loss: 1.851521611213684, Test_Loss: 1.805718183517456\n",
      "Epoch: 9, Train_Loss: 1.8231853246688843, Test_Loss: 4.506629467010498\n",
      "Epoch: 9, Train_Loss: 1.7718961238861084, Test_Loss: 1.7786381244659424 *\n",
      "Epoch: 9, Train_Loss: 1.7698193788528442, Test_Loss: 1.825740098953247\n",
      "Epoch: 9, Train_Loss: 1.8123760223388672, Test_Loss: 1.8160730600357056 *\n",
      "Epoch: 9, Train_Loss: 1.8437116146087646, Test_Loss: 1.8025290966033936 *\n",
      "Epoch: 9, Train_Loss: 2.051100969314575, Test_Loss: 1.791662335395813 *\n",
      "Epoch: 9, Train_Loss: 1.7979521751403809, Test_Loss: 1.7712901830673218 *\n",
      "Epoch: 9, Train_Loss: 2.1894383430480957, Test_Loss: 1.834189772605896\n",
      "Epoch: 9, Train_Loss: 1.7981220483779907, Test_Loss: 1.8377078771591187\n",
      "Epoch: 9, Train_Loss: 2.3498764038085938, Test_Loss: 1.7871476411819458 *\n",
      "Epoch: 9, Train_Loss: 1.930215835571289, Test_Loss: 1.768168568611145 *\n",
      "Epoch: 9, Train_Loss: 1.897707462310791, Test_Loss: 1.7776412963867188\n",
      "Epoch: 9, Train_Loss: 2.7678098678588867, Test_Loss: 1.7673319578170776 *\n",
      "Epoch: 9, Train_Loss: 1.7658387422561646, Test_Loss: 1.7705204486846924\n",
      "Epoch: 9, Train_Loss: 1.9267061948776245, Test_Loss: 1.7551945447921753 *\n",
      "Epoch: 9, Train_Loss: 1.8442248106002808, Test_Loss: 1.780932903289795\n",
      "Epoch: 9, Train_Loss: 1.7664636373519897, Test_Loss: 1.91646409034729\n",
      "Epoch: 9, Train_Loss: 10.572098731994629, Test_Loss: 1.814440369606018 *\n",
      "Epoch: 9, Train_Loss: 3.728518486022949, Test_Loss: 1.9626226425170898\n",
      "Epoch: 9, Train_Loss: 1.8637290000915527, Test_Loss: 1.8981852531433105 *\n",
      "Epoch: 9, Train_Loss: 7.318812370300293, Test_Loss: 1.8736727237701416 *\n",
      "Epoch: 9, Train_Loss: 2.395678758621216, Test_Loss: 1.7646710872650146 *\n",
      "Epoch: 9, Train_Loss: 4.958380699157715, Test_Loss: 1.8123900890350342\n",
      "Epoch: 9, Train_Loss: 1.7501581907272339, Test_Loss: 2.2812657356262207\n",
      "Epoch: 9, Train_Loss: 1.738081693649292, Test_Loss: 2.3645358085632324\n",
      "Epoch: 9, Train_Loss: 1.7351478338241577, Test_Loss: 2.166714906692505 *\n",
      "Epoch: 9, Train_Loss: 1.732638955116272, Test_Loss: 1.749253511428833 *\n",
      "Epoch: 9, Train_Loss: 1.7367018461227417, Test_Loss: 1.8050601482391357\n",
      "Epoch: 9, Train_Loss: 1.740097999572754, Test_Loss: 2.323026180267334\n",
      "Epoch: 9, Train_Loss: 1.7798559665679932, Test_Loss: 2.9406280517578125\n",
      "Epoch: 9, Train_Loss: 1.7298287153244019, Test_Loss: 2.427891969680786 *\n",
      "Epoch: 9, Train_Loss: 1.7237211465835571, Test_Loss: 1.7768487930297852 *\n",
      "Epoch: 9, Train_Loss: 1.7222888469696045, Test_Loss: 2.6415343284606934\n",
      "Epoch: 9, Train_Loss: 1.723242998123169, Test_Loss: 1.8090676069259644 *\n",
      "Epoch: 9, Train_Loss: 1.7194052934646606, Test_Loss: 1.9241100549697876\n",
      "Epoch: 9, Train_Loss: 1.7196848392486572, Test_Loss: 2.3745920658111572\n",
      "Epoch: 9, Train_Loss: 1.7338708639144897, Test_Loss: 2.2410101890563965 *\n",
      "Epoch: 9, Train_Loss: 1.7236411571502686, Test_Loss: 1.7394638061523438 *\n",
      "Epoch: 9, Train_Loss: 6.321157455444336, Test_Loss: 1.7604570388793945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 1.7187833786010742, Test_Loss: 2.1545839309692383\n",
      "Epoch: 9, Train_Loss: 1.7296303510665894, Test_Loss: 1.7781901359558105 *\n",
      "Epoch: 9, Train_Loss: 1.7294524908065796, Test_Loss: 1.744653582572937 *\n",
      "Epoch: 9, Train_Loss: 1.8785454034805298, Test_Loss: 2.5785090923309326\n",
      "Epoch: 9, Train_Loss: 1.7966448068618774, Test_Loss: 2.2643206119537354 *\n",
      "Epoch: 9, Train_Loss: 1.805275797843933, Test_Loss: 1.7315380573272705 *\n",
      "Epoch: 9, Train_Loss: 1.8429343700408936, Test_Loss: 1.7162573337554932 *\n",
      "Epoch: 9, Train_Loss: 1.7828364372253418, Test_Loss: 1.767135739326477\n",
      "Epoch: 9, Train_Loss: 1.7483586072921753, Test_Loss: 1.7180298566818237 *\n",
      "Epoch: 9, Train_Loss: 1.716103196144104, Test_Loss: 1.9291942119598389\n",
      "Epoch: 9, Train_Loss: 1.7033302783966064, Test_Loss: 1.788098931312561 *\n",
      "Epoch: 9, Train_Loss: 4.511952877044678, Test_Loss: 1.695515751838684 *\n",
      "Epoch: 9, Train_Loss: 1.7158747911453247, Test_Loss: 1.694825291633606 *\n",
      "Epoch: 9, Train_Loss: 1.720011591911316, Test_Loss: 4.5372724533081055\n",
      "Epoch: 9, Train_Loss: 1.7055867910385132, Test_Loss: 1.715914249420166 *\n",
      "Epoch: 9, Train_Loss: 1.7090497016906738, Test_Loss: 1.6906605958938599 *\n",
      "Epoch: 9, Train_Loss: 1.7052891254425049, Test_Loss: 1.6932528018951416\n",
      "Epoch: 9, Train_Loss: 1.6978682279586792, Test_Loss: 1.7041187286376953\n",
      "Epoch: 9, Train_Loss: 1.7007691860198975, Test_Loss: 1.7013460397720337 *\n",
      "Epoch: 9, Train_Loss: 1.703323245048523, Test_Loss: 1.6977167129516602 *\n",
      "Epoch: 9, Train_Loss: 1.731028437614441, Test_Loss: 1.6950379610061646 *\n",
      "Epoch: 9, Train_Loss: 1.6896803379058838, Test_Loss: 1.6902936697006226 *\n",
      "Epoch: 9, Train_Loss: 1.7525672912597656, Test_Loss: 1.6887996196746826 *\n",
      "Epoch: 9, Train_Loss: 1.7430862188339233, Test_Loss: 1.689789056777954\n",
      "Epoch: 9, Train_Loss: 1.7393147945404053, Test_Loss: 1.6871922016143799 *\n",
      "Epoch: 9, Train_Loss: 1.7256685495376587, Test_Loss: 1.686533808708191 *\n",
      "Epoch: 9, Train_Loss: 1.7367315292358398, Test_Loss: 1.6850279569625854 *\n",
      "Epoch: 9, Train_Loss: 1.684290885925293, Test_Loss: 1.6825246810913086 *\n",
      "Epoch: 9, Train_Loss: 1.6782325506210327, Test_Loss: 1.7162961959838867\n",
      "Epoch: 9, Train_Loss: 1.6948552131652832, Test_Loss: 4.574821472167969\n",
      "Epoch: 9, Train_Loss: 4.157086372375488, Test_Loss: 1.6747503280639648 *\n",
      "Epoch: 9, Train_Loss: 1.6829251050949097, Test_Loss: 1.7090778350830078\n",
      "Epoch: 9, Train_Loss: 1.670236587524414, Test_Loss: 1.7220442295074463\n",
      "Epoch: 9, Train_Loss: 1.6672148704528809, Test_Loss: 1.7257336378097534\n",
      "Epoch: 9, Train_Loss: 1.6658170223236084, Test_Loss: 1.6904898881912231 *\n",
      "Epoch: 9, Train_Loss: 1.740905523300171, Test_Loss: 1.6771368980407715 *\n",
      "Epoch: 9, Train_Loss: 1.7575384378433228, Test_Loss: 1.7290791273117065\n",
      "Epoch: 9, Train_Loss: 1.7013658285140991, Test_Loss: 1.7602729797363281\n",
      "Epoch: 9, Train_Loss: 1.843326210975647, Test_Loss: 1.6832695007324219 *\n",
      "Epoch: 9, Train_Loss: 1.7462303638458252, Test_Loss: 1.6699888706207275 *\n",
      "Epoch: 9, Train_Loss: 1.657545804977417, Test_Loss: 1.6733061075210571\n",
      "Epoch: 9, Train_Loss: 1.6610054969787598, Test_Loss: 1.6684966087341309 *\n",
      "Epoch: 9, Train_Loss: 1.6572165489196777, Test_Loss: 1.666965365409851 *\n",
      "Epoch: 9, Train_Loss: 1.6547112464904785, Test_Loss: 1.6599336862564087 *\n",
      "Epoch: 9, Train_Loss: 1.7263178825378418, Test_Loss: 1.6799509525299072\n",
      "Epoch: 9, Train_Loss: 1.833896279335022, Test_Loss: 1.8334980010986328\n",
      "Epoch: 9, Train_Loss: 1.752894401550293, Test_Loss: 1.7026309967041016 *\n",
      "Epoch: 9, Train_Loss: 1.748480200767517, Test_Loss: 1.8522049188613892\n",
      "Epoch: 9, Train_Loss: 1.8411989212036133, Test_Loss: 1.794162392616272 *\n",
      "Epoch: 9, Train_Loss: 1.70526921749115, Test_Loss: 1.8108245134353638\n",
      "Epoch: 9, Train_Loss: 3.2188549041748047, Test_Loss: 1.6739994287490845 *\n",
      "Epoch: 9, Train_Loss: 1.6782008409500122, Test_Loss: 1.6767456531524658\n",
      "Epoch: 9, Train_Loss: 1.6826666593551636, Test_Loss: 2.1520731449127197\n",
      "Epoch: 9, Train_Loss: 1.654175877571106, Test_Loss: 2.3584208488464355\n",
      "Epoch: 9, Train_Loss: 1.74004065990448, Test_Loss: 2.0169484615325928 *\n",
      "Epoch: 9, Train_Loss: 1.713827133178711, Test_Loss: 1.6786531209945679 *\n",
      "Epoch: 9, Train_Loss: 1.6744403839111328, Test_Loss: 1.7240397930145264\n",
      "Epoch: 9, Train_Loss: 1.6614892482757568, Test_Loss: 2.151853084564209\n",
      "Epoch: 9, Train_Loss: 1.6556520462036133, Test_Loss: 2.913571357727051\n",
      "Epoch: 9, Train_Loss: 1.6570298671722412, Test_Loss: 2.2940444946289062 *\n",
      "Epoch: 9, Train_Loss: 1.662642002105713, Test_Loss: 1.6592189073562622 *\n",
      "Epoch: 9, Train_Loss: 1.637332797050476, Test_Loss: 2.731830596923828\n",
      "Epoch: 9, Train_Loss: 1.632171392440796, Test_Loss: 1.6868054866790771 *\n",
      "Epoch: 9, Train_Loss: 1.6320816278457642, Test_Loss: 1.8376426696777344\n",
      "Epoch: 9, Train_Loss: 1.631433129310608, Test_Loss: 2.1179122924804688\n",
      "Epoch: 9, Train_Loss: 1.6320743560791016, Test_Loss: 2.2174930572509766\n",
      "Epoch: 9, Train_Loss: 1.6391257047653198, Test_Loss: 1.6319801807403564 *\n",
      "Epoch: 9, Train_Loss: 1.644221544265747, Test_Loss: 1.6480756998062134\n",
      "Epoch: 9, Train_Loss: 1.626319408416748, Test_Loss: 2.119445562362671\n",
      "Epoch: 9, Train_Loss: 1.6249440908432007, Test_Loss: 1.764175295829773 *\n",
      "Epoch: 9, Train_Loss: 1.6323094367980957, Test_Loss: 1.6453394889831543 *\n",
      "Epoch: 9, Train_Loss: 1.62185800075531, Test_Loss: 2.171635627746582\n",
      "Epoch: 9, Train_Loss: 1.6641641855239868, Test_Loss: 2.288696765899658\n",
      "Epoch: 9, Train_Loss: 1.6407324075698853, Test_Loss: 1.6673760414123535 *\n",
      "Epoch: 9, Train_Loss: 1.6370307207107544, Test_Loss: 1.6211408376693726 *\n",
      "Epoch: 9, Train_Loss: 1.624243140220642, Test_Loss: 1.6512497663497925\n",
      "Epoch: 9, Train_Loss: 1.635995864868164, Test_Loss: 1.6274874210357666 *\n",
      "Epoch: 9, Train_Loss: 1.718996524810791, Test_Loss: 1.86106276512146\n",
      "Epoch: 9, Train_Loss: 1.6488553285598755, Test_Loss: 1.7174134254455566 *\n",
      "Epoch: 9, Train_Loss: 1.623047947883606, Test_Loss: 1.608635663986206 *\n",
      "Epoch: 9, Train_Loss: 1.6118218898773193, Test_Loss: 1.6071105003356934 *\n",
      "Epoch: 9, Train_Loss: 1.6136929988861084, Test_Loss: 4.263889312744141\n",
      "Epoch: 9, Train_Loss: 1.6588255167007446, Test_Loss: 1.691817283630371 *\n",
      "Epoch: 9, Train_Loss: 1.6502872705459595, Test_Loss: 1.6011227369308472 *\n",
      "Epoch: 9, Train_Loss: 1.6553374528884888, Test_Loss: 1.60159170627594\n",
      "Epoch: 9, Train_Loss: 1.7160594463348389, Test_Loss: 1.6077494621276855\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 9\n",
      "Epoch: 9, Train_Loss: 1.612231731414795, Test_Loss: 1.6045550107955933 *\n",
      "Epoch: 9, Train_Loss: 1.5990443229675293, Test_Loss: 1.6051757335662842\n",
      "Epoch: 9, Train_Loss: 1.604263186454773, Test_Loss: 1.6086100339889526\n",
      "Epoch: 9, Train_Loss: 1.6137357950210571, Test_Loss: 1.596017837524414 *\n",
      "Epoch: 9, Train_Loss: 1.6109226942062378, Test_Loss: 1.5954971313476562 *\n",
      "Epoch: 9, Train_Loss: 1.6027978658676147, Test_Loss: 1.5950610637664795 *\n",
      "Epoch: 9, Train_Loss: 1.5977935791015625, Test_Loss: 1.5922966003417969 *\n",
      "Epoch: 9, Train_Loss: 1.6073881387710571, Test_Loss: 1.5929386615753174\n",
      "Epoch: 9, Train_Loss: 1.6177645921707153, Test_Loss: 1.5905144214630127 *\n",
      "Epoch: 9, Train_Loss: 1.6345289945602417, Test_Loss: 1.5887913703918457 *\n",
      "Epoch: 9, Train_Loss: 1.5995408296585083, Test_Loss: 1.6201951503753662\n",
      "Epoch: 9, Train_Loss: 1.5953527688980103, Test_Loss: 4.421705722808838\n",
      "Epoch: 9, Train_Loss: 1.5948901176452637, Test_Loss: 1.5861680507659912 *\n",
      "Epoch: 9, Train_Loss: 4.015446186065674, Test_Loss: 1.618411898612976\n",
      "Epoch: 9, Train_Loss: 1.90358304977417, Test_Loss: 1.6181286573410034 *\n",
      "Epoch: 9, Train_Loss: 1.6709439754486084, Test_Loss: 1.6550310850143433\n",
      "Epoch: 9, Train_Loss: 1.6321823596954346, Test_Loss: 1.6011793613433838 *\n",
      "Epoch: 9, Train_Loss: 1.6158521175384521, Test_Loss: 1.5886354446411133 *\n",
      "Epoch: 9, Train_Loss: 1.5939613580703735, Test_Loss: 1.6413159370422363\n",
      "Epoch: 9, Train_Loss: 2.463576555252075, Test_Loss: 1.6723496913909912\n",
      "Epoch: 9, Train_Loss: 2.2784667015075684, Test_Loss: 1.6022332906723022 *\n",
      "Epoch: 9, Train_Loss: 2.699519634246826, Test_Loss: 1.5811716318130493 *\n",
      "Epoch: 9, Train_Loss: 2.0920138359069824, Test_Loss: 1.5846502780914307\n",
      "Epoch: 9, Train_Loss: 1.9539144039154053, Test_Loss: 1.5870493650436401\n",
      "Epoch: 9, Train_Loss: 2.90834903717041, Test_Loss: 1.583559274673462 *\n",
      "Epoch: 9, Train_Loss: 1.594802737236023, Test_Loss: 1.5731122493743896 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_Loss: 1.9227943420410156, Test_Loss: 1.5986155271530151\n",
      "Epoch: 10, Train_Loss: 1.5938448905944824, Test_Loss: 1.735389232635498 *\n",
      "Epoch: 10, Train_Loss: 1.6844377517700195, Test_Loss: 1.6230944395065308 *\n",
      "Epoch: 10, Train_Loss: 1.8066198825836182, Test_Loss: 1.8036049604415894\n",
      "Epoch: 10, Train_Loss: 1.6716421842575073, Test_Loss: 1.6696721315383911 *\n",
      "Epoch: 10, Train_Loss: 1.8068866729736328, Test_Loss: 1.7259076833724976\n",
      "Epoch: 10, Train_Loss: 1.9271070957183838, Test_Loss: 1.585033893585205 *\n",
      "Epoch: 10, Train_Loss: 1.6420259475708008, Test_Loss: 1.5842269659042358 *\n",
      "Epoch: 10, Train_Loss: 1.598802924156189, Test_Loss: 2.0281295776367188\n",
      "Epoch: 10, Train_Loss: 1.5548068284988403, Test_Loss: 2.2713325023651123\n",
      "Epoch: 10, Train_Loss: 1.5568417310714722, Test_Loss: 1.9090535640716553 *\n",
      "Epoch: 10, Train_Loss: 1.598789930343628, Test_Loss: 1.6176661252975464 *\n",
      "Epoch: 10, Train_Loss: 1.6261967420578003, Test_Loss: 1.6240394115447998\n",
      "Epoch: 10, Train_Loss: 1.8341164588928223, Test_Loss: 2.083822250366211\n",
      "Epoch: 10, Train_Loss: 1.6040678024291992, Test_Loss: 2.7834815979003906\n",
      "Epoch: 10, Train_Loss: 1.9653279781341553, Test_Loss: 2.1774120330810547 *\n",
      "Epoch: 10, Train_Loss: 1.5507749319076538, Test_Loss: 1.5537158250808716 *\n",
      "Epoch: 10, Train_Loss: 2.1749520301818848, Test_Loss: 2.627920150756836\n",
      "Epoch: 10, Train_Loss: 1.6549986600875854, Test_Loss: 1.6250230073928833 *\n",
      "Epoch: 10, Train_Loss: 1.8171813488006592, Test_Loss: 1.663271427154541\n",
      "Epoch: 10, Train_Loss: 2.424983024597168, Test_Loss: 1.951879620552063\n",
      "Epoch: 10, Train_Loss: 1.5546152591705322, Test_Loss: 2.2769887447357178\n",
      "Epoch: 10, Train_Loss: 1.7653203010559082, Test_Loss: 1.5456976890563965 *\n",
      "Epoch: 10, Train_Loss: 1.5854276418685913, Test_Loss: 1.5574076175689697\n",
      "Epoch: 10, Train_Loss: 1.548959493637085, Test_Loss: 2.0010111331939697\n",
      "Epoch: 10, Train_Loss: 10.37198257446289, Test_Loss: 1.7549071311950684 *\n",
      "Epoch: 10, Train_Loss: 3.553250551223755, Test_Loss: 1.5694594383239746 *\n",
      "Epoch: 10, Train_Loss: 1.5702474117279053, Test_Loss: 1.9244797229766846\n",
      "Epoch: 10, Train_Loss: 7.525750637054443, Test_Loss: 2.2963550090789795\n",
      "Epoch: 10, Train_Loss: 1.706836462020874, Test_Loss: 1.597103476524353 *\n",
      "Epoch: 10, Train_Loss: 4.722131729125977, Test_Loss: 1.5371707677841187 *\n",
      "Epoch: 10, Train_Loss: 1.5411616563796997, Test_Loss: 1.5606235265731812\n",
      "Epoch: 10, Train_Loss: 1.5295541286468506, Test_Loss: 1.5537304878234863 *\n",
      "Epoch: 10, Train_Loss: 1.5252221822738647, Test_Loss: 1.7582790851593018\n",
      "Epoch: 10, Train_Loss: 1.523697853088379, Test_Loss: 1.631728172302246 *\n",
      "Epoch: 10, Train_Loss: 1.5310497283935547, Test_Loss: 1.520877480506897 *\n",
      "Epoch: 10, Train_Loss: 1.5410971641540527, Test_Loss: 1.5194153785705566 *\n",
      "Epoch: 10, Train_Loss: 1.5706017017364502, Test_Loss: 3.9409987926483154\n",
      "Epoch: 10, Train_Loss: 1.5243935585021973, Test_Loss: 2.000837802886963 *\n",
      "Epoch: 10, Train_Loss: 1.5179462432861328, Test_Loss: 1.5182859897613525 *\n",
      "Epoch: 10, Train_Loss: 1.5168125629425049, Test_Loss: 1.5182217359542847 *\n",
      "Epoch: 10, Train_Loss: 1.518842101097107, Test_Loss: 1.5290858745574951\n",
      "Epoch: 10, Train_Loss: 1.51394784450531, Test_Loss: 1.5323960781097412\n",
      "Epoch: 10, Train_Loss: 1.5165886878967285, Test_Loss: 1.5261878967285156 *\n",
      "Epoch: 10, Train_Loss: 1.5283788442611694, Test_Loss: 1.5210394859313965 *\n",
      "Epoch: 10, Train_Loss: 1.5422228574752808, Test_Loss: 1.5204765796661377 *\n",
      "Epoch: 10, Train_Loss: 6.064241886138916, Test_Loss: 1.5192230939865112 *\n",
      "Epoch: 10, Train_Loss: 1.5167758464813232, Test_Loss: 1.5232934951782227\n",
      "Epoch: 10, Train_Loss: 1.528152585029602, Test_Loss: 1.5195505619049072 *\n",
      "Epoch: 10, Train_Loss: 1.5246976613998413, Test_Loss: 1.5213302373886108\n",
      "Epoch: 10, Train_Loss: 1.7005279064178467, Test_Loss: 1.518246054649353 *\n",
      "Epoch: 10, Train_Loss: 1.5860059261322021, Test_Loss: 1.516664981842041 *\n",
      "Epoch: 10, Train_Loss: 1.6226569414138794, Test_Loss: 1.5506623983383179\n",
      "Epoch: 10, Train_Loss: 1.6513885259628296, Test_Loss: 4.45428466796875\n",
      "Epoch: 10, Train_Loss: 1.5739271640777588, Test_Loss: 1.5148221254348755 *\n",
      "Epoch: 10, Train_Loss: 1.5418293476104736, Test_Loss: 1.530594825744629\n",
      "Epoch: 10, Train_Loss: 1.5140527486801147, Test_Loss: 1.5191413164138794 *\n",
      "Epoch: 10, Train_Loss: 1.505011796951294, Test_Loss: 1.6032705307006836\n",
      "Epoch: 10, Train_Loss: 4.2981343269348145, Test_Loss: 1.52300226688385 *\n",
      "Epoch: 10, Train_Loss: 1.5202559232711792, Test_Loss: 1.5083630084991455 *\n",
      "Epoch: 10, Train_Loss: 1.5198183059692383, Test_Loss: 1.557769775390625\n",
      "Epoch: 10, Train_Loss: 1.5023539066314697, Test_Loss: 1.6041288375854492\n",
      "Epoch: 10, Train_Loss: 1.5108224153518677, Test_Loss: 1.531807541847229 *\n",
      "Epoch: 10, Train_Loss: 1.505569577217102, Test_Loss: 1.4940367937088013 *\n",
      "Epoch: 10, Train_Loss: 1.4973540306091309, Test_Loss: 1.4963881969451904\n",
      "Epoch: 10, Train_Loss: 1.502079963684082, Test_Loss: 1.4964853525161743\n",
      "Epoch: 10, Train_Loss: 1.5088205337524414, Test_Loss: 1.4959322214126587 *\n",
      "Epoch: 10, Train_Loss: 1.527479887008667, Test_Loss: 1.492600440979004 *\n",
      "Epoch: 10, Train_Loss: 1.4918694496154785, Test_Loss: 1.5051926374435425\n",
      "Epoch: 10, Train_Loss: 1.551020622253418, Test_Loss: 1.6886069774627686\n",
      "Epoch: 10, Train_Loss: 1.5420668125152588, Test_Loss: 1.5196436643600464 *\n",
      "Epoch: 10, Train_Loss: 1.5359532833099365, Test_Loss: 1.6808741092681885\n",
      "Epoch: 10, Train_Loss: 1.524040937423706, Test_Loss: 1.5964853763580322 *\n",
      "Epoch: 10, Train_Loss: 1.5443534851074219, Test_Loss: 1.7038064002990723\n",
      "Epoch: 10, Train_Loss: 1.4827114343643188, Test_Loss: 1.5166593790054321 *\n",
      "Epoch: 10, Train_Loss: 1.4813810586929321, Test_Loss: 1.4892939329147339 *\n",
      "Epoch: 10, Train_Loss: 1.6665517091751099, Test_Loss: 1.9969639778137207\n",
      "Epoch: 10, Train_Loss: 3.760695457458496, Test_Loss: 2.0871362686157227\n",
      "Epoch: 10, Train_Loss: 1.4863958358764648, Test_Loss: 1.881818175315857 *\n",
      "Epoch: 10, Train_Loss: 1.473376750946045, Test_Loss: 1.59916353225708 *\n",
      "Epoch: 10, Train_Loss: 1.4722930192947388, Test_Loss: 1.5100302696228027 *\n",
      "Epoch: 10, Train_Loss: 1.4711400270462036, Test_Loss: 2.058427333831787\n",
      "Epoch: 10, Train_Loss: 1.5574579238891602, Test_Loss: 2.6687424182891846\n",
      "Epoch: 10, Train_Loss: 1.5533342361450195, Test_Loss: 2.1821634769439697 *\n",
      "Epoch: 10, Train_Loss: 1.527769684791565, Test_Loss: 1.4844962358474731 *\n",
      "Epoch: 10, Train_Loss: 1.6606565713882446, Test_Loss: 2.4062576293945312\n",
      "Epoch: 10, Train_Loss: 1.5423325300216675, Test_Loss: 1.6701468229293823 *\n",
      "Epoch: 10, Train_Loss: 1.4646344184875488, Test_Loss: 1.5422030687332153 *\n",
      "Epoch: 10, Train_Loss: 1.4714961051940918, Test_Loss: 1.807770013809204\n",
      "Epoch: 10, Train_Loss: 1.4666119813919067, Test_Loss: 2.3545055389404297\n",
      "Epoch: 10, Train_Loss: 1.4619485139846802, Test_Loss: 1.4711098670959473 *\n",
      "Epoch: 10, Train_Loss: 1.543243408203125, Test_Loss: 1.491678237915039\n",
      "Epoch: 10, Train_Loss: 1.6237375736236572, Test_Loss: 1.8168308734893799\n",
      "Epoch: 10, Train_Loss: 1.5593221187591553, Test_Loss: 1.7095725536346436 *\n",
      "Epoch: 10, Train_Loss: 1.5559934377670288, Test_Loss: 1.482944369316101 *\n",
      "Epoch: 10, Train_Loss: 1.6846191883087158, Test_Loss: 1.7749754190444946\n",
      "Epoch: 10, Train_Loss: 1.4675331115722656, Test_Loss: 2.3920655250549316\n",
      "Epoch: 10, Train_Loss: 3.0547282695770264, Test_Loss: 1.5104711055755615 *\n",
      "Epoch: 10, Train_Loss: 1.486025094985962, Test_Loss: 1.4583427906036377 *\n",
      "Epoch: 10, Train_Loss: 1.4843848943710327, Test_Loss: 1.4761602878570557\n",
      "Epoch: 10, Train_Loss: 1.4758834838867188, Test_Loss: 1.475056767463684 *\n",
      "Epoch: 10, Train_Loss: 1.554771900177002, Test_Loss: 1.6228203773498535\n",
      "Epoch: 10, Train_Loss: 1.5224679708480835, Test_Loss: 1.6583648920059204\n",
      "Epoch: 10, Train_Loss: 1.4814198017120361, Test_Loss: 1.4532533884048462 *\n",
      "Epoch: 10, Train_Loss: 1.4691071510314941, Test_Loss: 1.4511168003082275 *\n",
      "Epoch: 10, Train_Loss: 1.4600903987884521, Test_Loss: 2.9185733795166016\n",
      "Epoch: 10, Train_Loss: 1.4707988500595093, Test_Loss: 2.721360445022583 *\n",
      "Epoch: 10, Train_Loss: 1.4660155773162842, Test_Loss: 1.4444458484649658 *\n",
      "Epoch: 10, Train_Loss: 1.4444659948349, Test_Loss: 1.446305751800537\n",
      "Epoch: 10, Train_Loss: 1.4420298337936401, Test_Loss: 1.4449800252914429 *\n",
      "Epoch: 10, Train_Loss: 1.4446141719818115, Test_Loss: 1.4471099376678467\n",
      "Epoch: 10, Train_Loss: 1.4432430267333984, Test_Loss: 1.4457108974456787 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 1.4434670209884644, Test_Loss: 1.456817388534546\n",
      "Epoch: 10, Train_Loss: 1.4511088132858276, Test_Loss: 1.4386374950408936 *\n",
      "Epoch: 10, Train_Loss: 1.453517198562622, Test_Loss: 1.4373962879180908 *\n",
      "Epoch: 10, Train_Loss: 1.4419949054718018, Test_Loss: 1.4368315935134888 *\n",
      "Epoch: 10, Train_Loss: 1.4423739910125732, Test_Loss: 1.4343289136886597 *\n",
      "Epoch: 10, Train_Loss: 1.4492323398590088, Test_Loss: 1.4349209070205688\n",
      "Epoch: 10, Train_Loss: 1.4385572671890259, Test_Loss: 1.4336438179016113 *\n",
      "Epoch: 10, Train_Loss: 1.4764615297317505, Test_Loss: 1.4329348802566528 *\n",
      "Epoch: 10, Train_Loss: 1.4574947357177734, Test_Loss: 1.4519004821777344\n",
      "Epoch: 10, Train_Loss: 1.4612535238265991, Test_Loss: 4.012517929077148\n",
      "Epoch: 10, Train_Loss: 1.438828706741333, Test_Loss: 1.628190517425537 *\n",
      "Epoch: 10, Train_Loss: 1.4499527215957642, Test_Loss: 1.470568299293518 *\n",
      "Epoch: 10, Train_Loss: 1.5230692625045776, Test_Loss: 1.4542092084884644 *\n",
      "Epoch: 10, Train_Loss: 1.4533518552780151, Test_Loss: 1.507943034172058\n",
      "Epoch: 10, Train_Loss: 1.4394152164459229, Test_Loss: 1.446503758430481 *\n",
      "Epoch: 10, Train_Loss: 1.4289449453353882, Test_Loss: 1.4366706609725952 *\n",
      "Epoch: 10, Train_Loss: 1.4348831176757812, Test_Loss: 1.4965451955795288\n",
      "Epoch: 10, Train_Loss: 1.4758553504943848, Test_Loss: 1.5084832906723022\n",
      "Epoch: 10, Train_Loss: 1.4674979448318481, Test_Loss: 1.472411870956421 *\n",
      "Epoch: 10, Train_Loss: 1.4826470613479614, Test_Loss: 1.4284173250198364 *\n",
      "Epoch: 10, Train_Loss: 1.525731086730957, Test_Loss: 1.4339131116867065\n",
      "Epoch: 10, Train_Loss: 1.4330164194107056, Test_Loss: 1.4394193887710571\n",
      "Epoch: 10, Train_Loss: 1.415440320968628, Test_Loss: 1.429616093635559 *\n",
      "Epoch: 10, Train_Loss: 1.4207872152328491, Test_Loss: 1.420528531074524 *\n",
      "Epoch: 10, Train_Loss: 1.4306482076644897, Test_Loss: 1.4351837635040283\n",
      "Epoch: 10, Train_Loss: 1.428062915802002, Test_Loss: 1.6022777557373047\n",
      "Epoch: 10, Train_Loss: 1.4157447814941406, Test_Loss: 1.4477784633636475 *\n",
      "Epoch: 10, Train_Loss: 1.4190467596054077, Test_Loss: 1.637847661972046\n",
      "Epoch: 10, Train_Loss: 1.4301809072494507, Test_Loss: 1.49732506275177 *\n",
      "Epoch: 10, Train_Loss: 1.438644289970398, Test_Loss: 1.6221885681152344\n",
      "Epoch: 10, Train_Loss: 1.4447654485702515, Test_Loss: 1.4374207258224487 *\n",
      "Epoch: 10, Train_Loss: 1.4225801229476929, Test_Loss: 1.4235892295837402 *\n",
      "Epoch: 10, Train_Loss: 1.4129388332366943, Test_Loss: 1.896763801574707\n",
      "Epoch: 10, Train_Loss: 1.425873041152954, Test_Loss: 1.9569034576416016\n",
      "Epoch: 10, Train_Loss: 4.1276679039001465, Test_Loss: 1.8502380847930908 *\n",
      "Epoch: 10, Train_Loss: 1.4279857873916626, Test_Loss: 1.5788713693618774 *\n",
      "Epoch: 10, Train_Loss: 1.5118579864501953, Test_Loss: 1.4252245426177979 *\n",
      "Epoch: 10, Train_Loss: 1.4312635660171509, Test_Loss: 1.9569571018218994\n",
      "Epoch: 10, Train_Loss: 1.4370758533477783, Test_Loss: 2.5114054679870605\n",
      "Epoch: 10, Train_Loss: 1.4126765727996826, Test_Loss: 2.2214527130126953 *\n",
      "Epoch: 10, Train_Loss: 2.406754493713379, Test_Loss: 1.4507884979248047 *\n",
      "Epoch: 10, Train_Loss: 1.9709006547927856, Test_Loss: 2.1493592262268066\n",
      "Epoch: 10, Train_Loss: 2.6724371910095215, Test_Loss: 1.7876652479171753 *\n",
      "Epoch: 10, Train_Loss: 1.7593241930007935, Test_Loss: 1.4557135105133057 *\n",
      "Epoch: 10, Train_Loss: 1.9593658447265625, Test_Loss: 1.7014713287353516\n",
      "Epoch: 10, Train_Loss: 2.5238895416259766, Test_Loss: 2.281069755554199\n",
      "Epoch: 10, Train_Loss: 1.419579267501831, Test_Loss: 1.4162744283676147 *\n",
      "Epoch: 10, Train_Loss: 1.7343807220458984, Test_Loss: 1.4060412645339966 *\n",
      "Epoch: 10, Train_Loss: 1.4144731760025024, Test_Loss: 1.715458869934082\n",
      "Epoch: 10, Train_Loss: 1.514701008796692, Test_Loss: 1.7429654598236084\n",
      "Epoch: 10, Train_Loss: 1.6344349384307861, Test_Loss: 1.4215787649154663 *\n",
      "Epoch: 10, Train_Loss: 1.4782037734985352, Test_Loss: 1.5429770946502686\n",
      "Epoch: 10, Train_Loss: 1.6643006801605225, Test_Loss: 2.349294900894165\n",
      "Epoch: 10, Train_Loss: 1.6999953985214233, Test_Loss: 1.445992350578308 *\n",
      "Epoch: 10, Train_Loss: 1.478774905204773, Test_Loss: 1.3869342803955078 *\n",
      "Epoch: 10, Train_Loss: 1.4154964685440063, Test_Loss: 1.4054739475250244\n",
      "Epoch: 10, Train_Loss: 1.3819580078125, Test_Loss: 1.404589056968689 *\n",
      "Epoch: 10, Train_Loss: 1.3846992254257202, Test_Loss: 1.469433069229126\n",
      "Epoch: 10, Train_Loss: 1.4254337549209595, Test_Loss: 1.7088936567306519\n",
      "Epoch: 10, Train_Loss: 1.4527256488800049, Test_Loss: 1.3852936029434204 *\n",
      "Epoch: 10, Train_Loss: 1.662259578704834, Test_Loss: 1.3815009593963623 *\n",
      "Epoch: 10, Train_Loss: 1.445363998413086, Test_Loss: 2.065962314605713\n",
      "Epoch: 10, Train_Loss: 1.7733391523361206, Test_Loss: 3.4556896686553955\n",
      "Epoch: 10, Train_Loss: 1.3780467510223389, Test_Loss: 1.3754433393478394 *\n",
      "Epoch: 10, Train_Loss: 2.0054028034210205, Test_Loss: 1.3762558698654175\n",
      "Epoch: 10, Train_Loss: 1.4549720287322998, Test_Loss: 1.3770874738693237\n",
      "Epoch: 10, Train_Loss: 1.7853398323059082, Test_Loss: 1.379390001296997\n",
      "Epoch: 10, Train_Loss: 2.114380359649658, Test_Loss: 1.3755924701690674 *\n",
      "Epoch: 10, Train_Loss: 1.3830796480178833, Test_Loss: 1.3820877075195312\n",
      "Epoch: 10, Train_Loss: 1.6011296510696411, Test_Loss: 1.3731273412704468 *\n",
      "Epoch: 10, Train_Loss: 1.4003431797027588, Test_Loss: 1.3695628643035889 *\n",
      "Epoch: 10, Train_Loss: 1.3781510591506958, Test_Loss: 1.3670071363449097 *\n",
      "Epoch: 10, Train_Loss: 10.241740226745605, Test_Loss: 1.3680168390274048\n",
      "Epoch: 10, Train_Loss: 3.3905396461486816, Test_Loss: 1.3668490648269653 *\n",
      "Epoch: 10, Train_Loss: 1.4024689197540283, Test_Loss: 1.365604043006897 *\n",
      "Epoch: 10, Train_Loss: 7.359947204589844, Test_Loss: 1.3660109043121338\n",
      "Epoch: 10, Train_Loss: 1.4230350255966187, Test_Loss: 1.383838415145874\n",
      "Epoch: 10, Train_Loss: 4.552467346191406, Test_Loss: 3.211841106414795\n",
      "Epoch: 10, Train_Loss: 1.3699040412902832, Test_Loss: 2.2982726097106934 *\n",
      "Epoch: 10, Train_Loss: 1.3610132932662964, Test_Loss: 1.3945844173431396 *\n",
      "Epoch: 10, Train_Loss: 1.3578723669052124, Test_Loss: 1.381287932395935 *\n",
      "Epoch: 10, Train_Loss: 1.3561971187591553, Test_Loss: 1.4580086469650269\n",
      "Epoch: 10, Train_Loss: 1.3622822761535645, Test_Loss: 1.3738460540771484 *\n",
      "Epoch: 10, Train_Loss: 1.375222086906433, Test_Loss: 1.3734030723571777 *\n",
      "Epoch: 10, Train_Loss: 1.397720217704773, Test_Loss: 1.407914161682129\n",
      "Epoch: 10, Train_Loss: 1.3573836088180542, Test_Loss: 1.4563987255096436\n",
      "Epoch: 10, Train_Loss: 1.3524158000946045, Test_Loss: 1.4133896827697754 *\n",
      "Epoch: 10, Train_Loss: 1.349301815032959, Test_Loss: 1.3536691665649414 *\n",
      "Epoch: 10, Train_Loss: 1.350117802619934, Test_Loss: 1.3509917259216309 *\n",
      "Model saved at location G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\\model.ckpt at epoch 10\n",
      "Epoch: 10, Train_Loss: 1.3470081090927124, Test_Loss: 1.354528546333313\n",
      "Epoch: 10, Train_Loss: 1.3516985177993774, Test_Loss: 1.351413607597351 *\n",
      "Epoch: 10, Train_Loss: 1.3678137063980103, Test_Loss: 1.348519206047058 *\n",
      "Epoch: 10, Train_Loss: 1.686882734298706, Test_Loss: 1.3593372106552124\n",
      "Epoch: 10, Train_Loss: 5.451210975646973, Test_Loss: 1.5765680074691772\n",
      "Epoch: 10, Train_Loss: 1.3619590997695923, Test_Loss: 1.3792296648025513 *\n",
      "Epoch: 10, Train_Loss: 1.3632252216339111, Test_Loss: 1.4990710020065308\n",
      "Epoch: 10, Train_Loss: 1.376456618309021, Test_Loss: 1.4283339977264404 *\n",
      "Epoch: 10, Train_Loss: 1.5463416576385498, Test_Loss: 1.6372472047805786\n",
      "Epoch: 10, Train_Loss: 1.4095227718353271, Test_Loss: 1.4052056074142456 *\n",
      "Epoch: 10, Train_Loss: 1.468538761138916, Test_Loss: 1.3499339818954468 *\n",
      "Epoch: 10, Train_Loss: 1.4682215452194214, Test_Loss: 1.8502321243286133\n",
      "Epoch: 10, Train_Loss: 1.3989248275756836, Test_Loss: 1.747441291809082 *\n",
      "Epoch: 10, Train_Loss: 1.388765573501587, Test_Loss: 1.8373072147369385\n",
      "Epoch: 10, Train_Loss: 1.3561463356018066, Test_Loss: 1.6086056232452393 *\n",
      "Epoch: 10, Train_Loss: 1.3466203212738037, Test_Loss: 1.3427788019180298 *\n",
      "Epoch: 10, Train_Loss: 4.0592474937438965, Test_Loss: 1.7948052883148193\n",
      "Epoch: 10, Train_Loss: 1.3603485822677612, Test_Loss: 2.381831407546997\n",
      "Epoch: 10, Train_Loss: 1.35545814037323, Test_Loss: 2.290292263031006 *\n",
      "Epoch: 10, Train_Loss: 1.3384864330291748, Test_Loss: 1.4891043901443481 *\n",
      "Epoch: 10, Train_Loss: 1.3496568202972412, Test_Loss: 1.856292963027954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_Loss: 1.342421054840088, Test_Loss: 1.8335912227630615 *\n",
      "Epoch: 10, Train_Loss: 1.3329846858978271, Test_Loss: 1.3826406002044678 *\n",
      "Epoch: 10, Train_Loss: 1.3389649391174316, Test_Loss: 1.5916924476623535\n",
      "Epoch: 10, Train_Loss: 1.350704312324524, Test_Loss: 2.275322437286377\n",
      "Epoch: 10, Train_Loss: 1.3621588945388794, Test_Loss: 1.430985927581787 *\n",
      "Epoch: 10, Train_Loss: 1.3335916996002197, Test_Loss: 1.3536771535873413 *\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SAVEDIR = \"G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 10**-4).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  \n",
    "    \n",
    "    for i in range(int(len(X)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.7})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.7})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 0.7})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR, \"model.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "record = pd.DataFrame(columns = [\"Epoch Number\",\"Test Loss\"])\n",
    "record[\"Epoch Number\"] = epoch_number\n",
    "record[\"Test Loss\"] = test_loss\n",
    "record.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"G:/Applied AI case study/Self driving car/Autopilot-TensorFlow-master/Saver2/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg', 0) \n",
    "rows, cols = img.shape\n",
    "smoothed_angle = 0\n",
    "\n",
    "i = 0\n",
    "while(cv2.waitKey(60) != ord(\"q\")):\n",
    "    full_image = scipy.misc.imread(test_x[i], mode=\"RGB\")\n",
    "    \n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    #degrees = model.y.eval(feed_dict={model.x: [image], model.keep_prob: 1.0})[0][0] * 180.0 / scipy.pi\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 0.5})[0][0] *180 / pi\n",
    "    cv2.imshow(\"frame\", cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR))\n",
    "    smoothed_angle += 0.2 * pow(abs((degrees - smoothed_angle)), 2.0 / 3.0) * (degrees - smoothed_angle) / abs(degrees - smoothed_angle)\n",
    "    #full_image = cv2.imread(test_x[i])\n",
    "    #cv2.imshow('Frame Window', full_image)\n",
    "    #image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    #degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 0.5})[0][0] *180 / pi \n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -smoothed_angle, 1) \n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) \n",
    "    cv2.imshow(\"Steering Wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read all the image along with steering andle from data.txt file\n",
    "* Split the data into train and text by 70:30\n",
    "* Then create a convnet model using 5 convulation layer with different kernel alon with 5 fully connected dense layer\n",
    "* Then train the model using a training dataset and store the loss at each epoch\n",
    "* At the end we visualize the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res=pd.read_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch Number  Test Loss\n",
      "0             0  18.109542\n",
      "1             1  12.926361\n",
      "2             2   9.416918\n",
      "3             3   7.008951\n",
      "4             4   5.298237\n",
      "5             5   4.094995\n",
      "6             6   3.270577\n",
      "7             7   2.656271\n",
      "8             8   2.210532\n",
      "9             9   1.912212\n"
     ]
    }
   ],
   "source": [
    "print(res[[\"Epoch Number\",\"Test Loss\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
